{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.4\n",
      "numpy 1.19.5\n",
      "pandas 1.1.5\n",
      "sklearn 0.24.2\n",
      "tensorflow 2.6.2\n",
      "keras.api._v2.keras 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_00.csv  test_08.csv   train_06.csv  train_14.csv  valid_02.csv\r\n",
      "test_01.csv  test_09.csv   train_07.csv  train_15.csv  valid_03.csv\r\n",
      "test_02.csv  train_00.csv  train_08.csv  train_16.csv  valid_04.csv\r\n",
      "test_03.csv  train_01.csv  train_09.csv  train_17.csv  valid_05.csv\r\n",
      "test_04.csv  train_02.csv  train_10.csv  train_18.csv  valid_06.csv\r\n",
      "test_05.csv  train_03.csv  train_11.csv  train_19.csv  valid_07.csv\r\n",
      "test_06.csv  train_04.csv  train_12.csv  valid_00.csv  valid_08.csv\r\n",
      "test_07.csv  train_05.csv  train_13.csv  valid_01.csv  valid_09.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls generate_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./generate_csv/train_01.csv',\n",
      " './generate_csv/train_12.csv',\n",
      " './generate_csv/train_04.csv',\n",
      " './generate_csv/train_02.csv',\n",
      " './generate_csv/train_03.csv',\n",
      " './generate_csv/train_17.csv',\n",
      " './generate_csv/train_08.csv',\n",
      " './generate_csv/train_11.csv',\n",
      " './generate_csv/train_13.csv',\n",
      " './generate_csv/train_15.csv',\n",
      " './generate_csv/train_16.csv',\n",
      " './generate_csv/train_19.csv',\n",
      " './generate_csv/train_18.csv',\n",
      " './generate_csv/train_07.csv',\n",
      " './generate_csv/train_14.csv',\n",
      " './generate_csv/train_06.csv',\n",
      " './generate_csv/train_09.csv',\n",
      " './generate_csv/train_00.csv',\n",
      " './generate_csv/train_10.csv',\n",
      " './generate_csv/train_05.csv']\n",
      "['./generate_csv/valid_00.csv',\n",
      " './generate_csv/valid_08.csv',\n",
      " './generate_csv/valid_03.csv',\n",
      " './generate_csv/valid_09.csv',\n",
      " './generate_csv/valid_06.csv',\n",
      " './generate_csv/valid_01.csv',\n",
      " './generate_csv/valid_04.csv',\n",
      " './generate_csv/valid_05.csv',\n",
      " './generate_csv/valid_07.csv',\n",
      " './generate_csv/valid_02.csv']\n",
      "['./generate_csv/test_04.csv',\n",
      " './generate_csv/test_07.csv',\n",
      " './generate_csv/test_00.csv',\n",
      " './generate_csv/test_08.csv',\n",
      " './generate_csv/test_06.csv',\n",
      " './generate_csv/test_09.csv',\n",
      " './generate_csv/test_01.csv',\n",
      " './generate_csv/test_02.csv',\n",
      " './generate_csv/test_05.csv',\n",
      " './generate_csv/test_03.csv']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"./generate_csv/\"\n",
    "\n",
    "# 通过判断开头去添加文件\n",
    "def get_filenames_by_prefix(source_dir, prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    results = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):\n",
    "            results.append(os.path.join(source_dir, filename))\n",
    "    return results\n",
    "\n",
    "train_filenames = get_filenames_by_prefix(source_dir, \"train\")\n",
    "valid_filenames = get_filenames_by_prefix(source_dir, \"valid\")\n",
    "test_filenames = get_filenames_by_prefix(source_dir, \"test\")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(train_filenames)\n",
    "pprint.pprint(valid_filenames)\n",
    "pprint.pprint(test_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面的接口都是之前用过的\n",
    "def parse_csv_line(line, n_fields = 9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x, y\n",
    "\n",
    "def csv_reader_dataset(filenames, n_readers=5,\n",
    "                       batch_size=32, n_parse_threads=5,\n",
    "                       shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_readers\n",
    "    )\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    # map，通过parse_csv_line对数据集进行映射，map只会给函数传递一个参数\n",
    "    dataset = dataset.map(parse_csv_line,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "train_set = csv_reader_dataset(train_filenames,\n",
    "                               batch_size = batch_size)\n",
    "valid_set = csv_reader_dataset(valid_filenames,\n",
    "                               batch_size = batch_size)\n",
    "test_set = csv_reader_dataset(test_filenames,\n",
    "                              batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_csv\t\t      tf03_tfrecord_basic_api.ipynb\r\n",
      "tf01_data_basic_api.ipynb     tf04_data_generate_tfrecord.ipynb\r\n",
      "tf02_data_generate_csv.ipynb  tfrecord_basic\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把基础的如何序列化的步骤搞到一个函数\n",
    "def serialize_example(x, y):\n",
    "    \"\"\"Converts x, y to tf.train.Example and serialize\"\"\"\n",
    "    input_feautres = tf.train.FloatList(value = x)\n",
    "    label = tf.train.FloatList(value = y)\n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\": tf.train.Feature(\n",
    "                float_list = input_feautres),\n",
    "            \"label\": tf.train.Feature(float_list = label)\n",
    "        }\n",
    "    )\n",
    "    # 把features变为example\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()  # 把example序列化\n",
    "\n",
    "# n_shards是存为多少个文件，steps_per_shard和 steps_per_epoch类似\n",
    "def csv_dataset_to_tfrecords(base_filename, dataset,\n",
    "                             n_shards, steps_per_shard,\n",
    "                             compression_type = None):\n",
    "    # 压缩文件类型\n",
    "    options = tf.io.TFRecordOptions(\n",
    "        compression_type = compression_type)\n",
    "    all_filenames = []\n",
    "    \n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath = '{}_{:05d}-of-{:05d}'.format(\n",
    "            base_filename, shard_id, n_shards)\n",
    "        # 打开文件\n",
    "        with tf.io.TFRecordWriter(filename_fullpath, options) as writer:\n",
    "            # 取出数据,为什么skip，上一个文件写了前500行，下一个文件存后面的数据\n",
    "            for x_batch, y_batch in dataset.skip(shard_id * steps_per_shard).take(steps_per_shard):\n",
    "                for x_example, y_example in zip(x_batch, y_batch):\n",
    "                    writer.write(\n",
    "                        serialize_example(x_example, y_example))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "    # 返回所有tfrecord文件名\n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[ 4.97103445e-02, -8.49241912e-01, -6.21469952e-02,\n",
      "         1.78787470e-01, -8.02535415e-01,  5.06606710e-04,\n",
      "         6.46645725e-01, -1.10607934e+00],\n",
      "       [ 6.36364639e-01, -1.08954263e+00,  9.26090255e-02,\n",
      "        -2.05381244e-01,  1.20256710e+00, -3.63012254e-02,\n",
      "        -6.78410172e-01,  1.82235345e-01],\n",
      "       [ 4.85305160e-01, -8.49241912e-01, -6.53012618e-02,\n",
      "        -2.33796556e-02,  1.49743509e+00, -7.79065788e-02,\n",
      "        -9.02363241e-01,  7.81451464e-01],\n",
      "       [-3.26526344e-01,  4.32361901e-01, -9.34545919e-02,\n",
      "        -8.40299204e-02,  8.46003592e-01, -2.66316477e-02,\n",
      "        -5.61767936e-01,  1.42287597e-01],\n",
      "       [ 8.01544309e-01,  2.72161424e-01, -1.16243929e-01,\n",
      "        -2.02311516e-01, -5.43051600e-01, -2.10396163e-02,\n",
      "        -5.89762092e-01, -8.24184567e-02],\n",
      "       [-1.45385098e+00,  1.87416613e+00, -1.13157141e+00,\n",
      "         3.61127615e-01, -3.97885799e-01, -3.27385925e-02,\n",
      "        -7.39064157e-01,  6.46627843e-01],\n",
      "       [-1.06354737e+00,  1.87416613e+00, -4.93448943e-01,\n",
      "        -6.96261302e-02, -2.73587584e-01, -1.34195149e-01,\n",
      "         1.03389800e+00, -1.34576583e+00],\n",
      "       [-7.43205428e-01,  9.12963331e-01, -6.44320250e-01,\n",
      "        -1.47909701e-01,  7.39851117e-01,  1.14276908e-01,\n",
      "        -7.95052409e-01,  6.81582153e-01],\n",
      "       [ 2.27542663e+00, -1.24974310e+00,  1.02947879e+00,\n",
      "        -1.71244323e-01, -4.54137534e-01,  1.05271518e-01,\n",
      "        -9.02363241e-01,  9.01294708e-01],\n",
      "       [-2.98072815e-01,  3.52261662e-01, -1.09205075e-01,\n",
      "        -2.50555217e-01, -3.40640247e-02, -6.03400404e-03,\n",
      "         1.08055484e+00, -1.06113815e+00],\n",
      "       [-1.11795020e+00,  3.52261662e-01, -1.74154803e-01,\n",
      "         1.02935731e-01, -2.43647128e-01, -6.19525239e-02,\n",
      "         1.90638196e+00, -1.12105978e+00],\n",
      "       [-9.97422278e-01,  1.23336422e+00, -7.57719278e-01,\n",
      "        -1.11092515e-02, -2.30037838e-01,  5.48742227e-02,\n",
      "        -7.57726908e-01,  7.06549466e-01],\n",
      "       [-8.24676275e-01, -4.82395217e-02, -3.44865829e-01,\n",
      "        -8.47758725e-02,  5.01234829e-01, -3.46999951e-02,\n",
      "         5.30003488e-01, -8.74119252e-02],\n",
      "       [-4.39434648e-01,  1.92061186e-01, -3.91724408e-01,\n",
      "        -6.23378716e-02,  6.82692051e-01, -1.20800082e-02,\n",
      "         9.35918450e-01, -1.24589646e+00],\n",
      "       [-3.05882934e-02, -9.29342151e-01,  2.59621471e-01,\n",
      "        -6.01274054e-03, -5.00409126e-01, -3.07798684e-02,\n",
      "         1.59844637e+00, -1.81515181e+00],\n",
      "       [ 7.11974323e-01, -8.49241912e-01,  2.80045152e-01,\n",
      "        -9.29236189e-02, -2.67236561e-01,  1.61800906e-01,\n",
      "        -1.37359798e+00,  1.27580476e+00],\n",
      "       [-1.86834182e-03, -1.24974310e+00,  2.20688999e-01,\n",
      "        -2.50608753e-02,  3.33386868e-01,  2.75048837e-02,\n",
      "         9.17255700e-01, -6.81634605e-01],\n",
      "       [ 7.00647458e-02,  3.18607129e-02, -2.57098645e-01,\n",
      "        -3.00019473e-01, -2.66329288e-01, -9.85835046e-02,\n",
      "         1.08522058e+00, -1.37073314e+00],\n",
      "       [ 2.09371112e-02,  1.07316375e+00, -1.60005420e-01,\n",
      "         3.40700382e-03, -2.53627270e-01, -1.21706501e-01,\n",
      "        -7.01738596e-01,  7.46497214e-01],\n",
      "       [ 1.90638328e+00,  5.12462139e-01,  4.47582811e-01,\n",
      "        -2.76721776e-01, -6.31058335e-01, -7.08114654e-02,\n",
      "        -7.06404328e-01,  7.46497214e-01],\n",
      "       [-3.81355345e-01, -1.00944233e+00,  5.68577409e-01,\n",
      "         5.82301915e-01, -1.17724466e+00,  5.64776845e-02,\n",
      "         1.24088459e-01, -2.12248623e-01],\n",
      "       [-4.90587056e-01,  3.52261662e-01, -6.64385974e-01,\n",
      "         2.82370716e-01,  5.10307670e-01,  4.01096910e-01,\n",
      "         8.51936042e-01, -1.31580508e+00],\n",
      "       [ 1.23452818e+00, -1.32984328e+00,  5.75435698e-01,\n",
      "        -1.19361848e-01, -4.16938782e-01,  1.54011786e-01,\n",
      "         1.09921765e+00, -1.34576583e+00],\n",
      "       [-4.71458197e-01,  5.92562377e-01, -3.13082099e-01,\n",
      "        -1.15125410e-01, -1.84673533e-01, -9.00404751e-02,\n",
      "         1.38382471e+00, -9.06340659e-01],\n",
      "       [ 3.88017392e+00, -9.29342151e-01,  1.29029870e+00,\n",
      "        -1.72691330e-01, -2.25501403e-01,  5.14101014e-02,\n",
      "        -8.46374989e-01,  8.86314332e-01],\n",
      "       [-9.45737004e-01,  3.18607129e-02, -7.51999438e-01,\n",
      "        -1.67815894e-01,  5.81983268e-01,  8.17321911e-02,\n",
      "        -7.48395503e-01,  6.31647468e-01],\n",
      "       [ 4.95588928e-01,  1.23336422e+00, -2.66338676e-01,\n",
      "        -3.35229747e-02, -1.15637708e+00, -1.26926363e-01,\n",
      "         8.65933120e-01, -1.33577895e+00],\n",
      "       [ 1.58142820e-01,  1.15326405e+00, -9.75820422e-02,\n",
      "        -2.74813175e-01, -6.60091519e-01, -9.36449692e-02,\n",
      "        -8.51040661e-01,  7.26523340e-01],\n",
      "       [ 1.99765837e+00, -2.88540244e-01,  1.02678835e+00,\n",
      "        -1.89570740e-01, -3.32561165e-01,  1.97733175e-02,\n",
      "         1.06655777e+00, -1.21593571e+00],\n",
      "       [ 3.72503400e-01, -6.89041436e-01,  6.45801365e-01,\n",
      "         8.00678432e-02, -3.15322757e-01, -2.51115970e-02,\n",
      "         5.62663257e-01, -5.74511178e-02],\n",
      "       [-8.59417319e-01, -1.24974310e+00,  1.97845548e-01,\n",
      "         2.18568936e-01,  4.32281077e-01, -2.69207880e-02,\n",
      "         4.88012254e-01,  2.27176547e-01],\n",
      "       [-3.88761789e-01, -9.29342151e-01,  1.09950021e-01,\n",
      "        -1.34794548e-01,  4.31373775e-01,  6.92178495e-03,\n",
      "         8.19276214e-01, -6.41686857e-01]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[2.286  ],\n",
      "       [2.429  ],\n",
      "       [2.956  ],\n",
      "       [2.431  ],\n",
      "       [3.226  ],\n",
      "       [1.875  ],\n",
      "       [1.982  ],\n",
      "       [1.438  ],\n",
      "       [3.798  ],\n",
      "       [1.514  ],\n",
      "       [0.603  ],\n",
      "       [1.739  ],\n",
      "       [0.717  ],\n",
      "       [1.618  ],\n",
      "       [1.598  ],\n",
      "       [1.33   ],\n",
      "       [1.293  ],\n",
      "       [1.61   ],\n",
      "       [2.811  ],\n",
      "       [5.00001],\n",
      "       [0.625  ],\n",
      "       [2.028  ],\n",
      "       [2.487  ],\n",
      "       [1.366  ],\n",
      "       [5.00001],\n",
      "       [2.146  ],\n",
      "       [2.538  ],\n",
      "       [2.26   ],\n",
      "       [3.302  ],\n",
      "       [1.431  ],\n",
      "       [0.905  ],\n",
      "       [1.139  ]], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[-1.15839255e+00,  1.23336422e+00, -7.80114472e-01,\n",
      "        -3.65703627e-02,  1.13002844e-02,  2.21132398e-01,\n",
      "        -7.76389658e-01,  6.61608279e-01],\n",
      "       [-6.00511491e-01, -9.29342151e-01,  8.90805721e-02,\n",
      "        -8.84979889e-02,  1.66709757e+00, -2.40625143e-02,\n",
      "         1.63577187e+00, -1.20594871e+00],\n",
      "       [ 4.06818151e-01,  1.79406595e+00,  5.06766677e-01,\n",
      "        -1.45333722e-01, -4.59581256e-01, -9.45077166e-02,\n",
      "         9.73243952e-01, -1.28085077e+00],\n",
      "       [ 1.19051588e+00, -7.69141674e-01,  9.89540458e-01,\n",
      "        -2.12735422e-02,  8.56977552e-02, -5.93205914e-03,\n",
      "        -7.06404328e-01,  9.56222892e-01],\n",
      "       [-1.31541979e+00,  1.47366500e+00, -5.66471815e-01,\n",
      "         3.48493189e-01, -2.48183563e-01,  6.95808083e-02,\n",
      "         5.06675005e-01, -1.17372729e-01],\n",
      "       [ 1.54565215e+00, -4.82395217e-02,  9.06798303e-01,\n",
      "        -2.08518714e-01, -4.51415658e-01,  6.49431208e-03,\n",
      "        -6.41084671e-01,  5.16797721e-01],\n",
      "       [ 1.18785167e+00,  1.87416613e+00,  3.70319873e-01,\n",
      "        -2.74600744e-01, -6.64627910e-01, -3.29345651e-02,\n",
      "        -6.92407250e-01,  7.06549466e-01],\n",
      "       [ 1.19904125e+00, -4.82395217e-02,  7.49122143e-01,\n",
      "         1.30882874e-01, -6.03753254e-02, -2.95489747e-02,\n",
      "        -5.52436531e-01,  3.24313045e-02],\n",
      "       [ 4.34579015e-01, -1.73034453e+00, -6.06062233e-01,\n",
      "        -2.97334760e-01, -5.58388941e-02, -1.39318004e-01,\n",
      "        -8.27712238e-01,  8.51360023e-01],\n",
      "       [-8.20626736e-01, -4.48740691e-01, -1.38116926e-01,\n",
      "        -2.34548375e-02,  1.94563448e+00, -2.73775794e-02,\n",
      "         5.20672083e-01, -7.24315196e-02],\n",
      "       [-8.47588301e-01, -3.68640482e-01, -7.65133917e-01,\n",
      "        -6.16804287e-02, -5.84786713e-01,  2.47334033e-01,\n",
      "        -8.69703472e-01,  6.66601717e-01],\n",
      "       [-7.96329319e-01, -1.00944233e+00, -3.03180069e-01,\n",
      "        -3.22863668e-01, -9.73105252e-01, -2.05771059e-01,\n",
      "         1.09921765e+00, -1.23590958e+00],\n",
      "       [-9.46110010e-01, -1.16964281e+00, -7.86615014e-01,\n",
      "         4.27422374e-02,  8.11526716e-01, -1.41121835e-01,\n",
      "         1.31383932e+00, -1.57546532e+00],\n",
      "       [-7.00418532e-01, -6.08941197e-01, -9.77702677e-01,\n",
      "        -8.18639174e-02,  2.71954966e+00,  1.30512059e-01,\n",
      "        -8.69703472e-01,  6.51621342e-01],\n",
      "       [ 9.52177525e-01, -1.16964281e+00,  1.23668119e-01,\n",
      "        -1.18349411e-01,  5.97307777e+00,  5.35122491e-03,\n",
      "         9.03258622e-01, -1.23091602e+00],\n",
      "       [-9.21599448e-01, -6.89041436e-01, -3.28999870e-02,\n",
      "        -1.79894075e-01, -6.04747057e-01, -3.22904205e-03,\n",
      "        -1.41855851e-01,  3.47019792e-01],\n",
      "       [-2.73136020e-01, -1.73034453e+00, -1.67168081e-01,\n",
      "         7.93834254e-02,  1.04288471e+00, -1.08005293e-01,\n",
      "        -6.97072923e-01,  1.13099420e+00],\n",
      "       [ 2.46559644e+00,  8.32863092e-01,  2.69356281e-01,\n",
      "        -7.10264966e-02, -3.48892331e-01, -3.94894332e-02,\n",
      "        -8.13715160e-01,  5.81712782e-01],\n",
      "       [-9.77494121e-01, -7.69141674e-01,  9.34700295e-02,\n",
      "         1.43184856e-01, -6.08376205e-01, -9.13964584e-02,\n",
      "         1.33716774e+00, -9.06340659e-01],\n",
      "       [ 8.48776326e-02,  1.11960948e-01, -7.06329763e-01,\n",
      "        -4.41764444e-01, -9.33184624e-01,  7.34858215e-02,\n",
      "        -6.36418939e-01,  2.07202688e-01],\n",
      "       [-1.14219427e+00,  1.87416613e+00, -5.08277178e-01,\n",
      "        -2.15575665e-01, -1.10738361e+00, -1.25163317e-01,\n",
      "         1.04322934e+00, -1.36074626e+00],\n",
      "       [ 3.65789652e-01, -8.49241912e-01,  3.15304190e-01,\n",
      "         7.98635185e-02,  2.30950043e-02,  1.90921947e-02,\n",
      "         5.44000506e-01, -1.05614471e+00],\n",
      "       [-7.93079019e-01, -4.82395217e-02, -8.71129185e-02,\n",
      "        -1.95470005e-01, -8.10701013e-01, -9.99945030e-02,\n",
      "         9.91906762e-01, -6.41686857e-01],\n",
      "       [ 3.89341086e-01,  3.18607129e-02,  2.58530855e-01,\n",
      "        -1.57210678e-01, -3.28932047e-01,  5.68162799e-02,\n",
      "         2.92053282e-01, -1.02392331e-01],\n",
      "       [-7.70753145e-01,  2.72161424e-01, -2.13212118e-01,\n",
      "         9.15781930e-02,  1.09287195e-01, -3.41971479e-02,\n",
      "         1.36982763e+00, -1.50056338e+00],\n",
      "       [ 2.75153965e-01, -6.89041436e-01,  3.22820395e-01,\n",
      "        -6.97026253e-02,  2.27234393e-01, -3.94091308e-02,\n",
      "         1.08055484e+00, -1.26087689e+00],\n",
      "       [-3.06438357e-01, -1.08954263e+00, -7.24840015e-02,\n",
      "         1.38242114e-02,  5.31175256e-01,  2.84265610e-03,\n",
      "         8.00613463e-01, -5.21843612e-01],\n",
      "       [-3.02655220e-01,  1.87416613e+00, -3.98439467e-01,\n",
      "        -8.31251666e-02, -6.52833223e-01, -2.02982292e-01,\n",
      "         1.01056945e+00, -1.33078539e+00],\n",
      "       [ 6.82530999e-02,  1.87416613e+00, -2.21317574e-01,\n",
      "        -1.46415502e-01, -2.74494857e-01, -9.58727002e-02,\n",
      "         8.93927276e-01, -1.36573970e+00],\n",
      "       [-8.00325632e-01, -2.88540244e-01, -3.32828134e-01,\n",
      "        -1.47034405e-02, -4.85892534e-01, -2.28945658e-01,\n",
      "        -9.53685880e-01,  9.31255519e-01],\n",
      "       [ 6.01730168e-01,  1.87416613e+00,  5.15301764e-01,\n",
      "         4.12285514e-02, -5.70270181e-01, -6.00491315e-02,\n",
      "         8.98592949e-01, -1.38571358e+00],\n",
      "       [-2.42071569e-01,  1.31346452e+00, -2.43087426e-01,\n",
      "        -7.42173567e-02, -1.48382083e-01, -3.49856466e-02,\n",
      "        -7.29732752e-01,  6.06680095e-01]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[1.125  ],\n",
      "       [0.72   ],\n",
      "       [2.519  ],\n",
      "       [2.541  ],\n",
      "       [0.496  ],\n",
      "       [3.482  ],\n",
      "       [2.618  ],\n",
      "       [5.00001],\n",
      "       [1.624  ],\n",
      "       [0.69   ],\n",
      "       [1.523  ],\n",
      "       [0.75   ],\n",
      "       [1.354  ],\n",
      "       [1.673  ],\n",
      "       [2.698  ],\n",
      "       [1.053  ],\n",
      "       [1.222  ],\n",
      "       [4.721  ],\n",
      "       [0.92   ],\n",
      "       [2.192  ],\n",
      "       [1.388  ],\n",
      "       [2.21   ],\n",
      "       [0.947  ],\n",
      "       [1.008  ],\n",
      "       [1.831  ],\n",
      "       [2.282  ],\n",
      "       [0.835  ],\n",
      "       [2.685  ],\n",
      "       [3.569  ],\n",
      "       [0.748  ],\n",
      "       [5.00001],\n",
      "       [3.979  ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for i in train_set.take(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 s, sys: 21.5 s, total: 1min 2s\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 训练集和测试集都分20\n",
    "n_shards = 20\n",
    "train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "valid_steps_per_shard = 3880 // batch_size // 10\n",
    "test_steps_per_shard = 5170 // batch_size // 10\n",
    "\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "valid_basename = os.path.join(output_dir, \"valid\")\n",
    "test_basename = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    train_basename, train_set, n_shards, train_steps_per_shard, None)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "    valid_basename, valid_set, 10, valid_steps_per_shard, None)\n",
    "test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "    test_basename, test_set, 10, test_steps_per_shard, None)\n",
    "# 执行会发现目录下总计生成了40个文件,这里文件数目改为一致，为了对比时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1960\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00000-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00001-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00002-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00003-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00004-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00005-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00006-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00007-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00008-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 47616 May  4 20:33 test_00009-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00000-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00001-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00002-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00003-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00004-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00005-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00006-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00007-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00008-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00009-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00010-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00011-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00012-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00013-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00014-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00015-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:32 train_00016-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:33 train_00017-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:33 train_00018-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 53568 May  4 20:33 train_00019-of-00020\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00000-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00001-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00002-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00003-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00004-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00005-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00006-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00007-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00008-of-00010\r\n",
      "-rw-rw-r-- 1 wp wp 35712 May  4 20:33 valid_00009-of-00010\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l generate_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一下压缩的\n",
    "# n_shards = 20\n",
    "# train_steps_per_shard = 11610 // batch_size // n_shards\n",
    "# valid_steps_per_shard = 3880 // batch_size // n_shards\n",
    "# test_steps_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "# output_dir = \"generate_tfrecords_zip\"\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.mkdir(output_dir)\n",
    "\n",
    "# train_basename = os.path.join(output_dir, \"train\")\n",
    "# valid_basename = os.path.join(output_dir, \"valid\")\n",
    "# test_basename = os.path.join(output_dir, \"test\")\n",
    "# # 只需修改参数的类型即可\n",
    "# train_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     train_basename, train_set, n_shards, train_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# valid_tfrecord_filenames = csv_dataset_to_tfrecords(\n",
    "#     valid_basename, valid_set, n_shards, valid_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")\n",
    "# test_tfrecord_fielnames = csv_dataset_to_tfrecords(\n",
    "#     test_basename, test_set, n_shards, test_steps_per_shard,\n",
    "#     compression_type = \"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !ls -l generate_tfrecords_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords/train_00000-of-00020',\n",
      " 'generate_tfrecords/train_00001-of-00020',\n",
      " 'generate_tfrecords/train_00002-of-00020',\n",
      " 'generate_tfrecords/train_00003-of-00020',\n",
      " 'generate_tfrecords/train_00004-of-00020',\n",
      " 'generate_tfrecords/train_00005-of-00020',\n",
      " 'generate_tfrecords/train_00006-of-00020',\n",
      " 'generate_tfrecords/train_00007-of-00020',\n",
      " 'generate_tfrecords/train_00008-of-00020',\n",
      " 'generate_tfrecords/train_00009-of-00020',\n",
      " 'generate_tfrecords/train_00010-of-00020',\n",
      " 'generate_tfrecords/train_00011-of-00020',\n",
      " 'generate_tfrecords/train_00012-of-00020',\n",
      " 'generate_tfrecords/train_00013-of-00020',\n",
      " 'generate_tfrecords/train_00014-of-00020',\n",
      " 'generate_tfrecords/train_00015-of-00020',\n",
      " 'generate_tfrecords/train_00016-of-00020',\n",
      " 'generate_tfrecords/train_00017-of-00020',\n",
      " 'generate_tfrecords/train_00018-of-00020',\n",
      " 'generate_tfrecords/train_00019-of-00020']\n",
      "['generate_tfrecords/valid_00000-of-00010',\n",
      " 'generate_tfrecords/valid_00001-of-00010',\n",
      " 'generate_tfrecords/valid_00002-of-00010',\n",
      " 'generate_tfrecords/valid_00003-of-00010',\n",
      " 'generate_tfrecords/valid_00004-of-00010',\n",
      " 'generate_tfrecords/valid_00005-of-00010',\n",
      " 'generate_tfrecords/valid_00006-of-00010',\n",
      " 'generate_tfrecords/valid_00007-of-00010',\n",
      " 'generate_tfrecords/valid_00008-of-00010',\n",
      " 'generate_tfrecords/valid_00009-of-00010']\n",
      "['generate_tfrecords/test_00000-of-00010',\n",
      " 'generate_tfrecords/test_00001-of-00010',\n",
      " 'generate_tfrecords/test_00002-of-00010',\n",
      " 'generate_tfrecords/test_00003-of-00010',\n",
      " 'generate_tfrecords/test_00004-of-00010',\n",
      " 'generate_tfrecords/test_00005-of-00010',\n",
      " 'generate_tfrecords/test_00006-of-00010',\n",
      " 'generate_tfrecords/test_00007-of-00010',\n",
      " 'generate_tfrecords/test_00008-of-00010',\n",
      " 'generate_tfrecords/test_00009-of-00010']\n"
     ]
    }
   ],
   "source": [
    "# 打印一下文件名\n",
    "pprint.pprint(train_tfrecord_filenames)\n",
    "pprint.pprint(valid_tfrecord_filenames)\n",
    "pprint.pprint(test_tfrecord_fielnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39 µs, sys: 27 µs, total: 66 µs\n",
      "Wall time: 72.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 把数据读取出来\n",
    "expected_features = {\n",
    "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example,\n",
    "                                         expected_features)\n",
    "    return example[\"input_features\"], example[\"label\"]\n",
    "\n",
    "def tfrecords_reader_dataset(filenames, n_readers=5,\n",
    "                             batch_size=32, n_parse_threads=5,\n",
    "                             shuffle_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "#         lambda filename: tf.data.TFRecordDataset(\n",
    "#             filename, compression_type = \"GZIP\"),\n",
    "          lambda filename: tf.data.TFRecordDataset(\n",
    "            filename),\n",
    "          cycle_length = n_readers\n",
    "    )\n",
    "    # 洗牌，就是给数据打乱\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_example,\n",
    "                          num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)  # 原来写进去是一条一条的sample，要分配\n",
    "    return dataset\n",
    "\n",
    "# 测试一下，tfrecords_reader_dataset是否可以正常运行\n",
    "# tfrecords_train = tfrecords_reader_dataset(train_tfrecord_filenames,\n",
    "#                                            batch_size = 3)\n",
    "# for x_batch, y_batch in tfrecords_train.take(10):\n",
    "#     print(x_batch)\n",
    "#     print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a09279d8> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a09279d8>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a09279d8> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a09279d8>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function parse_example at 0x7f27a0927e18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x7f27a0927e18>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function parse_example at 0x7f27a0927e18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <function parse_example at 0x7f27a0927e18>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a0927048> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a0927048>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a0927048> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a0927048>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a1400268> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a1400268>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a1400268> and will run it as-is.\n",
      "Cause: could not parse the source code of <function tfrecords_reader_dataset.<locals>.<lambda> at 0x7f27a1400268>: no matching AST found\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "CPU times: user 131 ms, sys: 0 ns, total: 131 ms\n",
      "Wall time: 126 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 得到dataset,dataset是tensor，可以直接拿tensor训练\n",
    "\n",
    "batch_size = 32\n",
    "tfrecords_train_set = tfrecords_reader_dataset(\n",
    "    train_tfrecord_filenames, batch_size = batch_size)\n",
    "tfrecords_valid_set = tfrecords_reader_dataset(\n",
    "    valid_tfrecord_filenames, batch_size = batch_size)\n",
    "tfrecords_test_set = tfrecords_reader_dataset(\n",
    "    test_tfrecord_fielnames, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfrecords_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[ 2.40306377e-01, -9.29342151e-01,  3.26784939e-01,\n",
      "        -1.03937596e-01,  1.51285887e+00, -7.02182502e-02,\n",
      "         1.50979829e+00, -7.41556227e-01],\n",
      "       [-4.78758067e-01,  7.52762854e-01, -3.41764122e-01,\n",
      "        -7.93565735e-02,  8.50539982e-01, -1.22063585e-01,\n",
      "        -1.35493517e+00,  1.20090282e+00],\n",
      "       [-9.41261172e-01,  1.92061186e-01, -4.92568582e-01,\n",
      "        -2.83727143e-02, -3.77018213e-01,  7.45931044e-02,\n",
      "        -7.67058253e-01,  1.08605301e+00],\n",
      "       [ 2.78647113e+00,  1.15326405e+00,  7.80198097e-01,\n",
      "        -1.65000588e-01,  3.48897241e-02, -7.05759078e-02,\n",
      "        -6.87741518e-01,  6.96562529e-01],\n",
      "       [-1.20560193e+00,  2.72161424e-01, -5.76291919e-01,\n",
      "         5.08244373e-02, -1.23885356e-01,  1.66515172e-01,\n",
      "        -7.67058253e-01,  6.61608279e-01],\n",
      "       [-5.00178158e-01, -1.08954263e+00,  7.68675879e-02,\n",
      "         5.26065975e-02,  4.82181817e-01, -2.72741280e-02,\n",
      "         1.52379537e+00, -6.51673794e-01],\n",
      "       [-8.86165738e-01,  4.32361901e-01, -3.20695907e-01,\n",
      "        -9.00600329e-02, -7.61707544e-01, -8.38399678e-02,\n",
      "         1.38382471e+00, -9.21321094e-01],\n",
      "       [-7.57911742e-01, -1.73034453e+00, -3.41406345e-01,\n",
      "        -3.20746675e-02, -3.29839319e-01, -6.00274168e-02,\n",
      "         1.64043760e+00, -1.02618384e+00],\n",
      "       [ 1.63950741e-01, -7.69141674e-01, -3.11889917e-01,\n",
      "        -3.08449976e-02, -3.66130769e-01, -4.92408201e-02,\n",
      "        -8.88366222e-01,  8.06418836e-01],\n",
      "       [ 1.83370423e+00, -1.28339753e-01,  9.53682184e-01,\n",
      "         6.68827057e-01, -7.57171094e-01, -1.66341856e-01,\n",
      "        -1.29894686e+00,  1.16095507e+00],\n",
      "       [-7.99473047e-01, -4.48740691e-01, -1.82180151e-01,\n",
      "        -1.35910306e-02, -2.63607442e-01, -7.32519701e-02,\n",
      "         1.92504466e+00, -1.01120353e+00],\n",
      "       [-2.01043069e-01,  6.72662616e-01,  3.46156023e-02,\n",
      "        -2.67019212e-01, -4.44157392e-01, -5.65528534e-02,\n",
      "        -7.20401347e-01,  9.56222892e-01],\n",
      "       [-3.66808861e-01, -1.16964281e+00,  3.10911357e-01,\n",
      "         1.28765717e-01, -4.64117676e-01, -1.83039643e-02,\n",
      "         1.08988619e+00, -3.27098370e-01],\n",
      "       [-9.47495401e-01,  6.72662616e-01,  7.84454584e+00,\n",
      "         7.48312140e+00, -9.89436388e-01, -7.54187405e-02,\n",
      "         1.15520585e+00, -2.72170246e-01],\n",
      "       [-1.15839255e+00,  1.39356470e+00, -8.04791093e-01,\n",
      "        -2.14850545e-01,  5.85656753e-03,  2.28882566e-01,\n",
      "        -7.43729830e-01,  6.91569090e-01],\n",
      "       [-8.95734206e-02,  7.52762854e-01, -4.88562912e-01,\n",
      "        -1.61846191e-01, -3.22581023e-01,  8.59187022e-02,\n",
      "        -6.31753266e-01,  5.71725845e-01],\n",
      "       [-2.73136020e-01, -2.88540244e-01, -1.29565611e-01,\n",
      "        -1.44866973e-01,  9.95705783e-01,  1.39723152e-01,\n",
      "        -8.60372066e-01,  7.91438401e-01],\n",
      "       [-3.23435873e-01, -4.82395217e-02, -2.20513972e-03,\n",
      "        -1.57939419e-01, -1.07554205e-01, -7.40554631e-02,\n",
      "         1.66376603e+00, -7.26575792e-01],\n",
      "       [-8.05973709e-01, -4.82395217e-02,  1.87927261e-01,\n",
      "         4.74256754e-01, -8.47899735e-01, -8.25198442e-02,\n",
      "         1.53312671e+00, -2.02261686e-01],\n",
      "       [-1.54526338e-01, -1.97064519e+00,  7.77319551e-01,\n",
      "         1.23348460e-01,  3.99618775e-01, -4.26260009e-02,\n",
      "        -4.91782576e-01,  1.16594851e+00],\n",
      "       [-4.15989786e-01, -1.28339753e-01,  1.63143575e-01,\n",
      "        -1.86865836e-01, -6.33780181e-01,  2.08882112e-02,\n",
      "         7.86616385e-01, -7.26575792e-01],\n",
      "       [ 2.11746290e-01, -1.40994358e+00,  2.43801379e+00,\n",
      "         1.81486380e+00, -4.78634268e-01, -5.81035204e-02,\n",
      "         9.31252778e-01,  3.07072043e-01],\n",
      "       [ 1.67822218e+00, -1.16964281e+00,  4.30337161e-01,\n",
      "        -3.14885497e-01, -2.72680283e-01,  9.87462513e-03,\n",
      "         5.53331912e-01, -1.27359673e-01],\n",
      "       [ 1.01734352e+00, -1.00944233e+00,  3.58371884e-01,\n",
      "         1.39490440e-01, -9.84899938e-01,  5.40965460e-02,\n",
      "        -5.71099281e-01,  1.72248408e-01],\n",
      "       [-7.63590485e-02,  9.12963331e-01, -2.36177996e-01,\n",
      "        -5.91241010e-03,  5.49320996e-01, -1.70023158e-01,\n",
      "        -7.15735674e-01,  6.16667032e-01],\n",
      "       [-6.39781594e-01,  1.31346452e+00, -2.33584076e-01,\n",
      "        -7.95667395e-02, -1.12997919e-01, -2.66586058e-02,\n",
      "         1.06655777e+00, -1.38072014e+00],\n",
      "       [ 4.71184939e-01, -1.00944233e+00,  3.00177097e-01,\n",
      "        -3.67553443e-01, -6.23800039e-01,  2.74736024e-02,\n",
      "         1.21119416e+00, -1.51055026e+00],\n",
      "       [ 1.62312675e+00, -1.81044471e+00,  7.99488246e-01,\n",
      "        -1.36211187e-01,  8.39652598e-01, -6.95003271e-02,\n",
      "         1.33716774e+00, -7.41556227e-01],\n",
      "       [-4.70179379e-01,  6.72662616e-01, -8.99235532e-02,\n",
      "        -3.37594002e-01, -6.83680952e-01, -1.12733416e-01,\n",
      "        -7.90386736e-01,  1.08105958e+00],\n",
      "       [-1.14619052e+00, -6.08941197e-01, -7.71592140e-01,\n",
      "        -1.57526657e-02,  3.47903460e-01, -4.17261571e-03,\n",
      "        -1.35026944e+00,  1.23086357e+00],\n",
      "       [-1.18572712e+00, -9.29342151e-01,  2.32304955e+01,\n",
      "         2.56380806e+01, -1.22079432e+00, -1.63276598e-01,\n",
      "        -7.81055331e-01,  2.53915215e+00],\n",
      "       [-7.67662644e-01, -2.88540244e-01, -7.74500906e-01,\n",
      "         2.13988554e-02,  2.25683355e+00,  8.89070332e-02,\n",
      "        -8.65037739e-01,  7.06549466e-01]], dtype=float32)>, <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[1.843  ],\n",
      "       [2.894  ],\n",
      "       [0.762  ],\n",
      "       [5.00001],\n",
      "       [1.029  ],\n",
      "       [1.273  ],\n",
      "       [0.844  ],\n",
      "       [1.038  ],\n",
      "       [2.3    ],\n",
      "       [5.00001],\n",
      "       [0.873  ],\n",
      "       [1.453  ],\n",
      "       [0.933  ],\n",
      "       [0.889  ],\n",
      "       [1.336  ],\n",
      "       [1.546  ],\n",
      "       [1.808  ],\n",
      "       [1.231  ],\n",
      "       [0.858  ],\n",
      "       [1.861  ],\n",
      "       [1.06   ],\n",
      "       [2.427  ],\n",
      "       [1.412  ],\n",
      "       [3.25   ],\n",
      "       [5.00001],\n",
      "       [1.121  ],\n",
      "       [2.045  ],\n",
      "       [2.631  ],\n",
      "       [1.321  ],\n",
      "       [1.     ],\n",
      "       [0.875  ],\n",
      "       [1.537  ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for i in tfrecords_train_set.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.7782 - val_loss: 0.9355\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4978 - val_loss: 0.5264\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4468 - val_loss: 0.4775\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4389 - val_loss: 0.4679\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4110 - val_loss: 0.4420\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4113 - val_loss: 0.4411\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4104 - val_loss: 0.4407\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3922 - val_loss: 0.4281\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3963 - val_loss: 0.4150\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3909 - val_loss: 0.4162\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3917 - val_loss: 0.4136\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3893 - val_loss: 0.4065\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3972 - val_loss: 0.4205\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.4075\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-2)]\n",
    "\n",
    "# 当是BatchDataset,必须制定steps_per_epoch，validation_steps\n",
    "history = model.fit(tfrecords_train_set,\n",
    "                    validation_data = tfrecords_valid_set,\n",
    "                    steps_per_epoch = 11160 // batch_size,\n",
    "                    validation_steps = 3870 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step - loss: 0.4466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44655656814575195"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfrecords_test_set, steps = 5160 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
