{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport sklearn\nimport pandas as pd\nimport os\nimport sys\nimport time\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\nprint(tf.__version__)\nprint(sys.version_info)\nfor module in mpl, np, pd, sklearn, tf, keras:\n    print(module.__name__, module.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:04.342576Z","iopub.execute_input":"2022-05-19T13:41:04.342965Z","iopub.status.idle":"2022-05-19T13:41:10.574487Z","shell.execute_reply.started":"2022-05-19T13:41:04.342836Z","shell.execute_reply":"2022-05-19T13:41:10.573753Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.6.3\nsys.version_info(major=3, minor=7, micro=12, releaselevel='final', serial=0)\nmatplotlib 3.5.2\nnumpy 1.21.6\npandas 1.3.5\nsklearn 1.0.2\ntensorflow 2.6.3\nkeras.api._v2.keras 2.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\ninput_filepath = \"../input/shakespeare/shakespeare.txt\"\ntext = open(input_filepath, 'r').read()\n\nprint(len(text))\nprint(text[0:100])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:10.576080Z","iopub.execute_input":"2022-05-19T13:41:10.576411Z","iopub.status.idle":"2022-05-19T13:41:10.604662Z","shell.execute_reply.started":"2022-05-19T13:41:10.576372Z","shell.execute_reply":"2022-05-19T13:41:10.603994Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"1115393\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1. generate vocab\n# 2. build mapping char->id\n# 3. data -> id_data  把数据都转为id\n# 4. abcd -> bcd<eos>  预测下一个字符生成的模型，也就是输入是a，输出就是b\n\n# 去重，留下独立字符，并排序\nvocab = sorted(set(text))\nprint(len(vocab))\nprint(vocab)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:10.605759Z","iopub.execute_input":"2022-05-19T13:41:10.606430Z","iopub.status.idle":"2022-05-19T13:41:10.622525Z","shell.execute_reply.started":"2022-05-19T13:41:10.606395Z","shell.execute_reply":"2022-05-19T13:41:10.621703Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"65\n['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","output_type":"stream"}]},{"cell_type":"code","source":"# 体会enumerate接口的作用\nfor idx,char in enumerate(['how','are','you']):\n    print(idx,char)\nfor i in enumerate(['how','are','you']):\n    print(i)\nprint(type(enumerate(['how','are','you'])))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:10.625148Z","iopub.execute_input":"2022-05-19T13:41:10.625842Z","iopub.status.idle":"2022-05-19T13:41:10.634092Z","shell.execute_reply.started":"2022-05-19T13:41:10.625804Z","shell.execute_reply":"2022-05-19T13:41:10.633211Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"0 how\n1 are\n2 you\n(0, 'how')\n(1, 'are')\n(2, 'you')\n<class 'enumerate'>\n","output_type":"stream"}]},{"cell_type":"code","source":"# enumerate对每一个位置处字符编号\n# 迭代器enumerate，其中每个元素都是元组(,)，下面是字典生成式\nchar2idx = {char:idx for idx, char in enumerate(vocab)}\nprint(char2idx)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:10.635391Z","iopub.execute_input":"2022-05-19T13:41:10.635783Z","iopub.status.idle":"2022-05-19T13:41:10.641813Z","shell.execute_reply.started":"2022-05-19T13:41:10.635747Z","shell.execute_reply":"2022-05-19T13:41:10.640953Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n","output_type":"stream"}]},{"cell_type":"code","source":"# 把vocab从列表变为ndarray\nidx2char = np.array(vocab)\nprint(idx2char)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:10.643332Z","iopub.execute_input":"2022-05-19T13:41:10.643893Z","iopub.status.idle":"2022-05-19T13:41:10.650472Z","shell.execute_reply.started":"2022-05-19T13:41:10.643857Z","shell.execute_reply":"2022-05-19T13:41:10.649147Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n","output_type":"stream"}]},{"cell_type":"code","source":"# 把字符都转换为id\ntext_as_int = np.array([char2idx[c] for c in text])\nprint(text_as_int.shape)\nprint(len(text_as_int))\nprint(text_as_int[0:10])\nprint(text[0:10])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:10.651898Z","iopub.execute_input":"2022-05-19T13:41:10.652210Z","iopub.status.idle":"2022-05-19T13:41:10.848780Z","shell.execute_reply.started":"2022-05-19T13:41:10.652116Z","shell.execute_reply":"2022-05-19T13:41:10.848005Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(1115393,)\n1115393\n[18 47 56 57 58  1 15 47 58 47]\nFirst Citi\n","output_type":"stream"}]},{"cell_type":"code","source":"# 把输入和输出分配好（细操）\ndef split_input_target(id_text):\n    \"\"\"\n    abcde -> abcd, bcde,输入是abcd，输出是bcde\n    \"\"\"\n    return id_text[0:-1], id_text[1:]\n\n# 把id_text转换为 dataset\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nseq_length = 100\n# 做一个batch，seq_length + 1目的是我们输入是5个字符时，输出是4\n# drop_remainder是最后不够就丢掉，这个batch是把字变为句子，一个句子是101个字符\nseq_dataset = char_dataset.batch(seq_length + 1,\n                                 drop_remainder = True)\nfor ch_id in char_dataset.take(2):\n    print(ch_id, idx2char[ch_id.numpy()])\n\n# seq_dataset 每一个都是句子（id组成）\nfor seq_id in seq_dataset.take(2):\n    print(seq_id)\n    print(repr(''.join(idx2char[seq_id.numpy()])))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:10.850103Z","iopub.execute_input":"2022-05-19T13:41:10.850373Z","iopub.status.idle":"2022-05-19T13:41:13.690528Z","shell.execute_reply.started":"2022-05-19T13:41:10.850339Z","shell.execute_reply":"2022-05-19T13:41:13.689326Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tf.Tensor(18, shape=(), dtype=int64) F\ntf.Tensor(47, shape=(), dtype=int64) i\ntf.Tensor(\n[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n  0 37 53 59  1], shape=(101,), dtype=int64)\n'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\ntf.Tensor(\n[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n 63 53 59  1 49], shape=(101,), dtype=int64)\n'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","output_type":"stream"},{"name":"stderr","text":"2022-05-19 13:41:10.911557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:11.036119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:11.036885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:11.042923: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-19 13:41:11.043233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:11.043946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:11.044612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:13.310619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:13.311495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:13.312181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-05-19 13:41:13.313727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# 然后通过split_input_target函数来对seq_dataset做映射，得到输入，输出\nseq_dataset = seq_dataset.map(split_input_target)\n\nfor item_input, item_output in seq_dataset.take(2):\n    print(item_input)\n    print(item_output)\nprint(seq_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:13.691882Z","iopub.execute_input":"2022-05-19T13:41:13.692122Z","iopub.status.idle":"2022-05-19T13:41:13.821031Z","shell.execute_reply.started":"2022-05-19T13:41:13.692089Z","shell.execute_reply":"2022-05-19T13:41:13.820365Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n  0 37 53 59], shape=(100,), dtype=int64)\ntf.Tensor(\n[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n 37 53 59  1], shape=(100,), dtype=int64)\ntf.Tensor(\n[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n 63 53 59  1], shape=(100,), dtype=int64)\ntf.Tensor(\n[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n 53 59  1 49], shape=(100,), dtype=int64)\n<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n","output_type":"stream"},{"name":"stderr","text":"2022-05-19 13:41:13.781996: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 64\nbuffer_size = 10000\n# 这个batch是真正的batch，上一个batch是把字变为句子，buffer_size是从数据集拿多少元素\nseq_dataset = seq_dataset.shuffle(buffer_size).batch(\n    batch_size, drop_remainder=True)\nprint(seq_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:13.824708Z","iopub.execute_input":"2022-05-19T13:41:13.824898Z","iopub.status.idle":"2022-05-19T13:41:13.834607Z","shell.execute_reply.started":"2022-05-19T13:41:13.824875Z","shell.execute_reply":"2022-05-19T13:41:13.833888Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 256  # 资料比较小，所以dim可以设大一些\nrnn_units = 1024\n\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    model = keras.models.Sequential([\n        keras.layers.Embedding(vocab_size, embedding_dim,\n                               batch_input_shape = [batch_size, None]),\n        # return_sequences是指要返回一个序列，也就是所有输出，而不是最后一个\n        keras.layers.SimpleRNN(units = rnn_units,\n                               stateful = True,  # 是否把上一批最后返回的状态添加到下一批作为输入\n                               recurrent_initializer = 'glorot_uniform',\n                               return_sequences = True),\n        # 全连接层，为什么最后一层全连接层的输出是vocab_size\n        keras.layers.Dense(vocab_size),\n    ])\n    return model\n\nmodel = build_model(\n    vocab_size = vocab_size,\n    embedding_dim = embedding_dim,\n    rnn_units = rnn_units,\n    batch_size = batch_size)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:13.835941Z","iopub.execute_input":"2022-05-19T13:41:13.836344Z","iopub.status.idle":"2022-05-19T13:41:13.947093Z","shell.execute_reply.started":"2022-05-19T13:41:13.836311Z","shell.execute_reply":"2022-05-19T13:41:13.945991Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           16640     \n_________________________________________________________________\nsimple_rnn (SimpleRNN)       (64, None, 1024)          1311744   \n_________________________________________________________________\ndense (Dense)                (64, None, 65)            66625     \n=================================================================\nTotal params: 1,395,009\nTrainable params: 1,395,009\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.variables","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:13.948366Z","iopub.execute_input":"2022-05-19T13:41:13.948591Z","iopub.status.idle":"2022-05-19T13:41:13.964878Z","shell.execute_reply.started":"2022-05-19T13:41:13.948560Z","shell.execute_reply":"2022-05-19T13:41:13.964156Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[<tf.Variable 'embedding/embeddings:0' shape=(65, 256) dtype=float32, numpy=\n array([[ 0.01919475, -0.01261197,  0.0445695 , ..., -0.02104477,\n         -0.0435562 , -0.00419811],\n        [-0.03338762, -0.01135081, -0.02068409, ...,  0.03816238,\n          0.0497164 ,  0.02503288],\n        [ 0.03240658, -0.01093385, -0.0321333 , ...,  0.00417049,\n         -0.03226669, -0.01414679],\n        ...,\n        [-0.0222993 ,  0.01016422, -0.00263877, ..., -0.02878145,\n         -0.04438659,  0.00964663],\n        [ 0.04537973, -0.02534884,  0.02271712, ..., -0.03358071,\n          0.02241695,  0.01282773],\n        [ 0.04327709, -0.02299103, -0.00440223, ..., -0.03154824,\n          0.02054248,  0.0226951 ]], dtype=float32)>,\n <tf.Variable 'simple_rnn/simple_rnn_cell/kernel:0' shape=(256, 1024) dtype=float32, numpy=\n array([[-0.00702935, -0.06676127, -0.04407291, ..., -0.01047852,\n          0.06464547,  0.02131308],\n        [ 0.03347615,  0.00835491, -0.04214556, ...,  0.03264932,\n          0.00427552,  0.04916932],\n        [-0.04383016,  0.02494804, -0.00468877, ...,  0.06149784,\n          0.04558988,  0.04059843],\n        ...,\n        [ 0.01796979, -0.0626496 , -0.01125001, ...,  0.05247438,\n         -0.00811417,  0.05554629],\n        [ 0.03999187, -0.04674409, -0.05414251, ..., -0.00589076,\n          0.00367179,  0.02244417],\n        [ 0.00449808,  0.04852279,  0.03490721, ...,  0.03609563,\n         -0.02270347, -0.05728196]], dtype=float32)>,\n <tf.Variable 'simple_rnn/simple_rnn_cell/recurrent_kernel:0' shape=(1024, 1024) dtype=float32, numpy=\n array([[ 0.02421548, -0.01174109, -0.04815859, ...,  0.05356598,\n         -0.00636939, -0.02555483],\n        [-0.0427412 ,  0.04989083,  0.01327011, ..., -0.04620437,\n         -0.00629669,  0.00835338],\n        [ 0.00901167, -0.01241816,  0.0082479 , ...,  0.01869178,\n         -0.04211604,  0.01998165],\n        ...,\n        [ 0.01860237, -0.00664373,  0.02863992, ..., -0.00511583,\n          0.04739099,  0.03821931],\n        [ 0.01517547,  0.04576007, -0.0068527 , ...,  0.01183624,\n          0.00694099, -0.03676863],\n        [ 0.01248327, -0.02310496,  0.02598545, ..., -0.05200769,\n         -0.00638099, -0.01007497]], dtype=float32)>,\n <tf.Variable 'simple_rnn/simple_rnn_cell/bias:0' shape=(1024,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n <tf.Variable 'dense/kernel:0' shape=(1024, 65) dtype=float32, numpy=\n array([[-0.03607105,  0.0294276 , -0.05800034, ...,  0.06801142,\n          0.01942769, -0.06831937],\n        [ 0.04626859,  0.05140728, -0.06153907, ..., -0.04916637,\n         -0.02863419,  0.07083519],\n        [ 0.04651047, -0.06790084,  0.05826864, ...,  0.00603582,\n          0.05543274, -0.02000289],\n        ...,\n        [ 0.04134753,  0.01752374,  0.00153436, ..., -0.03290462,\n          0.01912764,  0.03932064],\n        [ 0.0080529 , -0.0300771 , -0.06604816, ...,  0.04114357,\n          0.00199729, -0.0306863 ],\n        [-0.06460659,  0.01991381,  0.02543627, ...,  0.0140439 ,\n         -0.04882559, -0.05738093]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(65,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32)>]"},"metadata":{}}]},{"cell_type":"code","source":"for input_example_batch, target_example_batch in seq_dataset.take(1):\n    # 把model当函数来用，实际是调用类的call方法\n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:13.966285Z","iopub.execute_input":"2022-05-19T13:41:13.966530Z","iopub.status.idle":"2022-05-19T13:41:16.700550Z","shell.execute_reply.started":"2022-05-19T13:41:13.966497Z","shell.execute_reply":"2022-05-19T13:41:16.699768Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(64, 100, 65)\n","output_type":"stream"}]},{"cell_type":"code","source":"example_batch_predictions[0][0]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:16.702962Z","iopub.execute_input":"2022-05-19T13:41:16.703448Z","iopub.status.idle":"2022-05-19T13:41:16.715344Z","shell.execute_reply.started":"2022-05-19T13:41:16.703410Z","shell.execute_reply":"2022-05-19T13:41:16.714247Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(65,), dtype=float32, numpy=\narray([ 0.0113624 , -0.0326306 ,  0.04214111, -0.01659971, -0.01778518,\n       -0.00654469, -0.03008696,  0.0028754 , -0.00301468, -0.03413392,\n       -0.00217845,  0.04114346,  0.05130856, -0.02169739, -0.00398294,\n       -0.00693167,  0.01020009,  0.04199206,  0.01656809,  0.00454826,\n        0.0169168 , -0.01643466,  0.03392875, -0.02004773,  0.00881675,\n       -0.01126692,  0.03157157, -0.00883924,  0.00421403, -0.03911354,\n       -0.02010616, -0.03110015, -0.02959238, -0.00530336,  0.01399101,\n       -0.04115472, -0.00500408,  0.04638671,  0.04460899,  0.00499319,\n        0.00623027, -0.00924729, -0.02157588, -0.05315569,  0.00062216,\n       -0.04730015, -0.01823638, -0.02424935, -0.03827185,  0.0378241 ,\n       -0.00108806,  0.02002366,  0.03210833, -0.02465087, -0.00871665,\n       -0.00246845,  0.01842871,  0.00327547,  0.02961155,  0.02823824,\n        0.06209011, -0.01669264, -0.0037242 , -0.00405848, -0.00357904],\n      dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"example_batch_predictions[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:16.717179Z","iopub.execute_input":"2022-05-19T13:41:16.717611Z","iopub.status.idle":"2022-05-19T13:41:16.724932Z","shell.execute_reply.started":"2022-05-19T13:41:16.717576Z","shell.execute_reply":"2022-05-19T13:41:16.724052Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(100, 65), dtype=float32, numpy=\narray([[ 0.0113624 , -0.0326306 ,  0.04214111, ..., -0.0037242 ,\n        -0.00405848, -0.00357904],\n       [-0.01187989, -0.05884358, -0.04518011, ...,  0.01021862,\n         0.0156815 , -0.01187882],\n       [ 0.04486758, -0.01728885, -0.00642166, ..., -0.05024748,\n         0.01857883, -0.03882169],\n       ...,\n       [ 0.22144863,  0.04864826, -0.265059  , ..., -0.06042609,\n        -0.01507821, -0.00308478],\n       [-0.0110305 ,  0.11021354, -0.14840022, ...,  0.14240268,\n         0.15840203,  0.06076766],\n       [ 0.5676479 ,  0.24685271, -0.27520013, ...,  0.11362644,\n        -0.09338273, -0.1906892 ]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"code","source":"# random sampling.\n# greedy, random.\n# logits是计算分类任务之前，没有经过softmax的那个值就是logits\n# tf.random.categorical从分类分布中抽取样本,随机为了每次写文章有差异\nprint(example_batch_predictions[0][0])\nprint('-'*100)\nsample_indices = tf.random.categorical(\n    logits = example_batch_predictions[0], num_samples = 1,seed=1)\nprint(sample_indices)  # 这里的维度是（100,1）\nprint('-'*100)\n# (100, 1) -> (100, )  调用squeeze 去除1的维度，变为100的向量\nsample_indices = tf.squeeze(sample_indices, axis = -1)\nprint(sample_indices)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:16.726460Z","iopub.execute_input":"2022-05-19T13:41:16.727012Z","iopub.status.idle":"2022-05-19T13:41:16.740367Z","shell.execute_reply.started":"2022-05-19T13:41:16.726974Z","shell.execute_reply":"2022-05-19T13:41:16.739485Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"tf.Tensor(\n[ 0.0113624  -0.0326306   0.04214111 -0.01659971 -0.01778518 -0.00654469\n -0.03008696  0.0028754  -0.00301468 -0.03413392 -0.00217845  0.04114346\n  0.05130856 -0.02169739 -0.00398294 -0.00693167  0.01020009  0.04199206\n  0.01656809  0.00454826  0.0169168  -0.01643466  0.03392875 -0.02004773\n  0.00881675 -0.01126692  0.03157157 -0.00883924  0.00421403 -0.03911354\n -0.02010616 -0.03110015 -0.02959238 -0.00530336  0.01399101 -0.04115472\n -0.00500408  0.04638671  0.04460899  0.00499319  0.00623027 -0.00924729\n -0.02157588 -0.05315569  0.00062216 -0.04730015 -0.01823638 -0.02424935\n -0.03827185  0.0378241  -0.00108806  0.02002366  0.03210833 -0.02465087\n -0.00871665 -0.00246845  0.01842871  0.00327547  0.02961155  0.02823824\n  0.06209011 -0.01669264 -0.0037242  -0.00405848 -0.00357904], shape=(65,), dtype=float32)\n----------------------------------------------------------------------------------------------------\ntf.Tensor(\n[[31]\n [63]\n [64]\n [23]\n [51]\n [15]\n [40]\n [ 1]\n [48]\n [64]\n [17]\n [44]\n [59]\n [ 5]\n [40]\n [44]\n [ 3]\n [46]\n [11]\n [53]\n [56]\n [18]\n [ 6]\n [25]\n [ 8]\n [ 5]\n [41]\n [19]\n [15]\n [38]\n [63]\n [ 7]\n [18]\n [29]\n [56]\n [64]\n [44]\n [56]\n [31]\n [64]\n [54]\n [41]\n [45]\n [34]\n [61]\n [41]\n [16]\n [45]\n [32]\n [40]\n [28]\n [62]\n [56]\n [18]\n [35]\n [44]\n [44]\n [58]\n [58]\n [53]\n [53]\n [29]\n [27]\n [32]\n [28]\n [10]\n [36]\n [56]\n [46]\n [24]\n [23]\n [28]\n [ 0]\n [62]\n [24]\n [ 6]\n [ 6]\n [62]\n [18]\n [32]\n [48]\n [18]\n [13]\n [13]\n [48]\n [34]\n [23]\n [ 1]\n [ 7]\n [50]\n [55]\n [53]\n [63]\n [25]\n [ 4]\n [51]\n [61]\n [29]\n [58]\n [39]], shape=(100, 1), dtype=int64)\n----------------------------------------------------------------------------------------------------\ntf.Tensor(\n[31 63 64 23 51 15 40  1 48 64 17 44 59  5 40 44  3 46 11 53 56 18  6 25\n  8  5 41 19 15 38 63  7 18 29 56 64 44 56 31 64 54 41 45 34 61 41 16 45\n 32 40 28 62 56 18 35 44 44 58 58 53 53 29 27 32 28 10 36 56 46 24 23 28\n  0 62 24  6  6 62 18 32 48 18 13 13 48 34 23  1  7 50 55 53 63 25  4 51\n 61 29 58 39], shape=(100,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 理解random.categorical，虽然是随机的，但是还是偏向于概率较大的值\n# num_samples，意味着我们搞几次抽样\nfor i in tf.range(5):\n    samples = tf.random.categorical([[4.0,2.0,2.0,2.0,1.0]], 3)\n    tf.print(samples)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:16.741502Z","iopub.execute_input":"2022-05-19T13:41:16.741881Z","iopub.status.idle":"2022-05-19T13:41:16.760171Z","shell.execute_reply.started":"2022-05-19T13:41:16.741846Z","shell.execute_reply":"2022-05-19T13:41:16.759496Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[[0 0 0]]\n[[0 0 0]]\n[[3 0 0]]\n[[3 0 0]]\n[[0 0 0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Input: \", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Output: \", repr(\"\".join(idx2char[target_example_batch[0]])))\nprint()\nprint(\"Predictions: \", repr(\"\".join(idx2char[sample_indices])))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:16.761318Z","iopub.execute_input":"2022-05-19T13:41:16.761567Z","iopub.status.idle":"2022-05-19T13:41:16.770711Z","shell.execute_reply.started":"2022-05-19T13:41:16.761534Z","shell.execute_reply":"2022-05-19T13:41:16.769923Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Input:  'ly in both, if justice had her right.\\n\\nLORD ROSS:\\nMy heart is great; but it must break with silence,'\n\nOutput:  'y in both, if justice had her right.\\n\\nLORD ROSS:\\nMy heart is great; but it must break with silence,\\n'\n\nPredictions:  \"SyzKmCb jzEfu'bf$h;orF,M.'cGCZy-FQrzfrSzpcgVwcDgTbPxrFWffttooQOTP:XrhLKP\\nxL,,xFTjFAAjVK -lqoyM&mwQta\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# from_logits是否预期为对数张量。默认情况下，我们假设对概率分布进行编码\n# logits表示网络的直接输出 。没经过sigmoid或者softmax的概率化。\n# from_logits=False就表示把已经概率化了的输出，重新映射回原值。log（p/(1-p)）\ndef loss(labels, logits):\n    return keras.losses.sparse_categorical_crossentropy(\n        labels, logits, from_logits=True)\n\nmodel.compile(optimizer = 'adam', loss = loss)\nexample_loss = loss(target_example_batch, example_batch_predictions)\nprint(example_loss.shape)\nprint(example_loss.numpy().mean())  #看下样例的loss","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:16.771969Z","iopub.execute_input":"2022-05-19T13:41:16.772557Z","iopub.status.idle":"2022-05-19T13:41:16.794182Z","shell.execute_reply.started":"2022-05-19T13:41:16.772519Z","shell.execute_reply":"2022-05-19T13:41:16.793541Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(64, 100)\n4.1827135\n","output_type":"stream"}]},{"cell_type":"code","source":"# 定义一个文件夹，保存模型\noutput_dir = \"./text_generation_checkpoints\"\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\ncheckpoint_prefix = os.path.join(output_dir, 'ckpt_{epoch}')\ncheckpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath = checkpoint_prefix,\n    # 只保存权重的值\n    save_weights_only = True)\n\nepochs = 100\nhistory = model.fit(seq_dataset, epochs = epochs,\n                    callbacks = [checkpoint_callback])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T13:41:16.795217Z","iopub.execute_input":"2022-05-19T13:41:16.795579Z","iopub.status.idle":"2022-05-19T14:09:50.573819Z","shell.execute_reply.started":"2022-05-19T13:41:16.795548Z","shell.execute_reply":"2022-05-19T14:09:50.572997Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/100\n172/172 [==============================] - 14s 69ms/step - loss: 2.7582\nEpoch 2/100\n172/172 [==============================] - 13s 64ms/step - loss: 2.0147\nEpoch 3/100\n172/172 [==============================] - 13s 66ms/step - loss: 1.8030\nEpoch 4/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.6723\nEpoch 5/100\n172/172 [==============================] - 13s 63ms/step - loss: 1.5873\nEpoch 6/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.5276\nEpoch 7/100\n172/172 [==============================] - 13s 65ms/step - loss: 1.4838\nEpoch 8/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.4515\nEpoch 9/100\n172/172 [==============================] - 13s 63ms/step - loss: 1.4218\nEpoch 10/100\n172/172 [==============================] - 13s 69ms/step - loss: 1.3968\nEpoch 11/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.3776\nEpoch 12/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.3596\nEpoch 13/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.3407\nEpoch 14/100\n172/172 [==============================] - 13s 66ms/step - loss: 1.3261\nEpoch 15/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.3114\nEpoch 16/100\n172/172 [==============================] - 14s 69ms/step - loss: 1.2974\nEpoch 17/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.2845\nEpoch 18/100\n172/172 [==============================] - 13s 63ms/step - loss: 1.2710\nEpoch 19/100\n172/172 [==============================] - 13s 65ms/step - loss: 1.2598\nEpoch 20/100\n172/172 [==============================] - 13s 65ms/step - loss: 1.2480\nEpoch 21/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.2368\nEpoch 22/100\n172/172 [==============================] - 13s 63ms/step - loss: 1.2237\nEpoch 23/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.2132\nEpoch 24/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.2027\nEpoch 25/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.1920\nEpoch 26/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.1822\nEpoch 27/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.1720\nEpoch 28/100\n172/172 [==============================] - 12s 64ms/step - loss: 1.1619\nEpoch 29/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.1530\nEpoch 30/100\n172/172 [==============================] - 13s 65ms/step - loss: 1.1442\nEpoch 31/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.1351\nEpoch 32/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.1278\nEpoch 33/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.1182\nEpoch 34/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.1101\nEpoch 35/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.1030\nEpoch 36/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0982\nEpoch 37/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0910\nEpoch 38/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0850\nEpoch 39/100\n172/172 [==============================] - 12s 63ms/step - loss: 1.0794\nEpoch 40/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0730\nEpoch 41/100\n172/172 [==============================] - 14s 71ms/step - loss: 1.0706\nEpoch 42/100\n172/172 [==============================] - 13s 65ms/step - loss: 1.0657\nEpoch 43/100\n172/172 [==============================] - 13s 63ms/step - loss: 1.0593\nEpoch 44/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0562\nEpoch 45/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0525\nEpoch 46/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0510\nEpoch 47/100\n172/172 [==============================] - 13s 66ms/step - loss: 1.0464\nEpoch 48/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0445\nEpoch 49/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0429\nEpoch 50/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0399\nEpoch 51/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0408\nEpoch 52/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0391\nEpoch 53/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0350\nEpoch 54/100\n172/172 [==============================] - 13s 66ms/step - loss: 1.0377\nEpoch 55/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0353\nEpoch 56/100\n172/172 [==============================] - 13s 65ms/step - loss: 1.0321\nEpoch 57/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0336\nEpoch 58/100\n172/172 [==============================] - 12s 63ms/step - loss: 1.0345\nEpoch 59/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0353\nEpoch 60/100\n172/172 [==============================] - 13s 65ms/step - loss: 1.0353\nEpoch 61/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0372\nEpoch 62/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0339\nEpoch 63/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0375\nEpoch 64/100\n172/172 [==============================] - 14s 68ms/step - loss: 1.0363\nEpoch 65/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0394\nEpoch 66/100\n172/172 [==============================] - 14s 69ms/step - loss: 1.0404\nEpoch 67/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0427\nEpoch 68/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0427\nEpoch 69/100\n172/172 [==============================] - 12s 62ms/step - loss: 1.0469\nEpoch 70/100\n172/172 [==============================] - 13s 63ms/step - loss: 1.0459\nEpoch 71/100\n172/172 [==============================] - 13s 66ms/step - loss: 1.0484\nEpoch 72/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0503\nEpoch 73/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0509\nEpoch 74/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0537\nEpoch 75/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0571\nEpoch 76/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0618\nEpoch 77/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0635\nEpoch 78/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0658\nEpoch 79/100\n172/172 [==============================] - 13s 68ms/step - loss: 1.0691\nEpoch 80/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0715\nEpoch 81/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0742\nEpoch 82/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0773\nEpoch 83/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0807\nEpoch 84/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0825\nEpoch 85/100\n172/172 [==============================] - 12s 63ms/step - loss: 1.0866\nEpoch 86/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.0899\nEpoch 87/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.0920\nEpoch 88/100\n172/172 [==============================] - 13s 69ms/step - loss: 1.0969\nEpoch 89/100\n172/172 [==============================] - 13s 63ms/step - loss: 1.1041\nEpoch 90/100\n172/172 [==============================] - 14s 71ms/step - loss: 1.1065\nEpoch 91/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.1064\nEpoch 92/100\n172/172 [==============================] - 14s 68ms/step - loss: 1.1109\nEpoch 93/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.1166\nEpoch 94/100\n172/172 [==============================] - 14s 69ms/step - loss: 1.1174\nEpoch 95/100\n172/172 [==============================] - 12s 63ms/step - loss: 1.1229\nEpoch 96/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.1316\nEpoch 97/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.1379\nEpoch 98/100\n172/172 [==============================] - 13s 67ms/step - loss: 1.1347\nEpoch 99/100\n172/172 [==============================] - 13s 64ms/step - loss: 1.1449\nEpoch 100/100\n172/172 [==============================] - 13s 69ms/step - loss: 1.1480\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.train.latest_checkpoint(output_dir)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:09:50.575182Z","iopub.execute_input":"2022-05-19T14:09:50.575467Z","iopub.status.idle":"2022-05-19T14:09:50.582469Z","shell.execute_reply.started":"2022-05-19T14:09:50.575430Z","shell.execute_reply":"2022-05-19T14:09:50.581661Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'./text_generation_checkpoints/ckpt_100'"},"metadata":{}}]},{"cell_type":"code","source":"output_dir = \"./text_generation_checkpoints\"\nmodel2 = build_model(vocab_size,\n                     embedding_dim,\n                     rnn_units,\n                     batch_size = 1)\nmodel2.load_weights(tf.train.latest_checkpoint(output_dir))\n# 1是一个样本，None是可以变长序列\n# model2.build(tf.TensorShape([1, None]))\n\n# 下面是文本生成的流程\n# start ch sequence A, \n# A -> model -> b  A放入模型后得到b\n# A.append(b) -> B\n# B(Ab) -> model -> c\n# B.append(c) -> C\n# C(Abc) -> model -> ...\nmodel2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:09:50.584181Z","iopub.execute_input":"2022-05-19T14:09:50.584468Z","iopub.status.idle":"2022-05-19T14:09:50.673947Z","shell.execute_reply.started":"2022-05-19T14:09:50.584435Z","shell.execute_reply":"2022-05-19T14:09:50.673299Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (1, None, 256)            16640     \n_________________________________________________________________\nsimple_rnn_1 (SimpleRNN)     (1, None, 1024)           1311744   \n_________________________________________________________________\ndense_1 (Dense)              (1, None, 65)             66625     \n=================================================================\nTotal params: 1,395,009\nTrainable params: 1,395,009\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 文本生成重要部分","metadata":{}},{"cell_type":"code","source":"# 定义一个函数来实现上面的文本生成流程\ndef generate_text(model, start_string, num_generate = 1000):\n    # 这一次输出的是1维的,把字母变为对应的id\n    input_eval = [char2idx[ch] for ch in start_string]\n    print(input_eval)\n    # 做一个维度扩展\n    input_eval = tf.expand_dims(input_eval, 0)\n    print(input_eval)\n    text_generated = []\n    # 对model进行reset，连续调用的时候使用resets_states()\n    model.reset_states()\n    \n    for _ in range(num_generate):\n        # 1. model inference -> predictions\n        # 2. sample -> ch -> text_generated.\n        # 3. update input_eval\n        \n        # predictions : [batch_size, input_eval_len, vocab_size]\n        predictions = model(input_eval)\n#         print(predictions.shape)\n        # squeeze消掉 batch_size，变为predictions : [input_eval_len, vocab_size]\n        predictions = tf.squeeze(predictions, 0)\n        # predicted_ids: [input_eval_len, 1]\n        # a b c -> b c d\n#         print(predictions)\n        # 把predictions: [input_eval_len, vocab_size]维度数据变为 1个维度\n        predicted_id = tf.random.categorical(\n            predictions, num_samples = 1)[-1, 0].numpy()  # 65个logits得到是什么类别\n#         print(predicted_id)\n        # 得到预测id后，放入text_generated\n        text_generated.append(idx2char[predicted_id])\n        # 下面这是是我们原来的公式,为什么没有append作为新的输入,因为那样比较低效\n        # s, x -> rnn -> s', y\n        input_eval = tf.expand_dims([predicted_id], 0)\n#         print(input_eval)\n    return start_string + ''.join(text_generated)\n\nnew_text = generate_text(model2, \"All: \")\nprint(new_text)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:09:50.675311Z","iopub.execute_input":"2022-05-19T14:09:50.675568Z","iopub.status.idle":"2022-05-19T14:09:55.095165Z","shell.execute_reply.started":"2022-05-19T14:09:50.675533Z","shell.execute_reply":"2022-05-19T14:09:55.093662Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[13, 50, 50, 10, 1]\ntf.Tensor([[13 50 50 10  1]], shape=(1, 5), dtype=int32)\nAll: what lies your numbla?\n\nDUKE VINCENTIO:\nIs she should seek it,\nPardon men\nHow it still boyst-blown.\n\nQUEEN:\nWith all your holy bond\nTo his molefily.\nWhat simples of twice\nO, yet the eyest is face,\nThat, in the eyes:\nHear'sts hath wounds the watery unto the lords and plain.\n\nMENENIUS:\nAy, were I think, Katian you?' arm, when he married.\n\nMENENIUS:\nDo not frown in hell,\nWave justice, sty wedve love,\nBut night; the loss ingrorance.\n\nWARWICK:\nO,\nSP year to his royal-or\nAnd put or crustice:\nHow far I see the trotis agrees,\nPut on others, belly born\nHis stare of rarewing close Phants are sweet to do,\nHow many me annage: therefore lest have heart till both a\nmakety unto the subject lim\ndespite of the horsem'd: take your speed.\nIn fanding was worthbowns; at the thorns\nTo frantica fortunes\nDo gleatent this be lord's own belmon can obseement of the notestoies to this place.\n\nMENENIUS:\nIs aw the castl.\nThus I shall see the tempest of my mouth,\nThat one that they are prepared yestlate hath follow'\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}