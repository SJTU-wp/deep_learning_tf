{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里正则表达式中的^：一直吃直到......（可以理解为“取反”）\n",
    "b = \"I am a[]=+ man!. ? &***()\"\n",
    "b = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", b)\n",
    "print(b)\n",
    "c = '12345 im asn'\n",
    "c = re.sub(r\"[^a-z]+\", \"\", c)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为西班牙语有一些是特殊字符，所以需要unicode转ascii\n",
    "# 这样值变小了，因为unicode太大\n",
    "def unicode_to_ascii(s):\n",
    "    # NFD是转换方法，把每一个字节拆开，Mn是重音，所以去除\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# 简单测试\n",
    "# 加u代表对字符串进行unicode编码\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    # 变为小写，去掉多余的空格\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # eg: \"he is a boy.\" => \"he is a boy . \"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    # 因为可能有多余空格，替换为一个空格，所以处理一下\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格（上一个cell中有演示）\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))  # ¿是占用两个字节的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data_spa_en/spa.txt'\n",
    "\n",
    "# 1. Remove the accents  移除口音\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]--> 前面西班牙，后面英文\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    # word_pairs是二维列表\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    return zip(*word_pairs)\n",
    "\n",
    "en, sp = create_dataset(data_path, None)\n",
    "print(type(en))\n",
    "print(type(sp))\n",
    "print(len(en))\n",
    "print(en[-1])\n",
    "print(sp[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip演示\n",
    "a=[ [1, 2],[3,4],[5,6] ]\n",
    "c,d=zip(*a)  # *：解包\n",
    "print(c,d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    # Tokenizer帮我们把词语式的转换为id式的，filters是黑名单\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    # 把一个一个的单词变为id\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # 做padding，向最大的补齐\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs，目前是西班牙语翻译为英语\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)  # 如果要翻译互换，只需交换这里\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "# inp_lang  targ_lang 是tokenizer\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(data_path, num_examples)\n",
    "print('-'*50)\n",
    "print(input_tensor.shape, target_tensor.shape, inp_lang, targ_lang)\n",
    "print('-'*50)\n",
    "print(input_tensor[0])\n",
    "print(target_tensor[0])\n",
    "print(input_tensor[29999])\n",
    "print(target_tensor[29999])\n",
    "# Calculate max_length of the target tensors，可以看下最长的样本\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按单词进行切分的\n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        # 不等于0，就打印转换\n",
    "        if t != 0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "            \n",
    "print(\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print()\n",
    "print(\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n",
    "# 可以发现tokenizer正常工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分batch\n",
    "BUFFER_SIZE = len(input_tensor_train)  # 就是3万\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "# 下面两个都是超参数\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# 输入的,加1考虑到padding的0\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "print(vocab_inp_size)\n",
    "# 输出,加1考虑到padding的0\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "print(vocab_tar_size)\n",
    "# 训练集\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)  # 最后一个batch丢弃\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试，看一下迭代\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手法和之前的类似\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        # 编码单元\n",
    "        self.encoding_units = encoding_units\n",
    "        # 创建Embedding层\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # 定义GRU层，gru是lstm变种，gru把遗忘门和输入门变为一个，因为遗忘门+输入门=1\n",
    "        # return_state返回最后一个细胞的中间状态\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output,state= self.gru(x, initial_state = hidden)\n",
    "        return output,state\n",
    "\n",
    "    def initialize_hidden_state(self):  # 初始化全零的隐含状态\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "    \n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "# 获得初始化的hidden\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "# 获得输出和隐含状态\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# 输出的16是长度，1024是状态的size,是因为return_sequences为True，每一个输出都需要\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 理解模型中去做的运算\n",
    "encoder.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现Attention机制\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # 做全连接\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    # query传的是decoder_hidden,values是EO\n",
    "    def call(self, query, values):\n",
    "        # deocoder_hidden.shape: (batch_size,units)\n",
    "        # encoder outputs.shape: (batch size, length,units)\n",
    "        # 做维度扩展，扩展前后对比是下面两行\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        #接下来要实现Attention,Bahdanau方式的\n",
    "        # before V:(batch_size, length, units )\n",
    "        # after V: (batch_size, length, 1)\n",
    "        # score shape == (batch_size, max_length, 1)  （64,16,1）\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # 先算加权，values就是encoder_outputs，对应位置相乘，广播出去\n",
    "        # context_vector.shape: ( batch_size, length units )\n",
    "        context_vector = attention_weights * values\n",
    "        print(context_vector.shape)\n",
    "        # 在length的维度去求和\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 实现decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    # init传参和encoder很像\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_units, batch_size):\n",
    "        # 这里必须调用父类\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        # Embedding 层\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # GRU的decoder\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention，每一步都会被调用\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "\n",
    "    # 照着原来的原理图理解\n",
    "    def call(self, x, hidden, encoding_output):\n",
    "        # context vector. shape: ( batch size, units）\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, encoding_output)\n",
    "\n",
    "        # before embedding: x. shape: (batch_size, 1 )\n",
    "        # after embedding : x. shape: (batch size, 1, embedding units)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        print(context_vector.shape,x.shape)\n",
    "        # 把x和context_vector拼起来，context_vector为什么要扩展维度？\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        print(x.shape)\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output. shape:[batch_size,1,decoding_units ]\n",
    "        #state. shape:[batch_size, decoding_units ]\n",
    "        output, state = self.gru(x)\n",
    "        print(output.shape)\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        print('output变换后')\n",
    "        print(output.shape)\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output,decoder_hidden,decoder_aw = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "print(\"decoder_hidden.shape: \", decoder_hidden.shape )\n",
    "print(\"decoder_attention_weights.shape:\", decoder_aw.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器选用adam\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# 分类问题我们往往用SparseCategoricalCrossentropy，因为我们的fc是纯的输出，没有加softmax，\n",
    "# 因此这里的from_logits为True，否则改为false，reduction是损失函数如何做聚合\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "# 为了把padding部分的损失去除，这样计算损失更加准确\n",
    "def loss_function(real, pred):\n",
    "    # 是零的时候返回结果是True，因此要取反操作\n",
    "    # tf.math.equal(real, 0)是padding的部分都是1，不是padding的部分都是零，因此我们要取反\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)  # 这里是得到的损失\n",
    "    # 将张量转换为新类型,变为float类型\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    # padding部分的mask是零\n",
    "    loss_ *= mask\n",
    "    # 计算累计的损失平均\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_dir = './8-1_checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 把输入给encoder，得到encoding_output, encoding_hidden\n",
    "        encoding_output, encoding_hidden = encoder(inp, encoding_hidden)\n",
    "\n",
    "        decoding_hidden = encoding_hidden  # 最初是把encoding_hidden给decoding\n",
    "        # 第一次给进去的decoding_input全部是1，是64个1\n",
    "        decoding_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        # eg: <start> I am here <end>\n",
    "        # 1.<start>->I\n",
    "        # 2.I->am\n",
    "        # 3.am->here\n",
    "        # 4. here ->< end>\n",
    "        # 对于here，我们相当于要把I am  的信息都要给过去\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            # 根据我们前面的原理解析，我们这里需要给3项信息\n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing,这里是取第t列，再扩维\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "    # 这里是每个batch上平均的损失函数\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    print(type(encoder.trainable_variables))\n",
    "    print(type(decoder.trainable_variables))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    # 有了梯度以后，可以用optimizer去做apply\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.arange(6).reshape(3,2)\n",
    "t=1\n",
    "m[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "# 这里运行时间比较久\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    encoding_hidden = encoder.initialize_hidden_state()  # 第一次，全零的隐含状态\n",
    "    total_loss = 0\n",
    "    # 每次去取dataset.take(steps_per_epoch)这么多数据\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        # 这里增加打印\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs，保存模型\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测过程（翻译）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接收字符串，并进行翻译\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    # 对输入的语言做和训练时同样的预处理（eg: 标点符号前后加空格）后，下一步再进行word转id\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    # text到id的转换\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    # 加padding\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)  # 变为张量\n",
    "    # 结果先初始化一个空串\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    encoding_out, encoding_hidden = encoder(inputs, hidden)\n",
    "    # 按模型把encoding_hidden给decoding_hidden\n",
    "    decoding_hidden = encoding_hidden\n",
    "    # 第一次的decoding_input是全为1的（1,1)的矩阵\n",
    "    decoding_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    # eg:<start>->A\n",
    "    # A->B->C->D\n",
    "    # decoding_input. shape:(1， 1)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(\n",
    "            decoding_input, decoding_hidden, encoding_out)\n",
    "        \n",
    "        # attention weights. shape: (batch size, input length, 1) (1， 16， 1 )，需要变为长度为16的向量\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()  # 为了画图\n",
    "        # predictions.shape: (batch_ size, vocab_ size) (1, 4935)\n",
    "        # 获取概率最大的值作为下一步的输入\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        # 通过id知道单词\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        \n",
    "        # 终止循环\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "    # 到此decoding_input，decoding_hidden我们都做了更新\n",
    "    \n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# function for plotting the attention weights，把注意力关系完成可视化\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "    # 把标注写上，我们需要把第零个位置空出来，看图即可看出\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 通过这个函数，把上面两个函数串起来\n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    # 因为输出不一定有输入的长度长，也就是result长度小于输入的长度\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "    \n",
    "# 加载模型参数\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is terribly cold here\n",
    "# 热力图最后多一个单元，是因为预测时在<end>后添加了空格\n",
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'Hoy es un buen día')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
