{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
      "matplotlib 3.2.1\n",
      "numpy 1.18.5\n",
      "pandas 1.0.4\n",
      "sklearn 0.23.1\n",
      "tensorflow 2.2.0\n",
      "tensorflow.keras 2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvidia-smi: not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9020706173558768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(112/np.sqrt(14))/(np.exp(112/np.sqrt(14))+np.exp(96/np.sqrt(12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. loads data\n",
    "# 2. preprocesses data -> dataset\n",
    "# 3. tools\n",
    "# 3.1 generates position embedding\n",
    "# 3.2 create mask. (a. padding, b. decoder)\n",
    "# 3.3 scaled_dot_product_attention\n",
    "# 4. builds model 分为以下6步\n",
    "    # 4.1 MultiheadAttention\n",
    "    # 4.2 EncoderLayer\n",
    "    # 4.3 DecoderLayer\n",
    "    # 4.4 EncoderModel\n",
    "    # 4.5 DecoderModel\n",
    "    # 4.6 Transformer\n",
    "# 5. optimizer & loss\n",
    "# 6. train step -> train\n",
    "# 7. Evaluate and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tfds.core.DatasetInfo(\n",
      "    name='ted_hrlr_translate',\n",
      "    version=1.0.0,\n",
      "    description='Data sets derived from TED talk transcripts for comparing similar language pairs\n",
      "where one is high resource and the other is low resource.',\n",
      "    homepage='https://github.com/neulab/word-embeddings-for-nmt',\n",
      "    features=Translation({\n",
      "        'en': Text(shape=(), dtype=tf.string),\n",
      "        'pt': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=54781,\n",
      "    splits={\n",
      "        'test': 1803,\n",
      "        'train': 51785,\n",
      "        'validation': 1193,\n",
      "    },\n",
      "    supervised_keys=('pt', 'en'),\n",
      "    citation=\"\"\"@inproceedings{Ye2018WordEmbeddings,\n",
      "      author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n",
      "      title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n",
      "      booktitle = {HLT-NAACL},\n",
      "      year    = {2018},\n",
      "      }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "#葡萄牙语到英语，这个是基于subword的,as_supervised是True是tuple，是False返回的是字典\n",
    "examples, info = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                           with_info = True,\n",
    "                           as_supervised = True)\n",
    "print(type(examples))  #examples是字典，里边有训练集，验证集，测试集\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "print(info)#info里是数据集的描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'\n",
      "b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "b'mas e se estes fatores fossem ativos ?'\n",
      "b'but what if it were active ?'\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "b'mas eles n\\xc3\\xa3o tinham a curiosidade de me testar .'\n",
      "b\"but they did n't test for curiosity .\"\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "b'e esta rebeldia consciente \\xc3\\xa9 a raz\\xc3\\xa3o pela qual eu , como agn\\xc3\\xb3stica , posso ainda ter f\\xc3\\xa9 .'\n",
      "b'and this conscious defiance is why i , as an agnostic , can still have faith .'\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "b\"`` `` '' podem usar tudo sobre a mesa no meu corpo . ''\"\n",
      "b'you can use everything on the table on me .'\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "#葡萄牙语中有一些特除的字符，用转义字符来打印\n",
    "for pt, en in train_examples.take(5):\n",
    "    print(pt.numpy())\n",
    "    print(en.numpy())\n",
    "    print(type(pt))\n",
    "print(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里运行要点时间,如果这一步崩溃把内存分的大一些\n",
    "#我们自己转为subword数据集，2**13是8192，build_from_corpus\n",
    "en_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples),\n",
    "    target_vocab_size = 2 ** 13)\n",
    "pt_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples),\n",
    "    target_vocab_size = 2 ** 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string is Transformer is awesome.\n",
      "7915 --> \"T\"-->1\n",
      "1248 --> \"ran\"-->3\n",
      "7946 --> \"s\"-->1\n",
      "7194 --> \"former \"-->7\n",
      "13 --> \"is \"-->3\n",
      "2799 --> \"awesome\"-->7\n",
      "7877 --> \".\"-->1\n"
     ]
    }
   ],
   "source": [
    "#测试一个字符串,subword里边是包含空格的\n",
    "sample_string = \"Transformer is awesome.\"\n",
    "\n",
    "tokenized_string = en_tokenizer.encode(sample_string)\n",
    "print('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "origin_string = en_tokenizer.decode(tokenized_string)\n",
    "print('The original string is {}'.format(origin_string))\n",
    "\n",
    "assert origin_string == sample_string\n",
    "\n",
    "for token in tokenized_string:\n",
    "    print('{} --> \"{}\"-->{}'.format(token, en_tokenizer.decode([token]),len(en_tokenizer.decode([token]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8214"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#真正拆出来的词根是接近8192，刚好等于的概率很小\n",
    "pt_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8087"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "The original string is �\n"
     ]
    }
   ],
   "source": [
    "origin_string = en_tokenizer.decode([8086])\n",
    "print(len(origin_string))\n",
    "print('The original string is {}'.format(origin_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 20000\n",
    "batch_size = 64\n",
    "max_length = 40  #输入和输出的最大长度是40\n",
    "\n",
    "#把两段文本转为subword的id形式，\n",
    "def encode_to_subword(pt_sentence, en_sentence):\n",
    "    pt_sequence = [pt_tokenizer.vocab_size] \\\n",
    "    + pt_tokenizer.encode(pt_sentence.numpy()) \\\n",
    "    + [pt_tokenizer.vocab_size + 1]\n",
    "    en_sequence = [en_tokenizer.vocab_size] \\\n",
    "    + en_tokenizer.encode(en_sentence.numpy()) \\\n",
    "    + [en_tokenizer.vocab_size + 1]\n",
    "    return pt_sequence, en_sequence\n",
    "\n",
    "#用tf的API消去大于最大长度的\n",
    "def filter_by_max_length(pt, en):\n",
    "    return tf.logical_and(tf.size(pt) <= max_length,\n",
    "                          tf.size(en) <= max_length)\n",
    "#用py_function封装一下encode_to_subword\n",
    "def tf_encode_to_subword(pt_sentence, en_sentence):\n",
    "    return tf.py_function(encode_to_subword,\n",
    "                          [pt_sentence, en_sentence],\n",
    "                          [tf.int64, tf.int64])\n",
    "#把所有句子变为subword，subword都变为id\n",
    "train_dataset = train_examples.map(tf_encode_to_subword)\n",
    "train_dataset = train_dataset.filter(filter_by_max_length)  #去掉长度超过40的（subword的40）\n",
    "#接着做洗牌，padding，batch -1，-1代表两个维度，每个维度都在当前维度下扩展到最高的值\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size).padded_batch(\n",
    "    batch_size, padded_shapes=([-1], [-1])) #葡萄牙语，英语都补到40，有两个[-1],不够补0\n",
    "\n",
    "valid_dataset = val_examples.map(tf_encode_to_subword)\n",
    "valid_dataset = valid_dataset.filter(\n",
    "    filter_by_max_length).padded_batch(\n",
    "    batch_size, padded_shapes=([-1], [-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 36) (64, 36)\n",
      "(64, 33) (64, 36)\n",
      "(64, 38) (64, 39)\n",
      "(64, 39) (64, 40)\n",
      "(64, 40) (64, 40)\n"
     ]
    }
   ],
   "source": [
    "#不同batch不一致，是因为先batch，后padding的\n",
    "for pt_batch, en_batch in train_dataset.take(5):\n",
    "    print(pt_batch.shape, en_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(13)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(256)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle_rads.shape:(50, 512)\n",
      "(50, 256)\n",
      "(50, 256)\n",
      "tf.Tensor(\n",
      "[[[ 0.          0.          0.         ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.84147096  0.8218562   0.8019618  ...  1.          1.\n",
      "    1.        ]\n",
      "  [ 0.9092974   0.9364147   0.95814437 ...  1.          1.\n",
      "    1.        ]\n",
      "  ...\n",
      "  [ 0.12357312  0.97718984 -0.24295525 ...  0.9999863   0.99998724\n",
      "    0.99998814]\n",
      "  [-0.76825464  0.7312359   0.63279754 ...  0.9999857   0.9999867\n",
      "    0.9999876 ]\n",
      "  [-0.95375264 -0.14402692  0.99899054 ...  0.9999851   0.9999861\n",
      "    0.9999871 ]]], shape=(1, 50, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#第三步，写一些工具函数\n",
    "# PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "# PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "#pos 和i都是矩阵\n",
    "# pos.shape: [sentence_length, 1]\n",
    "# i.shape  : [1, d_model]\n",
    "# result.shape: [sentence_length, d_model]\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000,\n",
    "                               (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "#计算位置信息\n",
    "def get_position_embedding(sentence_length, d_model):\n",
    "    #sentence_length和d_model都扩展为矩阵\n",
    "#     print(np.arange(sentence_length)[:, np.newaxis])\n",
    "#     print(np.arange(d_model)[np.newaxis, :])\n",
    "    #pos是0到49，就是词的位置，i是从0到511，总计512，和dim相等\n",
    "    angle_rads = get_angles(np.arange(sentence_length)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    print('angle_rads.shape:',end='')\n",
    "    print(angle_rads.shape)\n",
    "    # sines.shape: [sentence_length, d_model / 2]\n",
    "    # cosines.shape: [sentence_length, d_model / 2]\n",
    "    #正玄一半，余玄一半\n",
    "    print(angle_rads[:, 0::2].shape)\n",
    "    print(angle_rads[:, 1::2].shape)\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    #把sines和cosines进行拼接\n",
    "    # position_embedding.shape: [sentence_length, d_model]\n",
    "    position_embedding = np.concatenate([sines, cosines], axis = -1)\n",
    "    #进行维度扩展\n",
    "    # position_embedding.shape: [1, sentence_length, d_model]\n",
    "    position_embedding = position_embedding[np.newaxis, ...]\n",
    "    #变为float32类型\n",
    "    return tf.cast(position_embedding, dtype=tf.float32)\n",
    "\n",
    "position_embedding = get_position_embedding(50, 512)\n",
    "print(position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEKCAYAAAD+XoUoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydeXhU1fnHP+feWZOZ7CtJ2HdFFq2CWAUX3HH/Fa0tVq3WWq11qVurrVqrtdVu1rW02qq4VUWKCwrWFWQRlUUgrCEJ2TPJTGa/5/fHvZNMQoABEiR4Ps9znrvOzMkwnDnzvuf7fYWUEoVCoVB8M9C+7g4oFAqFYv+hBn2FQqH4BqEGfYVCofgGoQZ9hUKh+AahBn2FQqH4BqEGfYVCofgG0auDvhBisxDiSyHECiHEUutcjhBivhBivbXN7s0+KBQKxdeFEGKWEKJWCLFyJ9eFEOLPQohyIcQXQogJSddmWuPkeiHEzJ7q0/6Y6U+VUo6TUh5hHd8CvCulHAa8ax0rFArFwcg/gVN2cf1UYJjVrgAeAXNyDNwJHAUcCdzZUxPkryO8cxbwlLX/FHD219AHhUKh6HWklO8Djbu45SzgaWmyCMgSQhQDJwPzpZSNUsomYD67/vJIGVtPPMkukMDbQggJPCalfBwolFJWW9e3A4XdPVAIcQXmNx8I2+F5UqMhLYOy/sU4NpazQU9jpD2Cuzifz7b6GFfsonFLPa1lg2hpauWwIgdb11aSadNwjBrJ2o1VONK9jC5Ow7dqPa0xg/xcN7b+gymvDdDW3AxGHJvbQ05OOv28TmiuIbC9mdZQnKiU2ASk6RourwPd5cCemQEuL2FD0BqJ4w9FCYXjxKJxjFgEIxZFGob5NiSUz0KA0BCahhAaQtcRmo6m6QghEBrWVqBpAk0IdF2gC4GmYW3N85own1ITwnzaxH7iZTDPg3nNel873uNO73eX93+Hf5DdXN/N+b2+cye3tYRjZNoFUmhokTbWt0J6xWaKxh/Cmm0+MusqKDjsEFZvrGZ0epTWugChQUOora5j3PASqlesIi6hdGQpa1tstDU14M3PY1iGRvNXm/DFDLJdNryD+uHT0qisbyMWDhEPB9FsDpxeDwWZLrJddkSgkXBjM2FfmLaYQVRKJOaMyiYEDk3gcGjY3XZsaU40lxPNmYa0OZCaDUNCzJBEDEk0bhCJG0RjkkjcIB43kIbEMCTSACml1QwwDKT12ZLS+oxJAwkdnzdr2+kcu1Hh93GVvgw21Esp8/f28VpGqSQWSvW1VgHJNz9ujXOpUgJUJB1vs87t7Pw+09uD/jFSykohRAEwXwjxVfJFKaW0vhB2wHrjHgfQ0vLkeUEP/xx9Crf99Vb6f2c6Z2UfztPFWzjs9ivx/ORNPrxlGM9d9RTv/vpp5r/8Hp/cWMa1x97Cybnp9P/vQo698Nf0/9ZUPrl9Av899GQW1rVx1ZljKPjzbKY/vIjPXnuVWMhPwejJXDRjInecMBheeYAlv/8v//uqge2hGDl2nQlZLkYcP4Ds4SUUnHIK8pCpbGiz8cGWJv63tpb1m5porG6ltWYLoaYaokE/RiyCNOIAaDYHms2B3e3B5krHkZ6JPT0TR1o6Tpcdh9uGzaHjdNlxum2kuWxkpdnxuOx4nTY8LrO57Tppdh1NCJw2DZdNw66Z+3ZNw66L9q0uBLr1m063viA0kbSP+WWQ+BJJnIOOLwlNdB5/O+7tPCprKX45aF2/ZXbCzm6bv7GZk0sdRG1u3FuXcuZ7Okf+7Pvc/NFHTLj1baY/fB0/fvd9xn7nPv47sZr3Hv2Y1X98gT//9gk+fusu7skdhy9q8PtZv+O4hVkse/EZJl95OXOnOZg7+RLmbfdz3sBcpv7rLua5D+e2WUup3bCO5s0rSc8vY/gxx/Dj00dyweh89E9eYPPsVyl/o5wVDUGqQlHiEhyaIM+hMyjdTmlZBoVjCsg7bDDekcNxDD0MI6eMsKeQtqhBfTBOVWuYypYQ25qDbGsKUt0cpLk1TCgQJRyMEgnGiIRjGHGDaKiNeDiIEYsQj0XMSUY0Yn3WDKQRRxpxDOtzJ+Px9s9gYtt1f1fn+hLRFf/Ysk9PEAthGzE91dcKJYWu+wS9Gt6RUlZa21rgFczYVI318wVrW9ubfVAoFIo9QgiEpqfUeoBKoCzpuNQ6t7Pz+0yvDfpCiHQhhDexD0wDVgJzgEQmeibwWm/1QaFQKPYc0f6LfHetB5gDfN9axTMR8Fnh77eAaUKIbCuBO806t8/0ZninEHjF+vlvA56VUr4phFgCvCCEuAzYAvxfL/ZBoVAo9gxrpt8zTyWeA6YAeUKIbZgrcuwAUspHgXnAaUA50Ab8wLrWKIS4G1hiPdVdUspdJYRTptcGfSnlRmBsN+cbgBP25LnSc3O5fEwp/82dyPe2Po/zvacY8MAmnnnseuoePI7+k2wsvPFXnHTlJO544zNGH/st1vzlfopcNsacfwh/WLyFaMDH+PHFGEvn8aUvTKHTRsmx4/i8LkhthY9YyI9mc5BZWMCYkkycrdupWVdBc7Uff8ww+6FreDOdpBVkkF6Uiy23iIDNTXOojaa2CA3+CJFgrD3emoirdo2RmslbDc3u6PipKASaTUO3aWYyVgOhCRw2DV3TrLh8R0vExHVhNjOx2xG/T2yTY+Kd9nfyXncXQ+8ap+96vLPzqSd1U+9Lgv6/nMmhRT/ikffu5ZEb/sqcMzOoazqZUx9ZzNM/+zYvPQyX/Gs5404/iXfv+RFTLj+Su+atpd/YYzHmP0ldOM6ELBexcadT8ddncGXmc9b4EoKL/8XqljAem0bBmAJk/zEsW95MS30ToaYaANzZRWTlpVGW6cIWqCdas5W2Wj++UIxA3CBuZal0AW5dw2PTcGY4cWS4sae70dK8CIcb6UgjEpdWM2iLxgnFDIKROJGYQSRmEI+ZyVwjLjGshK1hdKTB2j9j8R0/Z+33xPt2jH5/IzD/j/YEUsoLd3NdAlfv5NosYFaPdCSJ3k7kKhQKRd9CCLQemukfiKhBX6FQKLrQU+GdAxE16CsUCkUyPRjTPxBRg75CoVAkIRBoNvvX3Y1eo0+4bA73SnKefpW3HjiXh2Y+wWUfGzx78xTyHDaue/gTfnnZt5hX2ULx9b+mft0Sfjn9ED5+exOTB2YyYObFvP/xVuzpmcw4oozKNxZQE44xOsNB+lHH89GWRnxVmwBwpGeSXehhdL4HsX09zeVV1IXjBOMGDk2QaddIy3WTVpSLsyAPIz0Hf8SgMRijtiVMKBglEo51iGaikU7JtUTSVkta55tY+qXbNDTNUuJaCV1dE9isxK3DpllJXSuZm0jeJid1d5Jh7ZrM3VkiNkFXYVZPk6owa1c8+p+1bFv6Dq+sqWPuw0/yzrEXUX/JvSye/QKjP3qYC88cxvI5b/LYd8ezqDFI6U9vo3L5Qk47cSirHp9Lpl1jwqQS3tnUTNOWlWSWjeL4QTlsW7icmnCMQqeNoiOG0uTIZfmWJgK1W4kEfOgON+7sAoYVeinJcKK31hKorMNfE8AXNYgkJVkdmsCtC1wuG450Ow5vOvaMNLT0DAyHG2lzErGUuIkkbihmJnGDkRiRmGGqcC1FrhEz1bnJiVujm4UCXYVZij1k/67T3++omb5CoVB0oa8O6KmgBn2FQqFIRogeW7J5IKIGfYVCoUhCcHDP9PtETH/7V1v59jXPof3i+0Sl5MW//Ythbz7AzJumsOnDOVycU0eOQ+epTRJHeiZT0upZ2RJi7GXH0DT8BKpWLiV36ASmDsxk0zsbiEsom1BEbMDhLFxTS7ChCt3hJi23HyMGZFGWYSe65St8W1qoC8fbzbNyHDqewnTSi3LRc4sx0rLxRwwa2iI0BiKEgzGi4RjxSNBy2OxGmJUUx9faY/wCXdfQdGurCYQQ7TF8h01rj+2b8Xwzlp+I60NCoNUh0mpvlkTKNFHrHEtPNlvbG3or5p8K9z12EQ/+6SZuuuk4isefyKsbmzj3ngWk5fZj9tX/5pCH/0a4tZEBy2fTz2Xj9cYM4pEgP/32QBZ9tI2JOW5GfW8qsz7eTDTgo2REKQNFExUfVRCMS4Z67GSOG0d5U4iqCh8RfxNGLIIjPRNvjpthRR7y3DaM2q34K+toawjii8bbY/rJwix7ugNnphN7Rhp6uhct3Yu0p2HYnO3irFDMIGwJs9oiccJJwqx4zMCIGWZcPxHT7/LZ6jBTM7p9v1I1W1MAQkO3OVJqfRE101coFIpkxME901eDvkKhUCQhUOv0FQqF4hvFwTzo94mYvk2D5oo1/GXWCm547GKcnmyeuP4lbNf/kYzS4Xx29U2cdfxA/vDc5wycOJXqR35vxuBnXMFzK2sI1FUwaEwZ7vUf8MVWHzkOnbIpoylvkVRubCIS8OHKzMNTWMb4AVlkyQCt6zbQsq2FlpgZ9/TYNDJdNtIKPNjzC7HlFRF3Z9ESjtPQFqHBHyYcjBINhYhFgp0KpyQQmt5utiZ0K7Zvd6DpWnulLKEJM7bfHs/XuzVbS47rdzJgSzJbS9CdEVrXtfKa6Lqev6N4SuIx3T3XnrKvxVMSXOc5nzPf+A1fzLyfBfedxveP7c+Wj1/n+hsuYElTiF9/FmHQMafx4Q1PcNrUAdz78pfkDp1A/8pPWNMa5tBzRuE48fus+qwa3eHmxMNLMD5/l/WVrTg0Qb8xBdhGT2R5dQuNNX4iAR8Azsw8svLTGZKTjtdoI1a9yayu5gsTstbcg5kDcmnCMltz4PC6cHjTEGkZCLcXaXcSjhntMf22qEE4FjfN1uKm2ZoRN2P5UiaZrVmfq04tvmO8fm9RcX7UOn2FQqH4ZqHCOwqFQvGNQQiBZu+bK3NSQQ36CoVCkYwyXFMoFIpvFgfzoN8nErl5hwzj/geu5bSSDF499HLu/OXFVIWinPfIYmZcchovvrOJ8Q/+is0fv8VPzjuUxU8s4ti8NFaKEv79Tjm6w833vj2I2tdfoSIYZbjHQfa3p/Dh1iaaKquQRpz0/P7kFnkZU+DFVr+RpnUV1LRGCMYluoAMm0Z6YTrpxbnouUXgzaM1HKe+LUJdS5jWgFk1Kx4OYkQjOxhhdTVb02wdVbP0RMUsm9YuztI1gTPZYC3JeC25UhaY+wnRVjIiKTmbEGbtayJ2Z/R01azd8ewDf+Weu+Yz87pHCFzzHSa88QZDppzNLcVVnDcylyeffJv7f3gk/11bz7jf3MT6D95n3NSxbHj4UXQhGHDx/7Ei6KVu7TIyS4dzzqHFVM9/j81tEfIcOsVHDCSYM5iP19cTqNuKNOJoNgdpuSUMLPQyIMuF3lJN27YqWqv8NEbi7RXWHJqwzNY03A4dZ6YTR0Y6jox0NG8W0uFG2tOSkrhxwrE4obgpzkqYrcVj0hJnSctorbPZWjLJ4qtkszVVNWvv0KyFFbtrfZE+MegrFArF/kIIcxVdKi3F5ztFCLFWCFEuhLilm+sPCSFWWG2dEKI56Vo86dqcnvj7VHhHoVAouqDrPTMfFkLowMPAScA2YIkQYo6UcnXiHinlz5LuvwYYn/QUQSnluB7pjIWa6SsUCkUygp6c6R8JlEspN0opI8Bs4Kxd3H8h8FwP/BU7pU8M+qtrQly47G9MWz6X637xFD8Kf8ilF4xi+Ssv8+DxBUQMyTtiBAA/GJnO+/VtjLt4An9YWM7m5Z+TPfBQTh+eR/nrnxOMS4aNyoORk3l71Xb8NZvRbA6yigsZ2D+TIdkuIuVf0FjewPZQjIghceuaabZWkIanJB9bfgnx9Fz8UdNsrbY1TDgYMwuoWMIsI7oTcVZSbN8snmKzPkBmbF5ooOmdC6Z0iu0ni7Ks2L6eiNvvwmwteZugu3/8VD8QyWZrX0do88xrruQHJw5Cd7p5ePZqjvv9x7x66xTePPlajn/xdzRu/JzTjVU4NMFnuUfR1lDF3aeP5tP/rGFClou2sWfwxKIttDVUUTJ6BIdmwdb31uOLGgz1OMifNJ4NTWE2bG4m1FQDYJmteTikJIPCNBuydiutFbUEajsXUNGFGdf32ATODKfZsjzo6R60NC+GPQ3D7rJi+qbRWsJsLRwzhVmRSJx43GiP5cctw7VEPD+5gMrOzNZ2Fc9XIqydY7ps9tigXwJUJB1vs87t+LpCDAAGAQuSTruEEEuFEIuEEGfv5Z/UCRXeUSgUik6IPanulieEWJp0/LiU8vG9fOEZwEtSyuRv5AFSykohxGBggRDiSynlhr18fkAN+gqFQtEZK7yTIvVSyiN2cb0SKEs6LrXOdccM4OrkE1LKSmu7UQjxHma8f58G/T4R3lEoFIr9SQ+Gd5YAw4QQg4QQDsyBfYdVOEKIkUA28EnSuWwhhNPazwMmA6u7PnZP6RMz/XBrM/dc9xKf+k8l2tbCv8+/l4uqVuA84x7Kf/pDzjm8mJ8+vYz+R55E8xN3AzDgqmv4+IEttGxbxxEXXEhh7QpeXdtIpl1jwAkj2RJNZ0N5AyFfHe7sQvJKvBw1JJd8W4TWdWvxbWmhxVp37bFp5DltePp5cRYVYaTnYqRl09IUpc4yW4sEo0TDEctsLbpTszXNZjdN1pLM1nSrJQqiJ9bp65rAoXcUUulqtpZYn2/G8M3X2ZnZWntcHyt30H5e7LjG/gA3WwN4yvkWW55+jTnhKIHyF5n1ynOkx1/g9W0tbPUPoeyo01n0o19xxuHFXP/8CrIGHsr4yDqeagpxxYzR/Oerej74ZCuazcExE0oQX7zNV+sa0QUMGJGL47BjWbzNR8P2ViIBHzaXxzJbS2NobjqZWpRo9Wb82+rxN4UIxDti+m5dw6WZBVQcHjvODCeOjDQ0bzZaegYxh9ssnGKt0U+0YCROMGoWUUmYrcXjZpNS7mi0lqLZWncFVHZ13zcdIUC39UyiSkoZE0L8BHgL0IFZUspVQoi7gKVSysQXwAxgtpRSJj18FPCYEMLAnKDfl7zqZ2/pE4O+QqFQ7E96siqclHIeMK/LuTu6HP+qm8d9DIzpsY5YqEFfoVAokhCi76ptU0EN+gqFQtGFPUjk9jnUoK9QKBRdOJgH/T6xeqe4pJBj89JY8vy/ufH2S/ncF+bURxZz9qXn8twLqzn60TtYu+ANfjzjMBb94V0m57pZkzaS7Ss/RGg6F08ZTN2rs1nnDzPc46DgxBP43+ZG6jZtazdbmzA4l3HFGdjrymlas4XtzSH8MSPJbM0UZumWMKs1JqjxR9jeHMLnjxAOxogF/cTDQeJdqmZ1NVvrLNISnczWdF3DZtNw2jSzalYXw7VkszU9KZnbNYG7p2ZrPRjC7HWzNYCbvzeLYy/7C7n3/pDjFr1F/0ln8Nj9CzhrQCZ3P/QGv7lqIi8t2sbEP97EyvnvcdgJR7Lxod8DMOyHFzHr3Q1sX7WMjNLhXDihhJo33mJDIEK+00bJ5MEEC0bw4fo6Wqo3Y8QiOL3ZptlasZehuWnovkraNm+mtdo0WwvGzaS/LmivmOVx2nBlu3BmeXFmedHSvUi7abaWqJrVFjVoi6ZutgY7Jly7mq3tDSqJm4ToRui4k9YXUTN9hUKhSEIg0Gx9Yj68V6hBX6FQKJIRqESuQqFQfJPoySWbBxp94jdMfqie01e+xYiTzuNm8TFXzBjN4tkv8PgpRfhjBvPd45FGnKsO8fBObYCjfvAt7nt3HdGAj5zBYzlnVD5rX15GMC4ZNaYAxhzP3C+q283Wskv6ceTAbIbnuImsX0H92rodzNa8xZ52s7WQ7sYXjrebrYXaooSDUeKRIPFIaLdma7rN0W62ptm0TmZrIkmI1dVszZEosLIXZmtdJy67M1vbdfz/6zVbA5g5bTCa3cFDTyxn8h+X88avTkIXgmnz/kT9uiWcj2m2tqL4OAJ1Ffz+nEP58LkvmZDlInjEOWxc/hWBugrKDh3N+GzY8OZqfFGDUV4HhZMPp7wpzLqNTe1ma+7sIjLyMjmsLIvCNBvUbKa1ohZ/tZ/GiEEwbmpqEsVTujVb82RhOD0Ydhchy2zNLKBixvPbIvFdmq0lPle7M1szdiPaUvH7XWMarqXW+iK93m0hhC6E+EwIMdc6HiSEWGwVFHjekiYrFArFgYFQlbP2lZ8Ca5KO7wceklIOBZqAy/ZDHxQKhSJFBJqupdT6Ir3aayFEKXA68KR1LIDjgZesW54CesQjWqFQKHoCcZDP9Hs7kftH4OeA1zrOBZqllDHreFcFBa4ArgDwoHPUQ1+y+Ncn8GjhWC6u/Iy07zzEqksvYcbUgVz25BIGTz6Fuj/9El1A2Y+v54O7vyI9v4whR4wkv2IRz69pIMehM+iUMZSHXGxYZ5qtpeX2o7B/JuOKvBRobTSvXEXzxmaaombc02PTyE+z4y3NxFlURNyTjy8cxxeKs90fprYlRCgQIRIMEg35MXayRj9Vs7VEPN9h03dvtta+nhh0rWfN1tqPuzzX3tKTZmsALX95ng8zndTWvsqsV2cj6p7kyjtO5r7qfgw+9ize/94vOO/4gVzz9DJyh05gTNMynmgKcs2l43huZS1Nm1eiO9xMm9gfls5lTXkTuoD+h+bjGD+VTyqaqa9qIdzaiM3lwVtQTE5hOiPyPWSKMNGKdbRurcPXuKPZmsemkWnXcWY4cGW5cWZ5TLM1TxYxh7t9jX5ruMNszR+K9YrZWgIVx98zlDhrLxBCnAHUSimX7c3jpZSPSymPkFIe4Ubv4d4pFApF9wjBjqLInbS+SG/O9CcD04UQpwEuIAP4E5AlhLBZs/1dFRRQKBSKr4W+OqCnQq/N9KWUt0opS6WUAzG9ohdIKb8LLATOt26bCbzWW31QKBSKPUWQ2iy/r34xfB3irJuB2UKIe4DPgL9/DX1QKBSKbhECHAexDcN++cuklO9JKc+w9jdKKY+UUg6VUl4gpQzv7vHZaXZWzXuRpVOOpyoU5cT7/sf1N1zA03PXc8STf6T8f3O585LDWfCX9zmx2MvH8VJqvnyf0nFHctWJw6ia/SwbAhEOzXCSP+005m+op37TJqQRx1M4iEnD8hiQ6UDfvpaGVZuoagm3m61l23W8/SxhVmF/jPRcWsMGtYEw25tDtAYiRCyzNSMaIR7dtdmaZgmzTHGWtoPZmsMyW+s6o3DYtF2arSXTndnarhCi5z4I+2vuc9YP7qXhgjMY9ubbjDnj/3j40SU0XvJbHvzDi8z62TG8vLKWIx6+j9XvzGfqmUex+jd/wKEJhl79I2a9tY54JEj2wEO5eEIp216bx4ZAhH4uO/2PG4kvawjvrK6hpXoj0ojjyswju9DDiLIshuakYWuqwL9pKy0VrTRG4vitCmsOTeDSBJl2DbdDx53twpntxZntRfNmIR2m2VooLgnHOqpmBRJVsyIxgpF4t2ZriQUCXU3XupqtGUmfvVSTtyrJ2xkhwKaJlFpfRNkwKBQKRRKCgzumrwZ9hUKhSEb03Xh9Khy8gSuFQqHYC8yZvpZSS+n5hDhFCLHWsp65pZvrlwgh6oQQK6x2edK1mUKI9Vab2RN/X58Y9B3DhnPilZfz7KdV3HDvmaya9yK3FFeRbdf501YPjvRMzsuo5aOGIBNvPpk75qzCiEU494QhnDMqj9UvfEbEkIyYWEJs9PHMWVaJv2YzNpeHvP6FTByYg6NmLeFVi6n/qoHtoThxaQmznDoZpV68/QvRCvoTwEFNIExtwDRbC7ZGCIc6zNa6FrIQXWP5ycVTdA1NN7e6TWBLNldrL6Sitcftu5qtgSmaSsVsLTFvScTvd+Ui2NNma71RbKLs8Ck888FWJt4wl49vnsQor5Nz711AW0MV45b9gzK3ndkt/Qi3NvK7M0Yxf245JxR4qCibzKaly/EWD2HwhJEM1xoof2Md/pjB2CwXed+ezJe1bWzc0EhbQxVC00nP709JPy+HlWVSlG4jXlVOy+bq9gIqCWGWwyqekmE34/lmARUPujcL3ZuF4fAQt7kIxySh2I5ma22ROPGEKCtmCbRiBvFYDBmP7xC37xBqGZ3em4Roq/14L+L833R6avWOEEIHHgZOBUYDFwohRndz6/NSynFWSzgY5AB3AkcBRwJ3CiGy9/Vv6xODvkKhUOwvNGFOulJpKXAkUG4tYIkAs4GzUuzKycB8KWWjlLIJmA+csld/VBJq0FcoFIou6Jblye4akCeEWJrUrujyVCVARdLxzqxnzhNCfCGEeEkIUbaHj90jVCJXoVAokkjYMKRIvZTyiH18ydeB56SUYSHElZhGlMfv43PulD4x0/9qSz1zJvm59OTBLDvjNsqOOp03T76W7187mT/87V3Gn3kqK39+K0UuG55LfsGaDz4je+ChXPatUsT7z7C4ooUyt51h505iSXUbW9fWE25tJC2vH4OH5DCmIJ3Y+uU0frGW+k0dZmsZNp3cbBfe0mwcJQMwPHk0h+Nsbw1T3RKiujlIqC1KpC1ANLhrszWhaTuYrSVM1nSbadOaKJrisOmWeVpHfL87s7XkguiJbdeiKcnh9K6xdU10vr6vZmv7Grnfk9D/yptH8ovfnE7Nl+/z0dEnccncu9j04RyOmvF/vHDF35nxk6O588kl9J94GrkfzmKdP8Lh1xzLHz/YTMu2dZQeNp7vTRlMZMEzfFbZisemUXZMKdqYKfxvYwMNlfVEAz4c6ZlkFuYxYUA2o/M9eMKNRDevoWVLI40tYVpiptmaLsCtm2v0nZkOXNkuXNnpuLK8aJ4sRFom0plOKGYQihv4I6bBmj8cazdbC0bixKJxjJiBEZcYUu5gtmYkma2p+Hzv0YOK3EqgLOl4B+sZKWVDkl7pSeDwVB+7N/SJQV+hUCj2Fz0szloCDLOKRzkwLWnmdH49UZx0OJ2O+iNvAdOEENlWAneadW6fUOEdhUKhSEIgesyGQUoZE0L8BHOw1oFZUspVQoi7gKVSyjnAtUKI6UAMaAQusR7bKIS4G/OLA+AuKWXjvvZJDfoKhUKRxB7G9HeLlHIeMK/LuTuS9m8Fbt3JY2cBs3qsM6hBX6FQKDpxsNsw9ImYvozH+PMxP2HQC3OZeeszzLnzJF7f1kL67Y9Q99Ui/jHzcF57fT2nHdefJ75spHHj54w4eiz9KibIAdIAACAASURBVD5m/T9epioU4/BiD+nHn8dLn1fRuGk1QtPJKhvO1FEFFOtt+FasoO7zLWxtixGMGzg00S7MyhhYjL3fQOKefJqCZsWsbY1BAq0RwsEosaDfFGd1Z7am6+iWMCtZpGUmcJMEWu1rf/Ud1gLrlijLrgnsWofZmtaetO0QZkGH4Vq7SItdC6SSPwTtCeBu7tuVoGt/89DwM3nh2Bv4+V3X8MKXtdzXNpbBx57Fmz86kkWNQfLufJSti+Zx4/cn8NGt/6LMbSfvspuY9045usPNmccN4pyReax7/n0qglGGpDsYcOJ4tolsFqzcTmtVOQDu3H7kFnsYU5zBoCwXtsYt+DZU0rzFR104TjDeYbaWrpsVs1xZLtNsLcuLMycTPTMXw5mO4UgnGOtstuYPxWizzNbCkThGXBKLxjuJs7qtmtUu0DJSNltL9dw3HlVERaFQKL45JPz0D1bUoK9QKBRdUIO+QqFQfEPQDvIiKn1i0B82sJBguZ9jbn8b//bNZD56A2cNyOTcxxZTNHYqhfP/RFUoxvi7r+MHz63Enp7JDaeOZMtj17Hs7U24dcHw6aOo9A7hoxUfEairwOnNoWhANpNKs9G2fErtZ+XUr22gPhIjLiHHoVHkspFZmkF6/xJkdj+awgbVVjy/2hck6A8TDkaJhvwYsWgncdYOxVPsDjO2bzfj+Ta7bsbzEwItS5ilawKH3rmQil3TsOtauzAruXjKDoIrRCdhVvKEJdlsretEZk/j9T1ttran6YJ8p85V1/+B+uuKWX/uCI777VN8OvsW1l96HmcPzubiZz8nLbcflw6Iceu6BmacNIg36l1Uff4+ecO/xczDS8nZ/BFvfVhBXMKoodlkTDmd17f62L65mWBTDbrDjbdwAGMG5jAiL43iNI3I0lX4NlTSuj2AL7qj2Vq629ZutubKzUDLzEXzZhFzeomiEY7HaIvGaY10FE/xh824fsJoLR43kIa0th1CrGRhFuwkRr8LszVFivTw6p0DjT4x6CsUCsX+QrBjNbqDCTXoKxQKRRd6ww78QEEN+gqFQpGEwKxZcbDSJ7IVYusGbpz3KzZ9OIdLb7ycv923gGnz/sSy/7zC7Vcdy9s/e44TC9JZ1e9YNn+6gP7fmsopRQZfPP8ln/tCjM10UXr+2byxvoHqdVswYhEySoZz9OgCRuQ6Ca1cRN3qeipr2/BFOwqiZ5R4yRhUhK3fIOLeQppCcSpbQmz3BWnwhQgFokQDPuLhIPGdmK2Z6/LtnQqi2+x6twXRO63L1zo8vbsWRNdFR/GUrmZr3RVE14ToNma+s8lM8un9Zba2p8yoWMqAidO4a+Yscp54GaFppD98A7NeXMOJL/2W92bPZdK5J7PxjpuIGJLDbr+S++esJhrwMXrSMAYH1lM1+zlWtoTp57Ix+KSRBEon8MbKahq3bsCIRXBl5pFT7GXCgCxKPHbsDRsJlK+naWMz20MxWmIGcZm8Rl/Dle0iLS8NV24mrtxMtMxcpDsD6fQQjBqEYhJ/JG6arYVitIZj7QXRYxHDWqMv2+P6Riyyg5EfpFYQfXfxfBXv3wkCM3+WQuuLqJm+QqFQJCEAe4qlEPsiatBXKBSKJA728I4a9BUKhSIZ0XdDN6mgBn2FQqFIYndeVX2dPhG4qvOFuXzbCL79gx/wx4EVpOsa91X3w+728MO8Gt6qCXDi3Wfx09kriAX9XHzGSNpe+isfNQQJxiXjpvQnNmE6sz/ZQnPFGmwuD4WDS5g6LA937VpqPl1N9bZWtrZFiRgSj02jxG0ja0AGmUNK0IsGERAuqlrDVDUHqWkOEWyNEA5FiYX8xCMhjG7M1nR7RxI3Ybpmc9g7TNZ003TNlmy2ZgmzOpK45qxDF3RO6FrJ22SztXaDtaTqWZ2SsuwowurObK07kh+3g7BrJ4/pzf84w658kS9+dRSjvE6m3vYWt//yEh67fwH9XHaejo8m5Ktn1oVjmfPsSk4ty2Dr8FP56v1P8BYP4foThlH/4j9Y/cIKfFGDw/PSKDrtZJZU+Vn9VR2BugqEpuMpHMTgAVmMLczA1byV+NY1NK+voGVbK42RzmZrHptGtsNmJXG9uHIzsGXloHuzMBwe4jYXwZgkEInTGo7RGrEqZkXitEXiRJLEWQmjtXgs1i7MkkbnillmMzq9J12FWZ2uqaTtHpH4/7a71hdRM32FQqFIQgiw631iPrxXqEFfoVAokjjYwztq0FcoFIou9NXQTSr0id8wRYUeXnjoMd4+J4tZJ97ANQ9fyIN/eJHTLzmHT2bewHCPg/iFv+DL+e9TMHoyVx9VyvK/voM/ZjDc42D4RSfxzqZmNq2sJhrw4SkayJhRBYwv9hBZ+RHbl1WyKRClKWrGPbPtOrn56WQOKsBROph4ZhENwTjVrWG2NLQRaAnT5o8Qbm0hGvQTiwQxYpH2/iaEWe3iLLtVOMXp7myyZtPQLGFWIo7v7CLQ0oRpuGbTNSuWD3Zd7BDbT47ja3QWYyWM1hJogi7Xu/+E768FDHszqfLXbOL5YVO5+POX2LbkTa6Nf4wuBD988HzueGg+I0+ajv1fv2JDIMIxd53D7f9dQ2v1BoZOPJKp+TFW/Xsxn1a1kmnXGHLyYOTYacxdVUPdpm1EAz5cmfnklhVw9LA8BmY5kBVrCK1bSdP6Oup8oXZhli7AY9PIceiWMMuNKzeTtIJstIxc8OQiXV6CMYNgzDBj+RFLmBWK0RqKmsKsqNmMuCnMMuKdi6cYxo4FVLojVWGWYucIROdc2S5aSs8nxClCiLVCiHIhxC3dXL9eCLFaCPGFEOJdIcSApGtxIcQKq83p+ti9Qc30FQqFIpkedNkUQujAw8BJwDZgiRBijpRyddJtnwFHSCnbhBBXAb8DvmNdC0opx/VIZyz6xExfoVAo9hdmTD+1lgJHAuVSyo1SyggwGzgr+QYp5UIpZZt1uAgo7cE/ZwfUoK9QKBRJJGwYUmlAnhBiaVK7osvTlQAVScfbrHM74zLgjaRjl/W8i4QQZ/fE39cnBn1/TglDj5vOK0fMYE1rmI8mXU1bQxX/nD6A5z/Zxrk/nsR1r63GX7OZaaePxbngSd5f38jANDtHjS3EdsL3eWrRFpo2fo5mc1AwZDinHFJIXlsV9YuWU1veSFM0TjBurtEvcplr9LOHl2HvP5ygM5vt/ghbm9qobg4S9EcIBSLWGv1gt2v0ha6ba/TtHWv0dZvNiud3XxBdF6I9nu+waTh0DbvesUbf3h7X7zBaS16j38lwTexbQXRtJzH/VNfo9zaf/vtGNgSifPvp7Uy78lJmnXsvV95xMutPuYna1R/xz6uP5vU75zIxx0383J/zwRvLcGcX8ePTRxJ67VE+KW+iKhRjQpaLAdOPZ02rxkefV9NavQGA9Pwy+vXPYkJxJhmhesLlX9D41RaaNjWzPRTHH+taEF0jLc+NO9dDWkE29qws9Ox8DJeXuNNDIGoQihlmPN9ao+8Pm+v0g6EYsWjy+nwDw5Dtn6uua/Rhx4Loe7pGX8X8d4HA/P+VQgPqpZRHJLXH9/plhbgYOAJ4IOn0ACnlEcBFwB+FEEP25U+DXhz0hRAuIcSnQojPhRCrhBC/ts4PEkIstpIazwshHL3VB4VCodhTEpOlHkrkVgJlScel1rnOrynEicDtwHQpZThxXkpZaW03Au8B4/f6D7PozZl+GDheSjkWGAecIoSYCNwPPCSlHAo0Yf6cUSgUigME69d0Ci0FlgDDrMmuA5gBdFqFI4QYDzyGOeDXJp3PFkI4rf08YDKQnADeK3pt0JcmfuvQbjUJHA+8ZJ1/CuiROJVCoVD0BD0505dSxoCfAG8Ba4AXpJSrhBB3CSGmW7c9AHiAF7sszRwFLBVCfA4sBO7rsupnr+jVJZvWcqVlwFDMZUsbgGbrjYBdJDWshMgVALlFJaT1ZkcVCoXCQlhamJ5CSjkPmNfl3B1J+yfu5HEfA2N6rCMWvZrIlVLGrTWmpZhLl0buwWMfTyRHmlqjLL97Cu/Xt/Gzm6Zw+a9f46gZ/8eaK2aS49Ap/uWfeevl98kZPJY7pw1j+e9eZHsoxtGH5jPmsql80qDxxbIqgk3b8RQNZMTofI4uyyT25ftULd5IuT/anpjLtusU57rJHpaPa+AQ4pn9aAjGqPAF2dLQRktziLbWMOGAn0jARzwS2iGJm2ywlthqdgearmGz62ZzmNtkQVanJK6tI2mraQkhFu2CrY5KWp2Tt7CLilhCpCzM2ldSF67s3fNvmXo8ty24n2UvPsNrJ+is84dpvOS3XHjfQgYcfSYjlvyDRY1BTr35RO6cv4H6dUsYNPEYZozM4su/L6QiGMVj0xg5ZQC2SWfzysrtVK2vJOSrw+nNIaesjMnD8hia40JsW03jyk00ra2mrj5IUzROxJBJwiwNT7aL9MJ03PnZOHJz0LML0Lw5pjAragqzfFby1h82hVnBSIxwkjArFjXMillStlfLMmKRTsIsUEnY/UFiUcTuWl9kv4izpJTNQoiFwCQgSwhhs2b73SY1FAqF4utE+9rWpfU+vbl6J18IkWXtuzEVaWswY1PnW7fNBF7rrT4oFArFniJQM/29pRh4yorra5gJjLlCiNXAbCHEPZjy47/3Yh8UCoVijzmIC2f16uqdL6SU46WUh0kpD5VS3mWd3yilPFJKOVRKeUHymtSdYXN7WHDIMfzsZ8dQc9WD1K9bwps/OpJ/vbKW714yjhve2kLz5pUcd+YkCpY+z7vLqunnsjH2iuNxn3E5j320ibq1y9BsDvKHjubscSUUR+uo/2gRNSvrqAmbeWW3Lihx28genEXOyIE4Bo4knJ5vCrOag2ypD9DWYsbzowEf8UiQWLgbs7UkYZZmc6A73NgcTmwOfQdhltuhd1s8JSHMsmtWs4RZdq1DmNVeRCVJmNVeSAXzWsJsLZXiKX1FmAXw5roGTl1SwKSLv88zR83k2uuO4dx7F7D1k7k89rNj+O+VTzI200XGtQ/wn/8sw+nN4crpo4nP/Ssfr6jBrQvGZjoZdsFU1sWyeHtZJS2V6wDwFA6kaGAWkwZkkxtrIrLuMxrWVNKwvontoVgnYVaGTSfHoZNekE56gZe0gmz07AL07AIMdyaG00tb1CAYNfCFY7RG4vjaou1x/Ui4szArsZXGroundCfM6i7mr4RZe0GKs/y+OtNPadAXQpwrhFgvhPAJIVqEEK1CiJbe7pxCoVDsb0TPrtM/4Eg1vPM74Ewp5Zre7IxCoVAcCBzM4Z1UB/0aNeArFIpvCgfxmJ/yoL9UCPE88CqmvQIAUsr/9EqvFAqF4mviYC+XmGoiNwNoA6YBZ1rtjN7qVFcOLcvkjYoWaq75E+fc+h8mXvRd1l96Hh6bRv/fPclLzy4kZ/BYHpg+muW/eYqqUIwphxWQNv0KFrWms2TxNtoaqvAUDWT0mEKOHZCF8eV7VH5cztrWCP6YgVsX5DlsFOe6yR1RgHvIMOLZZdS2xdjcFGRjXaBdmBUN+FISZtkcbnSHO2VhVnJLRZiVSNRCZ2HWzn6aHizCLIB7F9zLB//4BwvP8bC8OUTwxofZ9OEc+k86g0mrn+Od2gBn33wCt7yxnpqV7zP46OP5wWH5fPaXeWwIRJiQ5eKw4wdinzKDl1dWU7nOFO85vTnkDhjI1FEFjMpLQ6tcTf2KdTSsb6K2NkB9ZNfCLGdBninMyszDcGfSFpMEkoRZLaEoraEY/lDUEmaZyduEMCseN0xBVjSyE2GWkfJ7pBK2e8/BnMhNaaYvpfxBb3dEoVAoDhT6hOf8XpLq6p1SIcQrQohaq70shOjV6i4KhULxdSCsX9WptL5Iql9o/8C0A+1ntdetcwqFQnHQcTCHd1Id9POllP+QUsas9k8gvxf71Qnfl2u4+Y5pnHPjs9SvW8LbPzyMWS+u4fs/OpIrX99I48bPOeW8b5P/8VO8/WkVA9PsTLj2FD7wufnje+XUrlmCZnNQOOwQLji8lJJINbXvfUjVl7Xtwqw8h40St43codnkHjIYx+BDCKXnU9kSYVNjWydhViQFYZbudLcbrfWWMCshxkoWZiVXzEoWZiVPSvq6MAvg+I8LmfrDy3jy8Iu58ZfTOP2Otxl87Fk8d8tUXrr0Eb6V7SLt2t/z0guf4MrM56fnH0r0pQd4b/l2PDaNcScNYvhF01gTzWTe4gqaN68EwFs8hJLB2RwzMIe8aAOhlYuoW7mN2toAlcFdC7PSi3PRswsQWXsizDLN1pKFWTurmJUsvkpFmNUdKs6/ewTm/5FUWl8k1X43CCEuFkLoVrsYaOjNjikUCsXXhRAipdYXSXXQvxT4P2A7UI1pmKaSuwqF4uDDWgGXSuuLpLp6Zwswfbc3KhQKRR9HAD1YQ+WAY5czfSHEz63tX4QQf+7a9k8XwR83+ODsO/BtW8dZV1/KsjPPpp/LTtZdTzLn6bkUjJ7Mg2eOZNEvn2Z7KMbUo0uxT7+W37+znuWLKgg2bSejdDgTJhRz3IAsosvepuKD9e1r9D02jf5pNkoK0sgd3Q/30JHEcvpTE4ixudlco+9rDBJoCRFpbSQWChANBXaI5yfW6OsOd/vW5nB2rM9PWqPvdug4rbh+mkO34vtmPN+M32vYdK19jb5d76ZcmxVZ15Ji+x3/djsare3JGv29/em6P9boAyx54VleH1NBRTDKmovuoXLJPF69bSrD3nyAjxqCnP/A+Vz18krqvlrEiKkncPEQB0t+P4+KYJSJOW6GzTwbfer3+OeSCipWbSTkq8OVmU/+oAGcPKaI0flpsHkFdZ+tp35tA5XBWKfiKZl2nRyHhjc3DW8/D2lFuTgL8tFzi8x4flo2gZjEHzVoDEbxhWI0tUVobovS3BYhGDKN1mLWWv1EbH/XxVOMXRqo7c5oTZE63+TwTsJ6YSlm2cOuTaFQKA4qzIUQPRfeEUKcIoRYK4QoF0Lc0s11pxDieev6YiHEwKRrt1rn1wohTu6Jv2+X4R0p5evWbpuU8sUuHb2gJzqgUCgUBxo9NYe36ok8jFlEahuwRAgxp0uB88uAJinlUCHEDOB+4DtCiNHADOAQzKXy7wghhksp9+lnXKqJ3FtTPKdQKBR9nG5CqTtpKXAkUG7VEYkAs4GzutxzFvCUtf8ScIIwY0dnAbOllGEp5Sag3Hq+fWKXM30hxKnAaUBJlxh+BhDb1xdXKBSKA449E17lCSGWJh0/LqV8POm4BKhIOt4GHNXlOdrvkVLGhBA+INc6v6jLY0tS7tlO2N3qnSrMeP50OsfwW4Gf7euLp0rJ0CKu/NnD/OS2K7lvdIAf/2Ar9z12EWc/uYRAXQXX3fAdtOfu4b8r6zg0w8nYGy7klY0BVi7aQEP5cmwuD2WHjuaiI8ooaF7P5vkfsHl1PVWhGLqAQqeNkn5ecoZlk3fYEGyDDsVnz2JrY4DyOj+ba/34m0OEW1uIBHzEIsF2AU0CzeZAtzvQHS40u5XMdbqTkrha+9ZhJW3dDhsOvbPRml0zTdZ0gZXA7TBf6yrMSnwwk03XunMITDZaS1WY1fXxyezs/8P+dCb89e9/zt0nncJtL/yUAT//F4df8F28j97EEw8s5MzSDOqm38xbM/+Mt3gI98wYR9MTd7NwXQP5Tp1xFxwCx36XD6vaWLB4K80VaxCaTmbZKEaOzOO4gblk+yvwf7aI2s+3Ud0QpD7SIcxy6xoZNo1Clx1vPw/pRVl4SvLRc4sRmQXE07KJ29Pwt8VoDcfxhWL4wtF2YZY/1CHKMuKSWMQUZ8VjsXajtV0Js4BOwqxUUcnd1BBSIlJ/r+qllEf0Zn96mt3F9D8HPhdCPCOlVDN7hULxjUDI1N1Md0MlUJZ0XGqd6+6ebUIIG5CJKX5N5bF7zO6WbL5g7X4mhPgiqX0phPhiX19coVAoDjwkSCO1tnuWAMOEEIOEEA7MxOycLvfMAWZa++cDC6SU0jo/w1rdMwgYBny6r3/d7sI7P7W2+807X6FQKL52pOyhp5ExIcRPgLcAHZglpVwlhLgLWCqlnAP8HfiXEKIcaMT8YsC67wVgNWYO9ep9XbkDuw/vVFu79UBQSmkIIYYDI4E39vXFU2VjJA1nZh53py/jxUn3cVqRh/Wn3MSn37mTocdN59Zxbl67+DUihuTEC0bRevTF/Omvn1C3ZhHxSJCC0ZOZNrE/xw3IpO3FR9iycCPr/BEihiTHoTMo3U7BmHyyh5fiGjGOWN5gqv0x1je08VV1Cy1NpjAr7DeFWYm4awLN5mgXZ7UXT3G6sTnsHYIsR4dAy2HTSHN0U0RF19pj+DZrf1fCLK09Tp9cTGX3RmudBFvdvN+9LTrpiaef8eY9fOx1crs8nmDDP3nv2su4J+8K/DGD6176Hd9+bDGt1Rs4+aofcpJtM689uIC6cJzzRuYy8LJLeX1jC7OXVlC5ahXRgA9P4UD6DSvhtDHFjMpzEf9kMTVLv6L+q4Z2o7W4TBitaeQ7ddIL0/AWe/CU5OMoLEbPL8FIzyFqc9MWieOPmMKslnAMX1uU5qApzAqHY0TDcVOYFYmbhVMSxVMS4qxkwzUj3kmYZXQjwtqdMEvF8/cAKVOdxaf4dHIeMK/LuTuS9kNAt0vgpZS/AX7TY50h9SWb7wMuIUQJ8DbwPeCfPdkRhUKhOFAQ0kip9UVSHfSFlLINOBf4m5TyAkzBgEKhUBxkSDBiqbU+SKqF0YUQYhLwXUz1GJjxKYVCoTi4kPRoeOdAI9VB/zpMBe4rVnJhMLCw97rVGV9tHSsevow/D/8Wm9si/OWrZxh+30Lsbg9/u3oSG269nHdqA5xR7GX4rbdxx0dbKF+8nHgkSFpuP4YeMZSLJpTgWP0uq+YuZvUWH3XhGA5NUOa2Uzwsh/yxg8kYPhhRNor6mJ11DS2srmqhqi5Aa2OQsK+OaMDXXjglESMVmo7QdGxON7rDhe40i6HrDneSwZq1Rt+h4Ww3WLPhticZrSXW6AvRXkDF3NfQ289p3a7RTxRD31moPBHjN/c7TNqS2V9r9HsqXXDf7/7Hn5uXcumJv+CX913Ppyedhi4El14witn2I/jiv7+j3+En8/D5Y1h9zXdYWNfGKK+T8VdNYfuAY3j42RVsXl1La9UGbC4PuUPGMHlsMZP7Z+Gq+oKaxYup+Xw7m1rC1EdixK28nsemke+0kZ/pIqM0A09JHukl+ej5JUhvHkZaNq0Rg0DUMNfmh2M0tEVo8EfwtUXwh6x4frTDaC0eN9foJ9bkx63YfrIWJLlwCrDHa/QVe4KEPShA39dI1Vr5f8D/hBAeIYRHSrkRuLZ3u6ZQKBRfD301Xp8KqRZGHyOE+AxYBawWQiwTQqiYvkKhODjpuXX6BxyphnceA66XUi4EEEJMAZ4Aju6lfikUCsXXg5RwEC9xTXXQT08M+ABSyveEEOm91CeFQqH4WvnGh3eAjUKIXwohBlrtF8DG3uxYMuk5uWg3XUQgbnDt5RO49ksvWz+Zy4kXn8XETa/zwjMr6eey8e27z+IjMYQX563Ft3UNGaXDKR5zJJcdN4SRWiM1c+ew5YMKNrdFiUvTaG1wQRpFh5eQOW4cztFHEsrqzxZfiLV1flOY1RCkzddCpM1nCrNinY3WhKaj2R1oNjuaPUmYZdexO23YnZ0N19xWEtehdwiz3A4du2aKsRJJXLtuJnbN/STDNa1DmCWsilnQYbTWVZjVXeJ0V0ZrycKsAzWJC/Dz645mzC8+pPRb07g+OJ9nFlVyxe0nMfQf/+G2h95Btzu4+fKjyJv/F954bT26gONOGkjmjGuYtaySdUs3UffVUoxYhMzS4QwYlc8ZhxTSX/gILX2X7YvXs21jMzXhGMG4WS3LrQuy7TpFLp2MUi+ZA7Lx9i/EVtgfPbcfRnouAUOnJRLHH4lT3xalKRil0R/BF4zS3BYlHIwSi8Tbk7lmErdDmNVuthbvLMxKJpHEVcKs3qJHbRgOOPakMHo+8B/gZSDPOqdQKBQHHwfxoL87P30X8CNgKPAlcIOUMro/OqZQKBRfCz1sw3CgsbuY/lNAFPgAOBUYhblmX6FQKA5KBAd3TH93g/5oKeUYACHE3+kBW8+9YXim5K/PruIPz17OthOu5anz76L/pDN49oIRvDP6h9SEY/zoO6PRLrydXzyymG2f/Q97eiYDJ4xn8rh+nDk8h+h//8T6179gRXMIf8wg064x1GOnaFwhhUeOxj78cGLZpWxrjbK61s+qSh+NdQH8zUFCvjqigZZ2YVaChMmazRJj2V0ebG4PdpcLh9OWVDhFb4/nm8Ksjq3boVtGayIphr9rozWRFM9PCLO6xvOT6c5orTt2Fc/fGfuzcEoyr557N1tvfJDqdx/gwYKxnDMsh+bL7+eiRxZTs/J9Js+8hB+WBnjrgufYEIhw1oBMRt94BQua0nj53dXUr11CLOQnLbcfJaOHMePIMr7VzwPL3qXqg8/YvqKGTYEojREzHu7WNTw2jSKXTlaxh4xSL97+hbjKyrAV9SfuySPi8NIajNMSiuMLx2gKRqn3h2kIRGhuixC0hFmRcKzdbC0WiZqiK8vEb2dGa+0mbHsYz1fsDRIOYvHb7mL67aGcPS2iIoQoE0IsFEKsFkKsEkL81DqfI4SYL4RYb22z96LfCoVC0TskbBgO0pj+7gb9sUKIFqu1Aocl9oUQLbt5bAwzBzAamAhcbVV3vwV4V0o5DHjXOlYoFIoDhoPZZXN3fvp7bapmefFXW/utQog1mEV9zwKmWLc9BbwH3Ly3r6NQKBQ9yzc7kdsjCCEGAuOBxUBhUnGW7UDhTh5zBXAFQKaw8bcTj+HpQRdz/21voDtcPHfLVNb/6Lu8vq2FswdnM/q393LT/A2sfPcjogEfZUedzvemDeOkIXmkr3qbVS+8x8r1jdT8f3t3Hh9XWTZ8/HfNPlmaNEmbIELZ1QAAIABJREFU7k3The4WKEgBCy1lqRap6AP4yIP6gIiv+upHQbb3dUMURQR9BKGKIIqAFMoiSNkKpchWSlsKpfuWNGmWZs/suZ8/zsl0kmaaKW0zM8n1/XzOJ3OWmXMOpHfOXPd9XbddaK0sx8PoKSUMP3kSvumnECk9jrpAjA9qmlm7p4kdlS207A8QaKgh0tZEJNDaczw/SaE1t8+Jxx6n73Q58PtcBxVa6yy25nYIPnvcfnx8frdCa27ngVi+09E1nt9TVP3AOP74f8/4djh4jH6v8f5D7u3d0Q79X//dW7jt9zew5tQziRnD/JWPMv3ml9n99guMmbOIh756Ihv++0KerWxm+iAvc274DFWTzuWWv65h95q3iAZbcfnyKJ0ym/mzR3FWeRG5FWuofvVVKt7cw7aGYLzQmschlHicFLidDCnwUTi2gIJxw8gfOwJX6WhMQSkducW0hDtoDseoaw8fVGitsTVMOBglEkosuBaLF1briIYPKrTWPZ5/KDo+/yjTRv/jE5E8rLH93zXGNCc2LsYYIyI9zktmjFkCLAEY6fAdnbnLlFKqN/28DEOqyVkfi4i4sRr8B40xj9ub94nIcHv/cKDmWF6DUkodHoOJRlJajkQqg1pEZJaIvGEPhlkvIhcn7LtfRHaIyFp7mZXKeY9Zoy/WI/29wEZjzG8SdiXO/P5l4MljdQ1KKXXYDNaTfirLkUllUEs7cJkxZhpwHnCHiBQm7L/GGDPLXtamctJjGd45DWsu3fdFpPNibgBuAf4hIpcDu4CLjuE1KKXUYTGYvpqkptdBLcaYzQmv94pIDVZJnMaPe9Jj1ugbY1aRvP/vrMP5LJcDhj38NNf9x88JNtVy4y1XM/G5W/nZPzYyfZCXM+/8Bo82DWHpspdpqdpG8YQTOGfBBC6dOYzC+s3s/PvDfLhyD5tbrY7Y0X43x40ZxKhTx1P4yTl0jJ3FrpYIlc0h1u9tZmNlE421bbTtbyDYVEu4vZlYONBltqzunbidiVlW560rYdYsJx670zbP58bvPpCY5XE57A5cJy7ngU5chxxcaE0EnHYRte6duL0VWuutE7e7TC601umEz1/C556/hZver+H2Zd9h4dIqdqx6ivzh47nzO6cj91zHY89spcjj5Lz/+gS+S2/k+me38tHr62mr3UNO8Qjyh09g1okjuHjWSEaH99Ly2r/Y8+pH7NzRyJ5AJF5orcjjZJjPRYnXxeDyQgrGlVAwfiSuEeNwDBlDNL+U5piTplCU2rYwde0RmkIRaptD7G+zOnPDoaiVlGXPltW9E7enQmvQLfkqFtNkrL5gOJyZs0pEZHXC+hK7PzIVKQ1q6SQiJwMeYFvC5ptF5IfY3xSMMaHeTtono3eUUip7HFZHbp0xZnaynSLyIjCsh103djnjIQa12J8zHPgr8GVj4kOLrsf6Y+HBGvRyLfDT3i5YG32llEpkzBF30h74KLMg2T4R2Sciw40xVYca1CIig4BngBuNMW8mfHbnt4SQiNwHXJ3KNR3T0TtKKZV9TLf6R8mXI9TroBYR8QDLgAeMMUu77escBSnAYmBDKifNiif9kmkTOf27S3H5cjl98UKuL9zEHd9bit8pXPzjT7N55sXc9OuV7Ht/JXmlZcycdzzfm1tO/tqnqFn1Gh89/iHrmoKEOwwjfC6ml/gZfdoYhn7qZGTSJ9kby2FddTO7GtpZs6uB+uoWWvY3E2isJtLeTCx0cDzf4fYcFM93ez24vS483gMTqPh9LjwuB/nd4vl+jxOfy9l14hQ7KcthF1vrTMrqPnFKqvH8xOJrH3filGTSGc8HeHVeI//31OX84Lun8tuCRay6+TbK517Af312Cmdse4x7fvY8TZEOLj17HGU33MTv3q3mX899xP7t63DnFjBs2kkMHzeYr5wylhn5YcIvPc3O5e+yZ30N29rCNEWsb9AFbicjfC6GF/vJG5pL0YRiCsaPxDt6HK4R5UQLhhFw+Ghsj1HbFqGmLUxtW4im9gg1LSHqW0MEAxHCASsxy4rrx4iGQ8TsAn7xxKx4UtaBxCygS6G1TjpxyjHUOXrn2OtxUIuIzAauMsZcYW+bCxSLyFfs933FHqnzoIgMwfpnvRarDH6vsqLRV0qpvmMOpyP345/FmHp6GNRijFkNXGG//hvwtyTvn/9xzquNvlJKJTL01ZDNtNBGXymluujfZRiyotH/cF8Qx4513HfXNVxY0sJDM69kbzDCN686ifBXfsbX/uffbH/9Obz5RUw583R+tmgq5XXvsumPD7L33WrerG2jKdJBkcfJjAIvY+eOYeT8k3HNnEudbyjv721l9a4GdtW3UVXZTFNdO+31lYRbGroUWkscn+9weXqcOMXrd+Hxu/H6XXi9LvLtmH5PE6f4XFaRNW/nGP2EcfrdJ07pPlY/WTy/06Hi+Yl6i+f3XMwtvfF8gP9/xjV8ed5Ytlx1Bzd/7VeUTDqJJ26Yx8T6NSw983/Y2BLiohlDOfE3P+TR2jz++Pi77Ht/JQ6Xh6FTT2Pep8r41Phi5o0dRMdrf2f3v1axZ1UFHzaH4hOnFLgdjPC5GF3gpXjCYHJLcymcNJrc8nLcYyYRGzSMkLeA/e1R6gMRqlpD7GsNUd0YpCUUpb41REtbmFAgSigYIRI8MHFK4vj8xHi+NV7/yCZO0Xj+ETqKo3cyUVY0+kop1Xf0SV8ppQaOvhu9kxba6CulVAKDwfTB6J100UZfKaUS6ZN++oVaGvn1L77F/FduY/mtL/Dm/gBXXTyV0l/9hUV/eIsNzz+Lw+1h0hnz+cnnZ3BidBvb77qLd5/dyo62CLWhGAVuB58o8FI+dwxjzj0J70nn0Fgwjg3Vbby5cz/vbKunvTlEw75W2mp3H9SJC8Q7cV2+XBwuD57cAty5BXhycvH63Hj8rnhyltfrIs/nIs/ntpKz4uvWzFlee8aszoQsX+e6s7PgmiNecC2ekEXyTtxOibNlQc+duD3NlnW0i6wda/PLCin4+9N85it34BtcyoM/vYD8u6/huT+9wYradi4YW8Dp91zLi86p/OKB1ex680VMR4yh005jzulj+fqcsYwf7MWx+kl2P7WcHS/uYF1jkH0ha7asPJeDET43ZbkeiiYWUXRcKbnDi8mbOAF32RRihSMJ5Q5hfyBGfXuUqpYQNW1WJ25VU4D2cIym1jDBtgihgNWJGw5FiYTCxEIBYuFAvBM33mmrnbiZwRhMJNz7cVkqKxp9pZTqO32TnJUu2ugrpVR3/fgbkzb6SimVyJh+HSbLikZ/2MhSLt98Hz+/ehlNkQ6+fsEkJtz3OIuWvMPqZU9iYjGOm7+QH18yi3mevez4za95+5ENrGkMEogZO57v47hPjaZ80Sfxn7qIpuJJrNvXxmvb63ljSx21Fc0E28O01VYQaqoj3NZELByIX4PT44/H813+PJwuDy5fXpd4vtdOyvL53eT5XBTmeMjzuvC6HFYs3+OMx/OtyVOsCVQOxPatWL5DpEs83+ngQIIWPcfzHdI1np+YrJWOeP6xDv1P/PernPzVOxGHkwd+eRmTH/sJv//Fi9SGYiwans/8+3/AG0PP4Lo/v8PWVS8QCwcYOvU05px5HN+fN5Hpjlo63nuPPUufYOu/trCutr1bPN/F+DwPQ6aVMGT6cEpmTsBdXIKnbDIdxWOJ5A+jvj1KXXuUiuYg1a0hKvcHqGoKUtMcIhyOEWy34vnhQDRpPF+TsjKTjt5RSqmBwhhMTBt9pZQaEIwxdESi6b6MY0YbfaWUSmTQJ/10Gxqs46ar/s74XA+nnjOO8fc/zsI/vMU7jz2BicWYsuDT/PzSE1jgqWDHrbfwxsPv805DkJix4vknFPqYfOZYyhd9kpy5i2ksnsR71W28srWO1zfVUlvRTGPVPsLtTSnF8z05BTi9/pTi+Z0F1xLH5yfG8xPH53eOzXfI0Y/npzppSjbE8wFmX3o7DreHR393JZMf/iG/vel5nALnjxrE2Q//P1YOncfV977N5hXPEQsHKJ0xl9PnT+YHZ01kuuyj9em/ULd+K1v+uYl1te3sCUS6xPMn5Xu7xPN9k6bjHDyUjpIyIvnDqG2PUtMWoaI5SGVLkMr9ASoa2qlpDtHWEiYaiaUUz49PiK7x/Iyijb5SSg0Qxhg6tJ6+UkoNHP159I5OjK6UUons0TupLEdCRIpE5AUR2WL/HJzkuJiIrLWXpxK2jxORt0Rkq4g8Yk+i3itt9JVSKkHn6J1UliN0HfCSMWYi8JK93pOAMWaWvXw2YfsvgduNMROABuDyVE6aFeGdvXsaOGlIEZ9f/hv2lX2Ks369ivXPPIHbn8eMRedyx38ezwmt69j0o9tY9fQW1jUFAZiU52VMjovjzimn7PxP4ZnzGWrzy1hd0cLKrXW8tbmWmopmmquraa+vJBYOEm5r6nGmLJcv1y6uZhVZc7oceH1ufLnuLjNlFea4yfO54524eZ0zZ7md5NgduZ0zZfXUiet0HP5MWT117ELfd+L2ZS02/+BhvHTHJbhuuoJb736HUq+Ly65fQOmFF/NoZCI/uesNdv57OQAjTjyXsxdM4HtnlDMxsJ39y/7C5sdW07C9kTUNgXhSVoHbwWi/m/GDfQyZWkLJjFGUzByPp3wajtGT6fDmE8wdQm1bhJq2CLubglTZnbhVTQGqGoME2yIE262O3GRF1qLhACYW007cDNbRNx25FwBn2q//ArwCXJvKG8X6hzwf+M+E9/8Y+ENv79UnfaWUSmQP2UwxvFMiIqsTlisP40ylxpgq+3U1UJrkOJ/92W+KyGJ7WzHQaIzp/LpRAYxM5aRZ8aSvlFJ95vAycuuMMbOT7RSRF4FhPey6sespjRERk+RjxhpjKkWkHHhZRN4HmlK9wO600VdKqQSGozd6xxizINk+EdknIsONMVUiMhyoSfIZlfbP7SLyCnA88BhQKCIu+2l/FFCZyjVlRaM/OMfNhRue5avP1/H2n19gx6qnyB8+nlMXz+d3F05nxPplvPur+1n1egWbW8P4ncL0QV5mnDyC4uOGMnLhfJwnnMMeRzFv7Wzk5U21fLB9P3WVzTRXVxBoqCbU0hAvfAVWPD8xKcuTW4A7pwBPbj4evxun04Ev143X78bjc+H39RzP93ucuB1W/L4znu9zWTF9K7Z/IJ7fJYafQjy/M4aeSjxfugXcszmeD7D1vst479xzeGDlbk4p8nPRXZex84xvct8H1dzz15fZ9/5KPLkFjJl9BhcvnMTls0dRuvt19j76CJuWrWf97iYaIjFqQzGcAkO8Tkb73YwblsuQqSUMmVnG4Knj8UyYCcPGEy0cRXvUUN8aZW9LiMrmIJXNQSr2B6huClDXHCLYHiHYFiYUiBKLdRAJRYkEg/F4fixqJWMdKLIW6RK3P1Q8P1ncXuP5x4AxdIT7pAzDU8CXgVvsn092P8Ae0dNujAmJSAlwGvAr+5vBCuALwMPJ3t8TjekrpVQiAx0dHSktR+gW4GwR2QIssNcRkdki8if7mCnAahFZB6wAbjHGfGjvuxb4nohsxYrx35vKSbPiSV8ppfqKoW+qbBpj6oGzeti+GrjCfv1vYEaS928HTj7c82qjr5RSiQzxMFt/lBWNvnfiJObcuYkNzy6jIxpm+PELuPJLs7n6lOG0P3AzK3/3Ait3NFIbijHE6+SkwX4mnlfO2EVz8ZRNITZ5LhubOli5q44VG2vYuaOB/ftaad23I15grfsE6E6vH5fHjzt3EG5fXnwCdF+OB6/fhcMep+/1u8jPcZPvc1Hg95Bvx/HzfC5yPS58LmtSFK/Ljut3i+MnToDeOTbfgR2/t+P6hxqbD90KryX8dzuSeH6mxvI7LR11PK/XB7jkxOHMffh2Hmwexc9ufpm6bR/QUrWN/OHjmTx3Dt9eeByLJxbCygfZ8sg/2fLcdtYmTIDucQilXhfjct2MKi+0xufPHE/+lCl4yqcRLRpLKKeY2rYogYiJF1iraAhQ0RCgpjlIY0soPj4/EowRCkYwHYZIsJ1Y6MDY/ENNmAJWQ6Nj8zOB0TIMH4eI/FlEakRkQ8K2lNKOlVIqbQ5vnH7WOZYdufcD53XblmrasVJKpYUxhlg4mtKSjY5Zo2+MWQns77b5Aqx0Yeyfi1FKqYxi7PBb70s26uuYfqppx9jpzFcCjBw1useUNqWUOup05qxjo5e0Y4wxS4AlAO7BY0zNU48w7BPzGD15ZLzA2uZvXc1rT2yOF1ibku/l+CnFTDj/Eww5dyEdU86gLuZi9e7WHgushVoaiIUD8U6xQxVYs2bGsmfJ8rlxeRxJC6zl+Vz27FhWgTWrqFryAmudSVjxTtteErJ66sCF/l1grbvKQJQf3rQQ73du48JH1rPqyb/RXLEZp8fPyJM+3bXA2j2/ZvNjq1m/oZZtbWFaox14HEKeS5IWWHOOmkRk8BgaYi7qm6wZsppC0aQF1kKBqJWMFYoSCbZjYrGkBdY6k7ISO3AhtQJr2oHbBwyYWNKmKev1daOfUtqxUkqli8H0VZXNtOjrjNzOtGM4jLRhpZTqMwZMh0lpyUbH7ElfRB7CqhVdIiIVwI+w0oz/ISKXA7uAi47V+ZVS6uMwBmLh/htGO2aNvjHmi0l2HZR23OtnxaKccfl/c+dFMxnnbqfx3h/z3G9XsHJfK02RDob5XJxUksOEhRMY89mzcM0+jypPKe/saGZXY4AVG2uo2NXI/qoG2mp3E2qqIxJoPWiyFIfbg9uXi8ufF4/le/x+O57vik+WkuN343E5KMzxHFRcrTMhK7G4mkOsOL4V07di+Q7pmpB1NIurwaFj+YnvSZQNsfxOV296grv35HDb959l77vLcfnzKJ97AcPKCrn6vMmcM8JJbMW9fPjIC2x6eRcbmkNUB60hdkUeq7jaEK+T0vJChs4opWTmeHInTcYzfgbRwaNo8RRSF4hZMfyWIHubgzS1R6hqClLVGKDZTsgKBSMH4vl2cbUOu7BarEtxtYMTsnSylAxljMb0lVJqIOnQRl8ppQYIHbKplFIDhwE6srSTNhXa6CulVCJjtCM33SaWDWX5/Cibr72UlW9X8eq2/dSGYhR5nJxbmsvEs8ooX3wGnjmfoS6/jNV7W1m5dRdvba6lrTlEfVULbbW7CTbs61JRs3sylsPlsWbIsitqen1ufLluPH43Hq8Tn98dT8byuBzkew8kY/ndTnLczngHbmIyVmdHbrKKmk5H8g5coMs2OLgDt8u2ft6B22nGbdvY+e/lAIw48dx4MlbZIDe89ne23/Y0W5/dxpqGQLyiZoHb0SUZK7c0l+Jp4xg0dTLu8ul0FI+lLXcIte1RaurtmbGaDyRjtQSjB1XUDIeiRELh+OxYiclY2oGbnYwmZyml1ACijb5SSg0kmpGrlFIDRx9l5KYyv4iIzBORtQlLUEQW2/vuF5EdCftmpXLerHjSl93b+f0pX2djSwiAYT4X548adHAyVmUzK97axtqt9dTtbaa5ei/h9qakyVhufx6uhGQsp9efNBkr3+eiICEZy+NyJE3GcjutWL7XTsZyOkhrMlY2F1ZLZvc7Kxh7yjl87pyJXHXKGEbVvkf13dew5aM9SZOxyofmMHRqCSXTR1M8YwKOwUMPTsaqbo8nY1XsD1DdFGBfY5Bge4RoOJY0GStqx/M7k7GsRWP52cjQZ+P0O+cXuUVErrPXr+1yLcasAGaB9UcC2Ao8n3DINcaYpYdz0qxo9JVSqs8YQ0ffjN65AKtUDVjzi7xCt0a/my8A/zLGtB/JSTW8o5RSCYyxnvRTWY5QyvOL2C4BHuq27WYRWS8it4uIN5WT6pO+Ukp1cxizYpWIyOqE9SX2XCAAiMiL0OMcUDd2OV8v84vYpehnAMsTNl+P9cfCgzX3yLXAT3u74Kxo9GubQjT5YlwwtoCiiUWMP/8EBi84n9C4U9hQG+CVTfWs2Lievbsbaahu7FJUrTOmCuBweXB6/UmLqrk8Drw+e6IUv5s8n+ugomp5Phc+l9MqoOa0YvluZ+fY/K5F1ZwOwYEVr3c6OPBaeo/jQ7ex+va2ZHH87vsS39NdKrH8TIzjJ1r6p+s4a5gQfekBtnzjJd58xYrjt0Y7CMQMfqdQluNmQp6H4ROLGDqjlKJp48ibPBX3uGlEi8bQ4c2nKgT1gSi797VQ2Rxkb2OAioYANc3Bg4qqdUQ7CIeixMKB+Lj83oqqAfEx+6Bx/KxgDuspvs4YMzv5R5kFyfaJyOHML3IRsMwYE0n47M5vCSERuQ+4OpUL1vCOUkolssfpp7IcocOZX+SLdAvt2H8oEOvpbzGwIZWTZsWTvlJK9RVDnxVc63F+ERGZDVxljLnCXi8DRgOvdnv/gyIyBOtL/VrgqlROqo2+UkolMoZY+Ng3+saYenqYX8QYsxq4ImF9JzCyh+Pmf5zzaqOvlFIJjIEOo2UY0mrY0DyuffBGZNbZBPzFrN/Xzsod9ax4eTW1Fc00VNfTVrObcGvDQUlY4nDi8ufh9uXizi3A7cvDnVuALzcnnnzl9bvx+Fw4XQ4Kctzk+9wU2AlZfo8z3nnbWVDN7ZB4ApaVjCUHdd5qEtaxNfSaS3nkjQo2toTZH47hFCsJa4TPzXH5HkomFTF0xjBKZk7AP2karrFTiA0eRYszj7pAlOr9YZpCVudtZcOBztuWlhDB9gjhQNQupnYgCct0xLp03lodt5qE1R/FtNFXSqmBwQD9uN6aNvpKKdWdPukrpdQA0WEgrDNnpVdb8Uj+T90sPlyyifbm0CFj+E6PH09uAS5fLp7cAquwWpIYfl6O2066clPod8eLqPUUw/d1S8Jy2hOj9BbDdyZMbqIx/KPnz89socjjZHyum/kTihgyrYQhM8vIGTr4oBj+zkCU6pYwlTuCVDbvjRdSawlGDxnDj8fvUyiklixurzH87KThHaWUGiAMRsM7Sik1UGhHrlJKDTDa6KfZrt37+Nutd3WJhTo9flxeP/7BpV2Kp3n9Xjx+a0Jzr8+N0yX4khRP83uc5LqdeF1W7N4p4HU54xOadx9/3xmvd9rB8ENNaH4kxdM0dt+7X9x7Gb5J03GOOo7o4NE0GS91gRiV4Ri7mwJUVYeo/LCOiobd1DSHaGsJEwpGCLZFrLh9KEosGu0yoXlP4+87+4t0/P3AYYyO3lFKqQHDoKN3lFJqwNCYvlJKDTAa3lFKqQHCiumn+yqOnaxo9F3+PMadvsia3crtOJBk5XVRmOMmr6cCaU4HXnuGKyuhytFrB22qBdISO2dBk6vS4Srn+dS8FyK4aj/B9mpCgSjhQIRYrINoOBLvoI3anbQmFot30HZEI/FOVu2gVT3RJ32llBogDNAnU6ikiTb6SimVwGB09I5SSg0U1ugdbfTTatqYQl7/5bnpvgyVQZbe/od0X4Lqr/p5R66j90OOPhE5T0Q2ichWEbkuHdeglFI96XzST2U5EiLyHyLygYh02JOhJzuux/ZSRMaJyFv29kdExJPKefu80RcRJ3AnsBCYCnxRRKb29XUopVQyMZPacoQ2ABcCK5Md0Et7+UvgdmPMBKABuDyVk6bjSf9kYKsxZrsxJgw8DFyQhutQSqmDdGCVYUhlORLGmI3GmE29HNZjeynWWPD5wFL7uL8Ai1M5bzpi+iOBPQnrFcAnux8kIlcCV9qroRy/f0MfXFtfKQHq0n0RR1F/ux/of/c0kO5n7JF8cB3h5fewqyTFw30isjphfYkxZsmRnL+bZO1lMdBojIkmbB+ZygdmbEeu/R9uCYCIrDbGJI15ZRu9n8zX3+5J7yd1xpjzjtZniciLwLAedt1ojHnyaJ3ncKSj0a8ERiesj7K3KaVUv2KMWXCEH5GsvawHCkXEZT/tp9yOpiOm/w4w0e559gCXAE+l4TqUUirT9dheGmMMsAL4gn3cl4GUvjn0eaNv/1X6FrAc2Aj8wxjzQS9vO5oxskyg95P5+ts96f1kGBH5nIhUAHOAZ0Rkub19hIg8C722l9cC3xORrVgx/ntTOq/px5lnSimlukpLcpZSSqn00EZfKaUGkIxu9LO1XIOI/FlEakRkQ8K2IhF5QUS22D8H29tFRH5n3+N6ETkhfVfeMxEZLSIrRORDO238O/b2rLwnEfGJyNsiss6+n5/Y23tMaxcRr72+1d5fls7rT0ZEnCLynoj8017P9vvZKSLvi8jazrHw2fo7l0kyttHP8nIN9wPdx/peB7xkjJkIvGSvg3V/E+3lSiATK4lFge8bY6YCpwDftP9fZOs9hYD5xphPALOA80TkFJKntV8ONNjbb7ePy0Tfwers65Tt9wMwzxgzK2FMfrb+zmUOY0xGLlg92ssT1q8Hrk/3dR3G9ZcBGxLWNwHD7dfDgU3263uAL/Z0XKYuWEPDzu4P9wTkAGuwshzrAJe9Pf77hzVyYo792mUfJ+m+9m73MQqrEZwP/BNrYrasvR/72nYCJd22Zf3vXLqXjH3Sp+f045TSjDNUqTGmyn5dDZTar7PqPu1QwPHAW2TxPdmhkLVADfACsI3kae3x+7H3N2ENkcskdwA/4MCkT4dK08+G+wGr4OXzIvKuXZYFsvh3LlNkbBmG/swYY0Qk68bKikge8BjwXWNMsyRM0ptt92SMiQGzRKQQWAZMTvMlfWwisgioMca8KyJnpvt6jqLTjTGVIjIUeEFEPkrcmW2/c5kik5/0+1u5hn0iMhzA/lljb8+K+xQRN1aD/6Ax5nF7c1bfE4AxphErs3EOdlq7vSvxmuP3Y+8vwEqDzxSnAZ8VkZ1YVRjnA78le+8HAGNMpf2zBusP88n0g9+5dMvkRr+/lWt4CitVGrqmTD8FXGaPPjgFaEr4+poRxHqkvxfYaIz5TcKurLwnERliP+EjIn6s/omNJE9rT7zPLwAvGztwnAmMMdcbY0YZY8qw/p28bIz5Ell6PwAikisi+Z2vgXNAFEEyAAACY0lEQVSw6s9n5e9cRkl3p8KhFuDTwGaseOuN6b6ew7juh4AqIIIVW7wcK2b6ErAFeBEoso8VrFFK24D3gdnpvv4e7ud0rPjqemCtvXw6W+8JmAm8Z9/PBuCH9vZy4G1gK/Ao4LW3++z1rfb+8nTfwyHu7Uzgn9l+P/a1r7OXDzr//Wfr71wmLVqGQSmlBpBMDu8opZQ6yrTRV0qpAUQbfaWUGkC00VdKqQFEG32llBpAtNFXaSciMbuS4gd25cvvi8jH/t0UkRsSXpdJQrVTpQY6bfRVJggYq5LiNKxEqYXAj47g827o/RClBiZt9FVGMVbK/ZXAt+zsSqeI3Coi79h10r8OICJnishKEXlGrDkX7hYRh4jcAvjtbw4P2h/rFJE/2t8knrezcJUakLTRVxnHGLMdcAJDsbKZm4wxJwEnAV8TkXH2oScD38aab2E8cKEx5joOfHP4kn3cROBO+5tEI/D5vrsbpTKLNvoq052DVVNlLVY552KsRhzgbWPMdmNVzHwIq1xET3YYY9bar9/FmutAqQFJSyurjCMi5UAMq4KiAN82xizvdsyZWPWAEiWrKRJKeB0DNLyjBix90lcZRUSGAHcDvzdWYajlwDfs0s6IyCS76iLAyXYVVgdwMbDK3h7pPF4p1ZU+6atM4LfDN26s+Xj/CnSWcP4TVjhmjV3iuRZYbO97B/g9MAGrjPAye/sSYL2IrAFu7IsbUCpbaJVNlZXs8M7VxphF6b4WpbKJhneUUmoA0Sd9pZQaQPRJXymlBhBt9JVSagDRRl8ppQYQbfSVUmoA0UZfKaUGkP8FbxbHCBZ2basAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#这个图和我们原理中展示的横纵坐标是颠倒的\n",
    "def plot_position_embedding(position_embedding):\n",
    "    plt.pcolormesh(position_embedding[0], cmap = 'RdBu')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.xlim((0, 512))\n",
    "    plt.ylabel('Position')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "plot_position_embedding(position_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如何生成mask\n",
    "# 1. padding mask, 2. look ahead\n",
    "\n",
    "# batch_data.shape: [batch_size, seq_len]\n",
    "def create_padding_mask(batch_data):\n",
    "    padding_mask = tf.cast(tf.math.equal(batch_data, 0), tf.float32)\n",
    "    # [batch_size, 1, 1, seq_len]\n",
    "    return padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "#设置3x5矩阵，0都是padding，是零的得到的都是1，非零的位置都是零\n",
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_weights.shape: [3,3]\n",
    "#第一个位置代表第一个单词和自己的attention，第二位置是第二个单词和第一个单词的attention\n",
    "#看不到后面的词刚好是下三角，使用库函数来实现\n",
    "# [[1, 0, 0],\n",
    "#  [4, 5, 0],\n",
    "#  [7, 8, 9]]\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask # (seq_len, seq_len)\n",
    "\n",
    "#前面看不到后面的padding，矩阵下面全部为0\n",
    "# 在mask里，应该被忽略的我们会设成1，应该被保留的会设成0，\n",
    "# 而如果mask相应位置上为1，那么我们就给对应的logits \n",
    "# 加上一个超级小的负数， -1000000000， 这样，\n",
    "# 对应的logits也就变成了一个超级小的数。然后在计算softmax的时候，\n",
    "# 一个超级小的数的指数会无限接近与0。也就是它对应的attention的权重就是0了,\n",
    "# 下面可以看到\n",
    "create_look_ahead_mask(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3缩放点积注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考文档，q是query，k，v代表k和value，q和k做完矩阵乘法后，做mask\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - q: shape == (..., seq_len_q, depth)\n",
    "    - k: shape == (..., seq_len_k, depth)\n",
    "    - v: shape == (..., seq_len_v, depth_v)\n",
    "    - seq_len_k == seq_len_v  这两个是相等的\n",
    "    - mask: shape == (..., seq_len_q, seq_len_k)\n",
    "    Returns:\n",
    "    - output: weighted sum\n",
    "    - attention_weights: weights of attention\n",
    "    \"\"\"\n",
    "    #计算attentions时，我们只用了后两维在计算\n",
    "    # transpose_b代表第二个矩阵是否做转置\n",
    "    # matmul_qk.shape: (..., seq_len_q, seq_len_k)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b = True)\n",
    "    \n",
    "    #获得dk，cast转换类型\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    #然后根据文档中的公式除以dk\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    #如果mask不是空的话，给scaled_attention_logits加一个mask（缩放）\n",
    "    if mask is not None:\n",
    "        # 使得在softmax后值趋近于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # attention_weights.shape: (..., seq_len_q, seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(\n",
    "        scaled_attention_logits, axis = -1)\n",
    "    \n",
    "    #根据原理图，v和attention_weights进行矩阵乘法\n",
    "    # output.shape: (..., seq_len_q, depth_v)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "#调用上面的函数，去验证\n",
    "def print_scaled_dot_product_attention(q, k, v):\n",
    "    temp_out, temp_att = scaled_dot_product_attention(q, k, v, None)\n",
    "    print(\"Attention weights are:\")\n",
    "    print(temp_att)\n",
    "    print(\"Output is:\")\n",
    "    print(temp_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#我们定义一个测试的Q，K，V\n",
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32) # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32) # (4, 2)\n",
    "\n",
    "temp_q1 = tf.constant([[0, 10, 0]], dtype=tf.float32) # (1, 3)\n",
    "#可以把这句注释，它的作用是做四舍五入，让结果清爽\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "print_scaled_dot_product_attention(temp_q1, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q2 = tf.constant([[0, 0, 10]], dtype=tf.float32) # (1, 3)\n",
    "#0.  0.  0.5 0.5 会和temp_v去做平均，因此得到的是550,5.5\n",
    "print_scaled_dot_product_attention(temp_q2, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q3 = tf.constant([[10, 10, 0]], dtype=tf.float32) # (1, 3)\n",
    "print_scaled_dot_product_attention(temp_q3, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  1.  0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[ 10.    0. ]\n",
      " [550.    5.5]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#拼起来再来测试\n",
    "temp_q4 = tf.constant([[0, 10, 0],\n",
    "                       [0, 0, 10],\n",
    "                       [10, 10, 0]], dtype=tf.float32) # (3, 3)\n",
    "print_scaled_dot_product_attention(temp_q4, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 60, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 8, 60, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=np.ones((1,8,60,64))\n",
    "k=np.ones((1,8,60,64))\n",
    "v=np.ones((1,8,60,64))\n",
    "d=tf.matmul(q, k, transpose_b = True)\n",
    "print(d.shape)\n",
    "tf.matmul(d,v).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 256)\n",
      "--------------------------------------------------\n",
      "(1, 60, 512)\n",
      "分割后的q:(1, 8, 60, 64)\n",
      "--------------------------------------------------\n",
      "scaled_attention_outputs.shape:(1, 8, 60, 64)\n",
      "attention_weights.shape:(1, 8, 60, 60)\n",
      "--------------------------------------------------\n",
      "attention_weights.shape:(1, 60, 8, 64)\n",
      "(1, 60, 512)\n",
      "(1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "#多头注意力的实现，多个缩放点积注意力\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    理论上:\n",
    "    x -> Wq0 -> q0\n",
    "    x -> Wk0 -> k0\n",
    "    x -> Wv0 -> v0\n",
    "    \n",
    "    实战中:把三个概念区分开\n",
    "    q -> Wq0 -> q0\n",
    "    k -> Wk0 -> k0\n",
    "    v -> Wv0 -> v0\n",
    "    \n",
    "    实战中技巧：q乘以W得到一个大的Q，然后分割为多个小q\n",
    "    q -> Wq -> Q -> split -> q0, q1, q2...\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        \n",
    "        #这里对应的大Q变小q怎么变，层次\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        \n",
    "        self.WQ = keras.layers.Dense(self.d_model)\n",
    "        self.WK = keras.layers.Dense(self.d_model)\n",
    "        self.WV = keras.layers.Dense(self.d_model)\n",
    "        #这里是拼接\n",
    "        self.dense = keras.layers.Dense(self.d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # x.shape: (batch_size, seq_len, d_model)\n",
    "        # d_model = num_heads * depth\n",
    "        #把x变为下面维度，用reshape\n",
    "        # x -> (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        x = tf.reshape(x,\n",
    "                       (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])#轴滚动\n",
    "    \n",
    "    def call(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        #经过Q K V变化\n",
    "        print(q.shape)\n",
    "        q = self.WQ(q) # q.shape: (batch_size, seq_len_q, d_model)\n",
    "        k = self.WK(k) # k.shape: (batch_size, seq_len_k, d_model)\n",
    "        v = self.WV(v) # v.shape: (batch_size, seq_len_v, d_model)\n",
    "        print('-'*50)\n",
    "        print(q.shape)\n",
    "        # q.shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        print('分割后的q:',end='')\n",
    "        print(q.shape)\n",
    "        # k.shape: (batch_size, num_heads, seq_len_k, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        # v.shape: (batch_size, num_heads, seq_len_v, depth)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        #开始做缩放点积，得到的多头的信息存在在num_heads，depth上\n",
    "        # scaled_attention_outputs.shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention_outputs, attention_weights = \\\n",
    "        scaled_dot_product_attention(q, k, v, mask)\n",
    "        print('-'*50)\n",
    "        print('scaled_attention_outputs.shape:',end='')\n",
    "        print(scaled_attention_outputs.shape)\n",
    "        print('attention_weights.shape:',end='')\n",
    "        print(attention_weights.shape)\n",
    "        print('-'*50)\n",
    "        #因此这里做一下转置，让num_heads，depth在后面\n",
    "        # scaled_attention_outputs.shape: (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention_outputs = tf.transpose(\n",
    "            scaled_attention_outputs, perm = [0, 2, 1, 3])\n",
    "        print('attention_weights.shape:',end='')\n",
    "        print(scaled_attention_outputs.shape)\n",
    "        #对注意力进行合并\n",
    "        # concat_attention.shape: (batch_size, seq_len_q, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention_outputs,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        #多头注意力计算完毕\n",
    "        # output.shape : (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "#创建一份虚拟数据\n",
    "y = tf.random.uniform((1, 60, 256)) # (batch_size, seq_len_q, dim)\n",
    "#开始计算，把y既当q，又当k，v\n",
    "output, attn = temp_mha(y, y, y, mask = None)\n",
    "print(output.shape)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 40, 512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义我们的feed_forward_network，d_model节点数\n",
    "def feed_forward_network(d_model, dff):\n",
    "    # dff: dim of feed forward network.\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Dense(dff, activation='relu'),\n",
    "        keras.layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "sample_ffn = feed_forward_network(512, 2048)\n",
    "#给一个输入测试\n",
    "sample_ffn(tf.random.uniform((64, 40, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40, 512)\n",
      "--------------------------------------------------\n",
      "(64, 40, 512)\n",
      "分割后的q:(64, 8, 40, 64)\n",
      "--------------------------------------------------\n",
      "scaled_attention_outputs.shape:(64, 8, 40, 64)\n",
      "attention_weights.shape:(64, 8, 40, 40)\n",
      "--------------------------------------------------\n",
      "attention_weights.shape:(64, 40, 8, 64)\n",
      "(64, 40, 512)\n"
     ]
    }
   ],
   "source": [
    "#自定义EncoderLayer\n",
    "class EncoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x -> self attention -> add & normalize & dropout\n",
    "      -> feed_forward -> add & normalize & dropout\n",
    "    原理对应文档Add & Normalize 标题下的图\n",
    "    \"\"\"\n",
    "    #d_model 给self attention和feed_forward_network，num_heads给self_attention用的\n",
    "    #dff给feed_forward_network，rate是做dropout的\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        # epsilon 将小浮点数添加到方差以避免被零除\n",
    "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        #下面两个层次用了做dropout，每次有10%的几率被drop掉\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, encoder_padding_mask):\n",
    "        # x.shape          : (batch_size, seq_len, dim=d_model)\n",
    "        # attn_output.shape: (batch_size, seq_len, d_model)\n",
    "        # out1.shape       : (batch_size, seq_len, d_model)\n",
    "        #x作为q，k，v  原理对应文档Add & Normalize 标题下的图\n",
    "        attn_output, _ = self.mha(x, x, x, encoder_padding_mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        #dim=d_model 两个必须相等，这样x才可以和attn_output做加法\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # ffn_output.shape: (batch_size, seq_len, d_model)\n",
    "        # out2.shape      : (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "#来测试，结果和我们最初的输入维度一致，相当于做了两次残差连接\n",
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_input = tf.random.uniform((64, 40, 512))\n",
    "sample_output = sample_encoder_layer(sample_input, False, None)\n",
    "print(sample_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[ 0.06049087,  0.05754187,  0.01295706, ...,  0.00312557,\n",
       "         -0.00820301, -0.02702056],\n",
       "        [ 0.02960116, -0.02637993,  0.00751164, ...,  0.03335854,\n",
       "         -0.07187759,  0.02813393],\n",
       "        [-0.03214407, -0.02282953,  0.05149531, ...,  0.01780858,\n",
       "          0.02392361, -0.03214193],\n",
       "        ...,\n",
       "        [ 0.02296404, -0.01550609, -0.05523246, ...,  0.03994527,\n",
       "          0.02157319, -0.06082605],\n",
       "        [-0.02424841,  0.01381076, -0.0400169 , ...,  0.00229418,\n",
       "         -0.03171059,  0.02917477],\n",
       "        [-0.06139388, -0.06241845,  0.04100421, ...,  0.0729678 ,\n",
       "          0.03927222, -0.04245267]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.06122693,  0.01555887,  0.03738999, ..., -0.03696217,\n",
       "          0.03529059,  0.03565594],\n",
       "        [-0.00180998, -0.07206024, -0.06322497, ..., -0.04225226,\n",
       "          0.01920396, -0.07502349],\n",
       "        [ 0.04749683,  0.00451926,  0.07135377, ..., -0.04734237,\n",
       "         -0.04977934, -0.00568973],\n",
       "        ...,\n",
       "        [ 0.00818846, -0.00485563, -0.04274394, ..., -0.0131172 ,\n",
       "         -0.01927886,  0.01029605],\n",
       "        [-0.05340102, -0.03331454, -0.05461083, ...,  0.02186224,\n",
       "         -0.00487422, -0.00450736],\n",
       "        [ 0.0261656 ,  0.04669029,  0.02043362, ...,  0.0524975 ,\n",
       "          0.02089027,  0.02561742]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.04734335, -0.00059238,  0.01228011, ...,  0.00013447,\n",
       "          0.05992004,  0.05948519],\n",
       "        [ 0.02927462, -0.05233993, -0.05097657, ...,  0.02865516,\n",
       "         -0.03064803,  0.03226003],\n",
       "        [ 0.06441016,  0.0107282 , -0.01116004, ..., -0.07041617,\n",
       "          0.01987309,  0.00356162],\n",
       "        ...,\n",
       "        [-0.04903611, -0.06923359,  0.01172072, ...,  0.00681531,\n",
       "          0.04106753,  0.03049526],\n",
       "        [ 0.04540833,  0.05287839, -0.04157829, ...,  0.04713433,\n",
       "          0.0341887 ,  0.07621514],\n",
       "        [ 0.03958351,  0.02954764, -0.04878619, ...,  0.02454026,\n",
       "         -0.04183755,  0.05160411]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.03056189,  0.03840245,  0.06369866, ..., -0.01982201,\n",
       "         -0.05127408, -0.03084429],\n",
       "        [-0.06163967,  0.00731573, -0.07541972, ..., -0.0257234 ,\n",
       "          0.03932022, -0.01806713],\n",
       "        [-0.04045641, -0.0572211 , -0.04422878, ...,  0.07415096,\n",
       "         -0.05030978,  0.06083208],\n",
       "        ...,\n",
       "        [-0.04348508, -0.03072826,  0.00988091, ..., -0.03283157,\n",
       "         -0.00456658,  0.0254339 ],\n",
       "        [ 0.02285618,  0.06321784, -0.07209493, ...,  0.00477888,\n",
       "         -0.05853254,  0.03860234],\n",
       "        [-0.01662637, -0.0460456 , -0.02558461, ...,  0.05648505,\n",
       "          0.07170648, -0.06233652]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
       " array([[ 0.04115755,  0.01608556,  0.01685996, ..., -0.02097944,\n",
       "          0.00297345,  0.00485408],\n",
       "        [-0.03240767, -0.00528982,  0.01182374, ..., -0.01297666,\n",
       "         -0.03332258,  0.02329987],\n",
       "        [-0.02961032,  0.00800954,  0.03368758, ...,  0.00299968,\n",
       "          0.01900576, -0.02707856],\n",
       "        ...,\n",
       "        [-0.01355667,  0.01415982,  0.04808404, ..., -0.00542272,\n",
       "          0.00146653, -0.00036811],\n",
       "        [ 0.01199534,  0.0369623 , -0.00664731, ...,  0.02949692,\n",
       "          0.0125438 , -0.04455305],\n",
       "        [-0.00322022,  0.00893662,  0.00362041, ..., -0.00938035,\n",
       "         -0.03391562,  0.03721663]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/bias:0' shape=(2048,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/kernel:0' shape=(2048, 512) dtype=float32, numpy=\n",
       " array([[ 0.01289848,  0.0122197 ,  0.01889016, ..., -0.01112282,\n",
       "          0.01118656, -0.02814102],\n",
       "        [-0.03967308, -0.0413747 , -0.02226558, ...,  0.02336296,\n",
       "          0.01109497, -0.01048355],\n",
       "        [ 0.00956212, -0.03420587, -0.04440473, ..., -0.03394549,\n",
       "         -0.02284238, -0.02356836],\n",
       "        ...,\n",
       "        [ 0.00453592,  0.04278537, -0.04771267, ...,  0.01804774,\n",
       "          0.00025654,  0.04614928],\n",
       "        [ 0.00865751, -0.02885134, -0.04365872, ...,  0.04267052,\n",
       "          0.03928886,  0.0228007 ],\n",
       "        [-0.02464567,  0.03942966, -0.02588443, ..., -0.04334484,\n",
       "          0.01247142,  0.02458515]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[ 0.06049087,  0.05754187,  0.01295706, ...,  0.00312557,\n",
       "         -0.00820301, -0.02702056],\n",
       "        [ 0.02960116, -0.02637993,  0.00751164, ...,  0.03335854,\n",
       "         -0.07187759,  0.02813393],\n",
       "        [-0.03214407, -0.02282953,  0.05149531, ...,  0.01780858,\n",
       "          0.02392361, -0.03214193],\n",
       "        ...,\n",
       "        [ 0.02296404, -0.01550609, -0.05523246, ...,  0.03994527,\n",
       "          0.02157319, -0.06082605],\n",
       "        [-0.02424841,  0.01381076, -0.0400169 , ...,  0.00229418,\n",
       "         -0.03171059,  0.02917477],\n",
       "        [-0.06139388, -0.06241845,  0.04100421, ...,  0.0729678 ,\n",
       "          0.03927222, -0.04245267]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_6/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.06122693,  0.01555887,  0.03738999, ..., -0.03696217,\n",
       "          0.03529059,  0.03565594],\n",
       "        [-0.00180998, -0.07206024, -0.06322497, ..., -0.04225226,\n",
       "          0.01920396, -0.07502349],\n",
       "        [ 0.04749683,  0.00451926,  0.07135377, ..., -0.04734237,\n",
       "         -0.04977934, -0.00568973],\n",
       "        ...,\n",
       "        [ 0.00818846, -0.00485563, -0.04274394, ..., -0.0131172 ,\n",
       "         -0.01927886,  0.01029605],\n",
       "        [-0.05340102, -0.03331454, -0.05461083, ...,  0.02186224,\n",
       "         -0.00487422, -0.00450736],\n",
       "        [ 0.0261656 ,  0.04669029,  0.02043362, ...,  0.0524975 ,\n",
       "          0.02089027,  0.02561742]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_7/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.04734335, -0.00059238,  0.01228011, ...,  0.00013447,\n",
       "          0.05992004,  0.05948519],\n",
       "        [ 0.02927462, -0.05233993, -0.05097657, ...,  0.02865516,\n",
       "         -0.03064803,  0.03226003],\n",
       "        [ 0.06441016,  0.0107282 , -0.01116004, ..., -0.07041617,\n",
       "          0.01987309,  0.00356162],\n",
       "        ...,\n",
       "        [-0.04903611, -0.06923359,  0.01172072, ...,  0.00681531,\n",
       "          0.04106753,  0.03049526],\n",
       "        [ 0.04540833,  0.05287839, -0.04157829, ...,  0.04713433,\n",
       "          0.0341887 ,  0.07621514],\n",
       "        [ 0.03958351,  0.02954764, -0.04878619, ...,  0.02454026,\n",
       "         -0.04183755,  0.05160411]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_8/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
       " array([[-0.03056189,  0.03840245,  0.06369866, ..., -0.01982201,\n",
       "         -0.05127408, -0.03084429],\n",
       "        [-0.06163967,  0.00731573, -0.07541972, ..., -0.0257234 ,\n",
       "          0.03932022, -0.01806713],\n",
       "        [-0.04045641, -0.0572211 , -0.04422878, ...,  0.07415096,\n",
       "         -0.05030978,  0.06083208],\n",
       "        ...,\n",
       "        [-0.04348508, -0.03072826,  0.00988091, ..., -0.03283157,\n",
       "         -0.00456658,  0.0254339 ],\n",
       "        [ 0.02285618,  0.06321784, -0.07209493, ...,  0.00477888,\n",
       "         -0.05853254,  0.03860234],\n",
       "        [-0.01662637, -0.0460456 , -0.02558461, ...,  0.05648505,\n",
       "          0.07170648, -0.06233652]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/multi_head_attention_1/dense_9/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
       " array([[ 0.04115755,  0.01608556,  0.01685996, ..., -0.02097944,\n",
       "          0.00297345,  0.00485408],\n",
       "        [-0.03240767, -0.00528982,  0.01182374, ..., -0.01297666,\n",
       "         -0.03332258,  0.02329987],\n",
       "        [-0.02961032,  0.00800954,  0.03368758, ...,  0.00299968,\n",
       "          0.01900576, -0.02707856],\n",
       "        ...,\n",
       "        [-0.01355667,  0.01415982,  0.04808404, ..., -0.00542272,\n",
       "          0.00146653, -0.00036811],\n",
       "        [ 0.01199534,  0.0369623 , -0.00664731, ...,  0.02949692,\n",
       "          0.0125438 , -0.04455305],\n",
       "        [-0.00322022,  0.00893662,  0.00362041, ..., -0.00938035,\n",
       "         -0.03391562,  0.03721663]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_10/bias:0' shape=(2048,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/kernel:0' shape=(2048, 512) dtype=float32, numpy=\n",
       " array([[ 0.01289848,  0.0122197 ,  0.01889016, ..., -0.01112282,\n",
       "          0.01118656, -0.02814102],\n",
       "        [-0.03967308, -0.0413747 , -0.02226558, ...,  0.02336296,\n",
       "          0.01109497, -0.01048355],\n",
       "        [ 0.00956212, -0.03420587, -0.04440473, ..., -0.03394549,\n",
       "         -0.02284238, -0.02356836],\n",
       "        ...,\n",
       "        [ 0.00453592,  0.04278537, -0.04771267, ...,  0.01804774,\n",
       "          0.00025654,  0.04614928],\n",
       "        [ 0.00865751, -0.02885134, -0.04365872, ...,  0.04267052,\n",
       "          0.03928886,  0.0228007 ],\n",
       "        [-0.02464567,  0.03942966, -0.02588443, ..., -0.04334484,\n",
       "          0.01247142,  0.02458515]], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/sequential_1/dense_11/bias:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/gamma:0' shape=(512,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'encoder_layer/layer_normalization_1/beta:0' shape=(512,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 60, 512)\n",
      "--------------------------------------------------\n",
      "(64, 60, 512)\n",
      "(64, 60, 512)\n",
      "--------------------------------------------------\n",
      "(64, 60, 512)\n",
      "(64, 60, 512)\n",
      "(64, 8, 60, 60)\n",
      "(64, 8, 60, 50)\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x -> self attention -> add & normalize & dropout -> out1\n",
    "    out1 , encoding_outputs -> attention -> add & normalize & dropout -> out2\n",
    "    out2 -> ffn -> add & normalize & dropout -> out3\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = feed_forward_network(d_model, dff)\n",
    "        #因为有两个attention，还有一个feed_forward_network，所以有3个\n",
    "        #LayerNormalization和3个dropout\n",
    "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        self.layer_norm3 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, encoding_outputs, training,\n",
    "             decoder_mask, encoder_decoder_padding_mask):\n",
    "        # decoder_mask: 由look_ahead_mask和decoder_padding_mask合并而来\n",
    "        \n",
    "        # x.shape: (batch_size, target_seq_len, d_model)\n",
    "        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        #按照上面类的注释的步骤依次来编写call实现\n",
    "        # attn1, out1.shape : (batch_size, target_seq_len, d_model)\n",
    "        attn1, attn_weights1 = self.mha1(x, x, x, decoder_mask)\n",
    "        attn1 = self.dropout1(attn1, training = training)\n",
    "        out1 = self.layer_norm1(attn1 + x)\n",
    "        \n",
    "        # attn2, out2.shape : (batch_size, target_seq_len, d_model)\n",
    "        attn2, attn_weights2 = self.mha2(\n",
    "            out1, encoding_outputs, encoding_outputs,\n",
    "            encoder_decoder_padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training = training)\n",
    "        out2 = self.layer_norm2(attn2 + out1)\n",
    "        \n",
    "        # ffn_output, out3.shape: (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layer_norm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights1, attn_weights2\n",
    "\n",
    "#测试一下\n",
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_decoder_input = tf.random.uniform((64, 60, 512))\n",
    "sample_decoder_output, sample_decoder_attn_weights1, sample_decoder_attn_weights2 = sample_decoder_layer(\n",
    "    sample_decoder_input, sample_output, False, None, None)\n",
    "\n",
    "print(sample_decoder_output.shape)\n",
    "print(sample_decoder_attn_weights1.shape)  #最后一维60是和x的维度一致的\n",
    "print(sample_decoder_attn_weights2.shape) #最后一维60是和x的维度相关的\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(64, 37, 512)\n",
      "--------------------------------------------------\n",
      "(64, 37, 512)\n",
      "(64, 37, 512)\n",
      "--------------------------------------------------\n",
      "(64, 37, 512)\n",
      "(64, 37, 512)\n"
     ]
    }
   ],
   "source": [
    "#我们多堆建几个EncoderLayer就是我们的EncoderModel\n",
    "class EncoderModel(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, input_vocab_size, max_length,\n",
    "                 d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        #这是layers数目\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        #构建embedding层\n",
    "        self.embedding = keras.layers.Embedding(input_vocab_size,\n",
    "                                                self.d_model)\n",
    "        # position_embedding.shape: (1, max_length, d_model)\n",
    "        self.position_embedding = get_position_embedding(max_length,\n",
    "                                                         self.d_model)\n",
    "        \n",
    "        self.dropout = keras.layers.Dropout(rate)\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, dff, rate)\n",
    "            for _ in range(self.num_layers)]\n",
    "        \n",
    "    \n",
    "    def call(self, x, training, encoder_padding_mask):\n",
    "        # x.shape: (batch_size, input_seq_len)\n",
    "        input_seq_len = tf.shape(x)[1]  #拿到输入的长度是40\n",
    "        tf.debugging.assert_less_equal(\n",
    "            input_seq_len, self.max_length,\n",
    "            \"input_seq_len should be less or equal to self.max_length\")\n",
    "        \n",
    "        # x.shape: (batch_size, input_seq_len, d_model)\n",
    "        x = self.embedding(x)\n",
    "        #x做缩放，是值在0到d_model之间\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #因为x长度比position_embedding可能要小，因此embedding切片后和x相加\n",
    "        x += self.position_embedding[:, :input_seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training = training)\n",
    "        \n",
    "        #得到的x不断作为下一层的输入\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, training,\n",
    "                                       encoder_padding_mask)\n",
    "        #x最终shape如下\n",
    "        # x.shape: (batch_size, input_seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "#测试\n",
    "sample_encoder_model = EncoderModel(2, 8500, max_length,\n",
    "                                    512, 8, 2048)\n",
    "sample_encoder_model_input = tf.random.uniform((64, 37))\n",
    "sample_encoder_model_output = sample_encoder_model(\n",
    "    sample_encoder_model_input, False, encoder_padding_mask = None)\n",
    "print(sample_encoder_model_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "--------------------------------------------------\n",
      "(64, 35, 512)\n",
      "(64, 35, 512)\n",
      "(64, 8, 35, 35)\n",
      "(64, 8, 35, 37)\n",
      "(64, 8, 35, 35)\n",
      "(64, 8, 35, 37)\n"
     ]
    }
   ],
   "source": [
    "#和encodermodel类似\n",
    "class DecoderModel(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, target_vocab_size, max_length,\n",
    "                 d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderModel, self).__init__()\n",
    "        self.num_layers = num_layers  #这个代表多少个decoder_layers\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = keras.layers.Embedding(target_vocab_size,\n",
    "                                                d_model)\n",
    "        self.position_embedding = get_position_embedding(max_length,\n",
    "                                                         d_model)\n",
    "        \n",
    "        self.dropout = keras.layers.Dropout(rate)\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, dff, rate)\n",
    "            for _ in range(self.num_layers)]\n",
    "        \n",
    "    \n",
    "    def call(self, x, encoding_outputs, training,\n",
    "             decoder_mask, encoder_decoder_padding_mask):\n",
    "        # x.shape: (batch_size, output_seq_len)\n",
    "        output_seq_len = tf.shape(x)[1]\n",
    "        tf.debugging.assert_less_equal(\n",
    "            output_seq_len, self.max_length,\n",
    "            \"output_seq_len should be less or equal to self.max_length\")\n",
    "        \n",
    "        #attention_weights都是由decoder layer返回，把它保存下来\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # x.shape: (batch_size, output_seq_len, d_model)\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  #做缩放\n",
    "        x += self.position_embedding[:, :output_seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training = training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            #attn1,attn2分别是两个attention\n",
    "            x, attn1, attn2 = self.decoder_layers[i](\n",
    "                x, encoding_outputs, training,\n",
    "                decoder_mask, encoder_decoder_padding_mask)\n",
    "            attention_weights[\n",
    "                'decoder_layer{}_att1'.format(i+1)] = attn1\n",
    "            attention_weights[\n",
    "                'decoder_layer{}_att2'.format(i+1)] = attn2\n",
    "        # x.shape: (batch_size, output_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "\n",
    "sample_decoder_model = DecoderModel(2, 8000, max_length,\n",
    "                                    512, 8, 2048)\n",
    "#测试\n",
    "sample_decoder_model_input = tf.random.uniform((64, 35))\n",
    "sample_decoder_model_output, sample_decoder_model_att \\\n",
    "= sample_decoder_model(\n",
    "    sample_decoder_model_input,\n",
    "    sample_encoder_model_output,#注意这里是encoder的output\n",
    "    training = False, decoder_mask = None,\n",
    "    encoder_decoder_padding_mask = None)\n",
    "\n",
    "print(sample_decoder_model_output.shape)\n",
    "for key in sample_decoder_model_att:\n",
    "    print(sample_decoder_model_att[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(40, 512)\n",
      "(40, 256)\n",
      "(40, 256)\n",
      "(64, 26, 512)\n",
      "--------------------------------------------------\n",
      "(64, 26, 512)\n",
      "(64, 26, 512)\n",
      "--------------------------------------------------\n",
      "(64, 26, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 512)\n",
      "--------------------------------------------------\n",
      "(64, 31, 512)\n",
      "(64, 31, 8000)\n",
      "--------------------------------------------------\n",
      "decoder_layer1_att1 (64, 8, 31, 31)\n",
      "decoder_layer1_att2 (64, 8, 31, 26)\n",
      "decoder_layer2_att1 (64, 8, 31, 31)\n",
      "decoder_layer2_att2 (64, 8, 31, 26)\n"
     ]
    }
   ],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(self, num_layers, input_vocab_size, target_vocab_size,\n",
    "                 max_length, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder_model = EncoderModel(\n",
    "            num_layers, input_vocab_size, max_length,\n",
    "            d_model, num_heads, dff, rate)\n",
    "        \n",
    "        self.decoder_model = DecoderModel(\n",
    "            num_layers, target_vocab_size, max_length,\n",
    "            d_model, num_heads, dff, rate)\n",
    "        \n",
    "        self.final_layer = keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, encoder_padding_mask,\n",
    "             decoder_mask, encoder_decoder_padding_mask):\n",
    "        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n",
    "        encoding_outputs = self.encoder_model(\n",
    "            inp, training, encoder_padding_mask)\n",
    "        \n",
    "        # decoding_outputs.shape: (batch_size, output_seq_len, d_model)\n",
    "        decoding_outputs, attention_weights = self.decoder_model(\n",
    "            tar, encoding_outputs, training,\n",
    "            decoder_mask, encoder_decoder_padding_mask)\n",
    "        \n",
    "        # predictions.shape: (batch_size, output_seq_len, target_vocab_size)\n",
    "        predictions = self.final_layer(decoding_outputs)\n",
    "        \n",
    "        return predictions, attention_weights\n",
    "\n",
    "#测试\n",
    "sample_transformer = Transformer(2, 8500, 8000, max_length,\n",
    "                                 512, 8, 2048, rate = 0.1)\n",
    "temp_input = tf.random.uniform((64, 26))\n",
    "temp_target = tf.random.uniform((64, 31))\n",
    "\n",
    "#得到输出\n",
    "predictions, attention_weights = sample_transformer(\n",
    "    temp_input, temp_target, training = False,\n",
    "    encoder_padding_mask = None,\n",
    "    decoder_mask = None,\n",
    "    encoder_decoder_padding_mask = None)\n",
    "#输出shape\n",
    "print(predictions.shape)\n",
    "print('-'*50)\n",
    "#attention_weights 的shape打印\n",
    "for key in attention_weights:\n",
    "    print(key, attention_weights[key].shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. initializes model.\n",
    "# 2. define loss, optimizer, learning_rate schedule\n",
    "# 3. train_step\n",
    "# 4. train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 128)\n",
      "(40, 64)\n",
      "(40, 64)\n",
      "(40, 128)\n",
      "(40, 64)\n",
      "(40, 64)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "#加2是因为最后两个位置是start和end\n",
    "input_vocab_size = pt_tokenizer.vocab_size + 2\n",
    "target_vocab_size = en_tokenizer.vocab_size + 2\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(num_layers,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          max_length,\n",
    "                          d_model, num_heads, dff, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学习率变化，是先增后减，因为前期可以快点，后期模型比较好，就要慢点\n",
    "# lrate = (d_model ** -0.5) * min(step_num ** (-0.5),\n",
    "#                                 step_num * warm_up_steps **(-1.5))\n",
    "#自定义的学习率调整设计实现\n",
    "#这里的公式看这里 https://tensorflow.google.cn/tutorials/text/transformer\n",
    "class CustomizedSchedule(\n",
    "    keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps = 4000):\n",
    "        super(CustomizedSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        \n",
    "        arg3 = tf.math.rsqrt(self.d_model)\n",
    "        \n",
    "        return arg3 * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomizedSchedule(d_model)\n",
    "optimizer = keras.optimizers.Adam(learning_rate,\n",
    "                                  beta_1 = 0.9,\n",
    "                                  beta_2 = 0.98,\n",
    "                                  epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train step')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEGCAYAAABGnrPVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3wW9Zn//9eVhBASSEIggRAIx4jiAYoRtVpPlRZcK1pr1drV2oN1V35tt7vb6n7XXdvtwdZ2tbZW13ZttSe17arUQz3gqbVaiUURBCS5EQggucMhkoRzrt8fM4EQcriT3HfuO7nfz8fjftz3PTOfmWsGkiufmc9cY+6OiIhIvGQkOwARERlclFhERCSulFhERCSulFhERCSulFhERCSuspIdQDKNHj3aJ02alOwwREQGlNdee63e3Ys7m5/WiWXSpElUVVUlOwwRkQHFzNZ1NV+nwkREJK6UWEREJK6UWEREJK6UWEREJK6UWEREJK4SmljMbJ6ZrTazajO7voP5Zma3h/OXmdns7tqa2SVmtsLMWsyssoN1lptZo5n9S+L2TEREOpOwxGJmmcAdwHxgBnC5mc1ot9h8oCJ8XQPcGUPb5cBHgRc72fStwBPx2xMREemJRN7HMgeodvcIgJndDywA3mqzzALgPg9q979iZoVmVgpM6qytu68Mpx2xQTO7EIgATYnaqWR7bd02MjMymDWhMNmhiIh0KJGnwsqADW2+14bTYlkmlraHMbM84KvA17pZ7hozqzKzqmg02uUOpKKL73yZC+94CT1HR0RSVSITy5FdCmj/27CzZWJp297XgFvdvbGrhdz9bnevdPfK4uJOKxKkpAMthw7B6i07kxiJiEjnEnkqrBaY0Ob7eGBTjMtkx9C2vZOBj5nZd4FCoMXMdrv7j3oRe0ratGPXwc9PvPkuR4/NT2I0IiIdS2SPZQlQYWaTzSwbuAxY1G6ZRcCV4eiwU4AGd98cY9vDuPsH3H2Su08CbgO+NZiSCkB1NOiMmcETyzcnORoRkY4lLLG4+35gIfAksBJ40N1XmNm1ZnZtuNjjBBfbq4GfAP/YVVsAM7vIzGqBU4HHzOzJRO1DqolEgzEJC8+exttbGqmu6/Ksn4hIUiS0urG7P06QPNpOu6vNZweui7VtOP0h4KFutntTL8JNeTXRRgqGDeETJ5fzw2er+ePyzSw8pyLZYYmIHEZ33g8gkWgjU4rzKC0YxvvKC3li+bvJDklE5AhKLANIJNrE1OLhAPzd8aWs2PQekahOh4lIalFiGSB27t5H3c49TCnOA+AjM8eRYfDw0o1JjkxE5HBKLANE64X71h7LmPwcTps2mode36ibJUUkpSixDBA14SmvqWGPBeDCWWVs2LaL19ZtT1ZYIiJHUGIZICLRJjIzjPKiQ4ll3nFjGTYkk//T6TARSSFKLANEpL6R8qJcsrMO/ZPlDc3iQ8eO4bFlm9mz/0ASoxMROUSJZYCoqWtiyui8I6Zf9L4yGnbt47lVdUmISkTkSEosA8CBFmft1iamlgw/Yt7p00ZTWpDDb17d0EFLEZH+p8QyAGzcvou9+1s67LFkZWbw8coJvLgmyoZtzUmITkTkcEosA0BNfTAibErxkT0WgEtPmoABDyxRr0VEkk+JZQCoqTtyqHFb4wqHcfb0Eh6o2sC+Ay39GZqIyBGUWAaASH0TBcOGUJSX3ekyl88pJ7pzD4tXbunHyEREjqTEMgBEoo1MLc7DrKMHawbOml5MaUEOv/rr+n6MTETkSEosA0BNtKnT6yutsjIz+MSccv60pp41emyxiCSREkuKe2/3PqI79xysEdaVK06ZyNCsDO55aW0/RCYi0jEllhTXWnxySicX7tsqysvmo7PH8/u/bWRr455EhyYi0iEllhQX6aD4ZFc+c/ok9u5v0bUWEUkaJZYU11Hxya5MKxnBmUcVc9/L61Q/TESSIqGJxczmmdlqM6s2s+s7mG9mdns4f5mZze6urZldYmYrzKzFzCrbTJ9rZq+Z2Zvh+zmJ3Lf+UhM9svhkdz5z+mTqG/foIWAikhQJSyxmlgncAcwHZgCXm9mMdovNByrC1zXAnTG0XQ58FHix3brqgY+4+/HAVcAv4r1PyRA8jji23kqrD1SM5riyfH78fA37dcOkiPSzRPZY5gDV7h5x973A/cCCdsssAO7zwCtAoZmVdtXW3Ve6++r2G3P3pe6+Kfy6Asgxs6GJ2bX+0Vp8sruhxu2ZGQvPrmDd1mYeXbY5QdGJiHQskYmlDGhbvKo2nBbLMrG07crFwFJ3P2JolJldY2ZVZlYVjUZ7sMr+11Xxye58aMYYpo8ZwY+eq6alRY8uFpH+k8jE0tFt4u1/w3W2TCxtO96o2bHAd4DPdzTf3e9290p3rywuLo5llUlz8HHEHZTL705GhrHwnGlU1zXyxPJ34x2aiEinEplYaoEJbb6PBzbFuEwsbY9gZuOBh4Ar3b2mFzGnlNbE0pseC8B5x5cypTiPHz67Rr0WEek3iUwsS4AKM5tsZtnAZcCidsssAq4MR4edAjS4++YY2x7GzAqBx4Ab3P2leO9MMkTqmyjM7br4ZFcyM4wvnFPBqnd38odl3eZlEZG4SFhicff9wELgSWAl8KC7rzCza83s2nCxx4EIUA38BPjHrtoCmNlFZlYLnAo8ZmZPhutaCEwDbjSz18NXSaL2rz/U1DUyZXTXxSe7c8HMcRxTms/3n3qbvfs1QkxEEs/c0/cUSWVlpVdVVSU7jE6d9M1nOOuoYm65ZGaf1vPc6jqu/tkSvr7gWK48dVJ8ghORtGVmr7l7ZWfzded9imotPtnTocYdOeuoYk6eXMTti9fQtGd/HKITEemcEkuK6knxye6YGV+dfzT1jXv56Z9U+VhEEkuJJUUdKj7Z9x4LwOzykZx3/FjueqGGTTt2xWWdIiIdUWJJUTXRxrD4ZG7c1nnD/GNocedbj6+M2zpFRNpTYklRkWgTE3tYfLI7E4pyufbMqTy6bDOvRLbGbb0iIm0psaSommhjXK6vtPcPZ02lrHAYNy1aoQKVIpIQSiwp6ECL8059c1xGhLWXMySTf/+7Y1j17k5++cq6uK9fRESJJQXVbm9m74GWHpfLj9W848bygYrR3PLkal3IF5G4U2JJQYeGGse/xwLB8ONvXXQ8LQ7//vBy0vkmWRGJPyWWFFQT56HGHZlQlMu/fHg6z66q4w96ZouIxJESSwqqifat+GSsPvX+ScycUMjXFq1ge9PehG5LRNKHEksKikQbE9pbaZWZYXzn4uNp2LWPGx/RKTERiQ8llhRUE23q9TNYeurosfn809yjeHTZZh55XaX1RaTvlFhSzHu791HfGJ/ik7G69sypVE4cyY0PL6d2e3O/bVdEBicllhTTOiIsUUONO5KZYdx66Swc+PKDb3BAT5sUkT5QYkkxNXXh44j7sccCwSixmy44llfXbuOuFwb8U51FJImUWFJMpL6RrAxj4qj4FZ+M1cWzyzj/hFK+/9Rq1RITkV5TYkkxNXVNlBflMiSz//9pzIybLz6BSaPzWPjrpdS9t7vfYxCRgU+JJcVE6hNTfDJWw4dmcecVJ9K0Zz8Lf7NUhSpFpMcSmljMbJ6ZrTazajO7voP5Zma3h/OXmdns7tqa2SVmtsLMWsysst36bgiXX21mH07kviVCa/HJ/riHpSvTx47gmxcdx6trt3HLU6uTGouIDDwJSyxmlgncAcwHZgCXm9mMdovNByrC1zXAnTG0XQ58FHix3fZmAJcBxwLzgB+H6xkwWotPJrPH0uqjs8dzxcnl/M8LER5eujHZ4YjIAJLIHsscoNrdI+6+F7gfWNBumQXAfR54BSg0s9Ku2rr7Snfv6M/oBcD97r7H3dcC1eF6BoxDQ42T22Np9Z8fOZZTphTxld8v47V125MdjogMEIlMLGXAhjbfa8NpsSwTS9vebA8zu8bMqsysKhqNdrPK/tVafLK/hxp3JjsrgzuvOJHSghw+/4sq3TwpIjFJZGKxDqa1v/Ous2Viadub7eHud7t7pbtXFhcXd7PK/lUTbWJkPxSf7ImRedn871UnsWd/C5+9t4rGPfuTHZKIpLhEJpZaYEKb7+OB9sWoOlsmlra92V5KCx5HnBq9lbamlQznjk/MZk1dI9f+4jX27D+Q7JBEJIUlMrEsASrMbLKZZRNcWF/UbplFwJXh6LBTgAZ33xxj2/YWAZeZ2VAzm0wwIODVeO5QokX6sfhkT51xVDHfvfgE/lxdzz8/+AYtKvsiIp3IStSK3X2/mS0EngQygXvcfYWZXRvOvwt4HDiP4EJ7M3B1V20BzOwi4IdAMfCYmb3u7h8O1/0g8BawH7jO3QfMn9YNu4Lik1NLUq/H0uriE8eztWkP33p8FaPysrnpgmMx6+gMpIiks4QlFgB3f5wgebSddlebzw5cF2vbcPpDwEOdtPkm8M0+hJw0kdYL9ynaY2l1zRlTie7cw0/+tJaivKF88dyKZIckIikmoYlFYndwqHEK91ha3TD/GLY17ePWZ94mK9O47uxpyQ5JRFKIEkuKqIkGxSfLi/q/+GRPZWQY3/3YCexvaeGWJ1eTmWFce+bUZIclIilCiSVFRKLJKz7ZG5kZxvcvmUmLw81PrCLTjM+dMSXZYYlIClBiSRGpOtS4K1mZGdz68Zm0uPPNx1dywF09FxFRYkkFB1qcdVubOefokmSH0mNZmRn84NJZZJhx8xOr2NG8j6/Om67RYiJprNvEYsFviCuAKe7+dTMrB8a6+4C6RySVtRafTJUaYT2VlZnBbZfOIj8ni7teqKFh116+ceHxZGYouYiko1h6LD8GWoBzgK8DO4HfAyclMK60cqhGWGoPNe5KZobxjQuPY2RuNj96rpqGXfu49dJZDM0aUAWmRSQOYkksJ7v7bDNbCuDu28O74SVOUq2qcW+ZGf/y4ekU5g7hG4+tpL7xVe7++xMpzNV/F5F0EssQpH3hc00cwMyKCXowEic10UZG5g5hZAoVn+yLz35gCj+4bBavr9/BRT/+C2vrm5Idkoj0o1gSy+0Ed7qXmNk3gT8D305oVGmmJto04EaEdWfBrDJ+9bmT2dG8l4t+/BKvrt2W7JBEpJ90m1jc/VfAVwiSyWbgQnd/MNGBpZNItImpA/j6SmdOmlTEw9edRlFeNp/86V/5bdWG7huJyIDXbWIxs1+4+yp3v8Pdf+TuK83sF/0RXDpoLT452HosrSaOyuOhfziNkyaP5F9/t4x/f/hN9u7XmVSRwSyWU2HHtv0SXm85MTHhpJ/W4pMD/cJ9Vwpyh3Dv1XP4/JlT+OUr67n07pfZ3LAr2WGJSIJ0mljM7AYz2wmcYGbvmdnO8Hsd8Ei/RTjI1YQjwgbyUONYZGVmcMP8Y7jzitm8/e5OPvLDP/NyzdZkhyUiCdBpYnH3b7v7COAWd8939xHha5S739CPMQ5qkQFUfDIe5h9fyiMLTyN/2BCu+Okr/PdTq9l/QKfGRAaTWC7e32BmI81sjpmd0frqj+DSQU20kfJRA6f4ZDxMKxnBooWnc9H7xnP7s9Vcevcr1G5vTnZYIhInsVy8/yzwIsHTHL8Wvt+U2LDSR/A44sF7faUzw4dm8f2Pz+QHl81i9bs7mf+DP/Hosk3JDktE4iCWP5O/SFC+ZZ27nw28D4gmNKo0sf9AC+u2NjO1ZHBfX+nKglllPP6FDzC1eDgLf72ULz/wOg3N+5Idloj0QSyJZbe77wYws6HuvgqYntiw0kPt9l1B8ck07LG0VT4ql99eeypf+GAFj7yxibm3vsAzb21Jdlgi0kuxJJZaMysEHgaeNrNHAJ2ziINIfTjUOI17LK2GZGbw5blH8Uh4Q+Vn76vinx54nR3Ne5Mdmoj0UCwX7y9y9x3ufhNwI/C/wIWxrNzM5pnZajOrNrPrO5hvZnZ7OH+Zmc3urq2ZFZnZ02a2JnwfGU4fYmb3mtmbZrbSzFJ+5FpNXTjUOM17LG0dV1bAooWn84UPVvCHNzYx99YXeXTZJtw92aGJSIy6TCxmlmFmy1u/u/sL7r7I3bv9MzK8kfIOYD4wA7jczGa0W2w+UBG+rgHujKHt9cBid68AFoffAS4Bhrr78QQ3cH7ezCZ1F2cyReoHV/HJeMnOCnovD193GiUjhrLw10u58p5XeUfFLEUGhC4Ti7u3AG+ED/fqqTlAtbtHwkR0P7Cg3TILgPs88ApQaGal3bRdANwbfr6XQ70nB/LMLAsYBuwF3utF3P2mJto0qO+476vjygp45LrT+M+PzGDp+h186LYXue2Zt9m970CyQxORLsRyjaUUWGFmi81sUesrhnZlQNuqg7XhtFiW6artGHffDBC+tz7P93dAE0GhzPXA99z9iJK6ZnaNmVWZWVU0mtzBbZFo46C/476vsjIzuPq0ySz+5zP50Iwx3PbMGubd9iLPrtqi02MiKSqWB319rZfr7ui5tO1/E3S2TCxt25sDHADGASOBP5nZM+4eOWwl7ncDdwNUVlYm7TdTQ/M+6hv3qscSozH5OfzoE7O59KQo//nICj798ypOnzaafz//GI4em5/s8ESkjW4Ti7u/0Mt11wIT2nwfz5GjyTpbJruLtlvMrNTdN4enzerC6Z8A/uju+4A6M3sJqAQOSyypoqa+9XHESiw98YGKYv74pTP41V/XcdszazjvB3/i0pPK+fLcoygeMTTZ4YkIsZ0K660lQIWZTQ4fZXwZ0P4U2iLgynB02ClAQ3h6q6u2i4Crws9Xcagg5nrgnHBdecApwKpE7VxfRdKk+GQiZGcFp8de+Nez+NT7J/Pbqg2cdctz/OjZNTTt2Z/s8ETSXsISi7vvBxYSlIBZCTzo7ivM7FozuzZc7HGCHkU18BPgH7tqG7a5GZhrZmuAueF3CEaRDQeWEySmn7n7skTtX1/VpFnxyUQozM3mPz4yg6f+6QxOmzaa7z31Nmfe8hz/++e1usAvkkSWzhdAKysrvaqqKinb/vwvqlhT18iz/3xWUrY/GP1t/Xa+/9RqXqreSmlBDv/fORVcUjk+rQp8ivQHM3vN3Ss7mx9LEco3w5sX277+ZGa3mtmo+IabPiIaahx3s8tH8qvPnsKvP3cypQU5/NtDb/LB77/Ag1Ub9NRKkX4Uy59yTwCPAVeErz8QVDt+F/h5wiIbxPYfaOGdrU26vpIg7586mt//w/u551OVjMjJ4iu/W8ZZtzzHz19ay669OkUmkmixDDc+zd1Pa/P9TTN7yd1PM7NPJiqwwax2+y72HXD1WBLIzDjn6DGcPb2E59+Ocsez1dz0h7f44bPVfPr0yfz9qRPJzxmS7DBFBqVYeizDzezk1i9mNofgIjmAhuD0Qs3B59yrx5JoZsbZ00v43T+8nwc/fyrHlRVwy5OrOe3bz/KdP65ic8OuZIcoMujE0mP5LHCPmQ0nuHHxPeCz4ZDebycyuMHq4FBjFZ/sV3MmFzFn8hyWb2zgx89Xc9cLNfzkxQjnHV/Kp0+fzKwJhckOUWRQiOUGySXA8WZWQDCKbEeb2Q8mLLJBrCbaSFFetopPJslxZQX8+IoTWb+1mXtffocHlmxg0RubmF1eyKdPn8y8Y8eSpZFkIr3WbWIxs6HAxcAkIMssqLbi7l9PaGSDWPA4Yp0GS7byUbnceP4MvnRuBb97rZaf/+UdFv56KeMKcvjEyeV8vHICJfk5yQ5TZMCJ5VTYI0AD8BqwJ7HhpIdIfSMfPHpMssOQ0IicIVx92mSuPHUSz66q42cvreV7T73Nbc+sYe6MMXzi5HJOmzqajIyOStiJSHuxJJbx7j4v4ZGkidbikxpqnHoyM4y5M8Ywd8YYItFG7l+ygd9WbeCJ5e9SXpTL5XPK+diJ41WTTKQbsZxI/ouZHZ/wSNJEa/FJDTVObVOKh/Nv5x3DK//2QX5w2SxKC3L4zh9Xceq3F/O5+6r44/J3ddOlSCdi6bGcDnzKzNYSnAozwN39hIRGNkjV1LVWNVaPZSAYmpXJglllLJhVRnVdIw9WbeChpRt5+q0tFOYO4YKZ4/jo7PHMHF9A6/VHkXQXS2KZn/Ao0kikvomsDGOCik8OONNKgl7MVz48nT9X1/P7v23kgSUbuO/ldUwtzuOjs8dz4fvKKCscluxQRZKq08RiZvnu/h6wsx/jGfQi0UYmjspVYcQBLCszg7Oml3DW9BLe272Px5dt5v/+tpFbnlzNLU+uZnZ5IeefMI6/O6GUMRpVJmmoqx7Lr4HzCUaDtX+qowNTEhjXoFUTbdLDvQaR/JwhXDannMvmlLN+azN/WLaJR5dt5uuPvsV/PfYWJ00s4vyZpcw7biwlI5RkJD2obH4/ls3ff6CFY/7jj3zm9ClcP//oftuu9L/qukYeW7aZx97cxNtbGjGDkycX8XcnjGPuMWMYW6AkIwNXd2XzY7nGgpmVARPbLu/uL/Y9vPSyISw+qQv3g9+0kuF88dwKvnhuBW9v2cmjyzbz6LJN3Pjwcm58eDkzxxcwd8YYPnTsWCpKhuvCvwwqsdx5/x3gUuAtoLXmuBOUzpceiKj4ZFo6aswIvjx3BP90bgVr6hp5+q0tPPXWFr731Nt876m3mTgql7nHBEnmxIkjydSNmDLAxdJjuRCY7u66676PWqsaq/hkejIzjhozgqPGjOC6s6ex5b3dPP3WFp5+awv3vbyOn/55LUV52Zx5VDFnTS/mAxXFFKmenAxAsSSWCDAElXPps0i0ScUn5aAx+Tl88pSJfPKUiezcvY8X3o7yzFtbeOHtKA8t3YgZnDC+8GCimTm+UL0ZGRBiSSzNwOtmtpg2ycXdv9BdQzObB/wAyAR+6u43t5tv4fzzwu18yt3/1lVbMysCHiAoivkO8HF33x7OOwH4HyAfaAFOcvfdMexjvwgeR6zTYHKkETlDOP+EcZx/wjgOtDjLNzbw/Oooz79dxw+fXcPti9dQmDuED1QUc9ZRxZxeMVpDmSVlxZJYFoWvHjGzTOAOYC5QCywxs0Xu/labxeYDFeHrZOBO4ORu2l4PLHb3m83s+vD7V80sC/gl8Pfu/oaZjQL29TTuRKqJNnLuMSo+KV3LzDBmTihk5oRCvnhuBdub9vKn6nqeX13Hi29H+cMbm4BggMD7p47i/VNHc8qUIgpz1ROW1BDL81ju7eW65wDV7h4BMLP7gQUEgwBaLQDu82DM8ytmVmhmpQS9kc7aLgDOCtvfCzwPfBX4ELDM3d8I497ay7gTYkfzXrY27WVqiXos0jMj87K5YOY4Lpg5jpYW563N7/FSdT1/qdnKb6tque/ldZjBceMKgkQzbTQnTRpJbnZMgz5F4i6WUWEVBE+KnAEc7Hu7e3c3SJYBG9p8ryXolXS3TFk3bce4++Ywhs1mVhJOPwpwM3sSKAbud/fvdrA/1wDXAJSXl3ezC/FTo6dGShxkZBjHlRVwXFkBnz9zKnv3t/BG7Q7+Ur2Vl2rqueeltfzPixGGZBqzJhQyZ3IRJ00q4sSJIxmRMyTZ4UuaiOVPmp8B/wncCpwNXM3hd+F3pqNl2t+N2dkysbRtL4ugYOZJBNdrFoc38Sw+bCXudwN3Q3CDZDfrjJvWoca6h0XiKTsrg5MmBcnji+dWsGvvAZa8s42/1Gzl5chW7nohwh3P1ZBhcPTY/IOJ5qTJI1UJQBImlsQyzN0Xm5m5+zrgJjP7E0Gy6UotMKHN9/HAphiXye6i7RYzKw17K6VAXZt1veDu9QBm9jgwGzgssSRLpL6JIZkqPimJNSw7kzOOKuaMo4oBaN67n6Xrd/Dq2m1UrdvGA0s28PO/vAPApFG5B5PS7IkjmTI6Tw8zk7iIJbHsNrMMYI2ZLQQ2AiXdtAFYAlSY2eSwzWXAJ9otswhYGF5DORloCBNGtIu2i4CrgJvD90fC6U8CXzGzXGAvcCZBLysl1NQ1Ul6k4pPSv3Kzszht2mhOmzYagH0HWlix6T2WrN3Gq+9s45mVW/jta7UA5OdkMXNCIe8rH8n7yguZNb5QQ+OlV2JJLF8CcoEvAP9FcDrsqu4aufv+MBE9STBk+B53X2Fm14bz7wIeJxhqXE1w+urqrtqGq74ZeNDMPgOsBy4J22w3s/8mSGgOPO7uj8Wwf/0iUt+kh3tJ0g3JzGDWhEJmTSjkc2dMoaXFidQ38rf1O1i6fgevb9jBj55dQ0t4knjy6DxmTSgMEs2EQo4pzdcfR9KtLotQhsN+b3b3f+2/kPpPfxWhVPFJGUia9uxnWW0Dr2/YwdL121m6YQfRncEtbEOzMjh2XD7HlxVwbFkBx5cVUFEynCwlm7TSpyKU7n7AzE4Mr6+kbxnkPlLxSRlI8oZmcerUUZw6dRQA7s6mht1Bklm/gzdrG/jda7Xc+/I6IEg2R5fmc3xZkHCOKyugomQE2VlKNukqllNhS4FHzOy3QFPrRHf/v4RFNci0Po5Yp8JkIDIzygqHUVY4jPNPGAcQnkJrYsWmBt6sbeDNjQ08vHQTv3xlPQDZmRkcXTqC48oKmFGazzGl+UwfO4LhQ3VvTTqI5V+5CNgKnNNmmgNKLDGK1KuqsQwuGRnGtJLhTCsZzoJZZUCQbNZta+bNjQ0s3xgknD+8sYlf/3X9wXblRbkcPXYER5fmc8zYERxTmk95Ua5Gow0ysdx5f3V/BDKYRaJNjMrLVskNGdQyMozJo/OYPDqPC2YGPRt3Z+OOXazavJNV777Hys07WfnuezyzcsvBAQLDhmQyfewIjikdwdFj84PEMzafglzd0DlQxXLn/VEENbzGuPtxYaHHC9z9GwmPbpCoiTbq+oqkJTNj/Mhcxo/M5dwZh+rk7dp7gDV1O1kVJppVm3fyxPJ3+c2rhwpujM3PoWLM8IM9o4qSEVSUDNcQ6AEgllNhPwH+laBqMO6+zMx+DSixxCgSbWLuDBWfFGk1LDuTE8YXcsL4woPT3J26nXtYuTno2ayp20l1XSMPLNlA894DB5cbPTybqcXDqRhzKNlMGzOc4uFD9STOFBFLYsl191fb/YPtT1A8g05r8Un1WES6ZmaMyc9hTH4OZ00/dA92S4uz+b3drNkSJJo1WxpZU7eTR17fxM7dh34V5edkUTFmBNOKhxTEfxwAABJPSURBVDOlODglN6U4jwlFuQzNykzGLqWtWBJLvZlNJazVZWYfAzYnNKpBRMUnRfomI+PQqLS2Cae1hxMkm52sqWtkTV0jz6zcwtaqvYfaG4wfmXvw+k9r0pk8Oo9xBcM0cCABYkks1xEUbTzazDYCa4ErEhrVIHLwOfclSiwi8dS2h9NasqZVQ/M+1m5tYm19I2ujTUTqm1hb38SSd7YddlptaFYGk0aFiaY4j8mj8igflcvEUbmMGZGjpNNLsYwKiwDnmlkekOHuO83sS8BtCY9uEKiJhsUnRw5LdigiaaMgdwizcoMyNG25O9Gdew4mmrX1TUSiTayp28niVVvYd+DQfeDZWRlMGDmM8qJcJo4KTqlNLMqlfFQuE0bmMixbp9c6E/PdSu7e1Obrl1FiiUkk2sjEUXkqeSGSAsyMkvwcSvJzOGXKqMPm7T/QwsYdu1i/rTl4bQ3e121tZsk722ncc/il5ZIRQykPE02QfIL38SNzKR4+NK17O729DTZ9j1gP1UQbdce9yACQlZnBxFF5TBx15EAbd2d7874w0TSxIUw467c183LNVh5aupG2Ra+yMzMYV5hD2cjg2tD4kbnBdaKRwxg/chhj83MG9R+bvU0sqhsWg30HWli/rZm5M8YmOxQR6QMzoygvm6K87CNOrwHs3neA2u27WL+tiY3bd1G7Y1fwvn0Xz62OHizi2SozwxibHySe8WHCOZiARg5jXGHOgB7J1mliMbOddJxADNAFgxhs2NbMvgOuUi4ig1zOkMyDN3J2ZPe+A2xu2E3t9mY2bt/Fxh1B0tm4fRd/XbuNza/vOliJoFXxiKGUFuQwNj+H0oIcSguHtfk+jDEFQ1M2+XSaWNx9RH8GMhhFWoca61SYSFrLGZJ5cIhzR/YdaOHdht1sbNPT2dywi80Nu1m3tZmXI1sPu2en1ejh2YwtyGFsftDLGVuQEyaf4PuY/BxyhvR/8lGp0QRS8UkRicWQzAwmFOV2+ejyxj37ebdh98GEE3wOvtdub2bJO9to2LXviHZFedmMyc9hbP5QxhbkHByiPX3sCGaXj0zI/iixJFBNnYpPikh8DB+a1eXpNoDmvfsPSzrvNuxiU/h9y3u7eXNjA/WNwc2jF8wcp8QyEEXqNSJMRPpPbnYWU4uHd/l7Z+/+FqKNezqdHw+Dd7xbCqiJNqlGmIiklOysjIMlchIloYnFzOaZ2Wozqzaz6zuYb2Z2ezh/mZnN7q6tmRWZ2dNmtiZ8H9luneVm1mhm/5LIfevOjua9bFPxSRFJQwlLLGaWCdwBzAdmAJeb2Yx2i80HKsLXNQTPfemu7fXAYnevABaH39u6FXgi7jvUQ63FJ3UqTETSTSJ7LHOAanePuPte4H5gQbtlFgD3eeAVoNDMSrtpuwC4N/x8L3Bh68rM7EIgAqxI1E7FqiYsPqmhxiKSbhKZWMqADW2+14bTYlmmq7Zj3H0zQPheAhAWyfwq8LWugjKza8ysysyqotFoj3aoJyIqPikiaSqRiaWjemLt7+TvbJlY2rb3NeBWd2/saiF3v9vdK929sri4uJtV9l6Nik+KSJpK5HDjWmBCm+/jgU0xLpPdRdstZlbq7pvD02Z14fSTgY+Z2XeBQqDFzHa7+4/isjc9FFHxSRFJU4n8c3oJUGFmk80sG7gMWNRumUXAleHosFOAhvD0VldtFwFXhZ+vAh4BcPcPuPskd59EUNL/W8lKKvsOtLBua7Me7iUiaSlhPRZ3329mC4EngUzgHndfYWbXhvPvAh4HzgOqgWbg6q7ahqu+GXjQzD4DrAcuSdQ+9NaGbc3sb3GmdFIXSERkMEvonffu/jhB8mg77a42n53g0ccxtQ2nbwU+2M12b+pFuHHTWnxSPRYRSUe6spwArUONp45WYhGR9KPEkgCRaBOjh2dTkDsk2aGIiPQ7JZYEqIk2MkW9FRFJU0osCRCpV/FJEUlfSixxtr0pKD6pe1hEJF0pscRZ61Mj1WMRkXSlxBJnqmosIulOiSXOaqKNDMk0xqv4pIikKSWWOItEm1R8UkTSmn77xVlNtJGpur4iImlMiSWO9h1oYf3WZj3cS0TSmhJLHLUWn9SFexFJZ0oscdQ6IkxDjUUknSmxxFFExSdFRJRY4qkm2qjikyKS9pRY4igSbVLxSRFJe0oscRSpb2Jqia6viEh6U2KJk9bik+qxiEi6U2KJk9bik+qxiEi6S2hiMbN5ZrbazKrN7PoO5puZ3R7OX2Zms7tra2ZFZva0ma0J30eG0+ea2Wtm9mb4fk4i9629mrpwqLF6LCKS5hKWWMwsE7gDmA/MAC43sxntFpsPVISva4A7Y2h7PbDY3SuAxeF3gHrgI+5+PHAV8IsE7VqHaupVfFJEBBLbY5kDVLt7xN33AvcDC9otswC4zwOvAIVmVtpN2wXAveHne4ELAdx9qbtvCqevAHLMbGiidq69mromJqn4pIhIQhNLGbChzffacFosy3TVdoy7bwYI30s62PbFwFJ339Pr6HsoUt+oO+5FREhsYrEOpnmMy8TStuONmh0LfAf4fCfzrzGzKjOrikajsayyW63FJ1UjTEQksYmlFpjQ5vt4YFOMy3TVdkt4uozwva51ITMbDzwEXOnuNR0F5e53u3ulu1cWFxf3eKc6sj4sPqmqxiIiiU0sS4AKM5tsZtnAZcCidsssAq4MR4edAjSEp7e6aruI4OI84fsjAGZWCDwG3ODuLyVwv44QOfg4Yp0KExHJStSK3X2/mS0EngQygXvcfYWZXRvOvwt4HDgPqAaagau7ahuu+mbgQTP7DLAeuCScvhCYBtxoZjeG0z7k7gd7NIlSExafVI9FRCSBiQXA3R8nSB5tp93V5rMD18XaNpy+FfhgB9O/AXyjjyH3SqS1+OQwFZ8UEdHY2DiIRJvUWxERCSmxxIGecy8icogSSx9ta9rL9uZ9GmosIhJSYumjyMEL9+qxiIiAEkuftQ41VvFJEZGAEksf1UQbyc7MUPFJEZGQEksf1USbmDgqV8UnRURC+m3YR5H6Rl24FxFpQ4mlD1qLT+rCvYjIIUosfdBafFI9FhGRQ5RY+qCmTkONRUTaU2Lpg0h9ONRYPRYRkYOUWPqgpq6R0cOHqvikiEgbSix9EKlv0mkwEZF2lFj6IBLVUGMRkfaUWHrpUPFJ9VhERNpSYuml1uKT6rGIiBxOiaWXalTVWESkQ0osvRSJNoXFJ3OTHYqISEpRYumlmmgTk0bnkplhyQ5FRCSlJDSxmNk8M1ttZtVmdn0H883Mbg/nLzOz2d21NbMiM3vazNaE7yPbzLshXH61mX04kfsWiTbqGSwiIh1IWGIxs0zgDmA+MAO43MxmtFtsPlARvq4B7oyh7fXAYnevABaH3wnnXwYcC8wDfhyuJ+72HWhh/bZmppbo+oqISHuJ7LHMAardPeLue4H7gQXtllkA3OeBV4BCMyvtpu0C4N7w873AhW2m3+/ue9x9LVAdrifu1m0Nik+qxyIicqREJpYyYEOb77XhtFiW6artGHffDBC+l/Rge5jZNWZWZWZV0Wi0RzvU1nnHj2XGuPxetxcRGawSmVg6uqrtMS4TS9vebA93v9vdK929sri4uJtVdmxayXB+fMWJHFOqxCIi0l4iE0stMKHN9/HAphiX6artlvB0GeF7XQ+2JyIiCZbIxLIEqDCzyWaWTXBhfVG7ZRYBV4ajw04BGsLTW121XQRcFX6+CnikzfTLzGyomU0mGBDwaqJ2TkREOpaVqBW7+34zWwg8CWQC97j7CjO7Npx/F/A4cB7BhfZm4Oqu2oarvhl40Mw+A6wHLgnbrDCzB4G3gP3Ade5+IFH7JyIiHTP37i5dDF6VlZVeVVWV7DBERAYUM3vN3Ss7m68770VEJK6UWEREJK6UWEREJK6UWEREJK7S+uK9mUWBdX1YxWigPk7hxJPi6hnF1TOKq2cGY1wT3b3TO8zTOrH0lZlVdTUyIlkUV88orp5RXD2TjnHpVJiIiMSVEouIiMSVEkvf3J3sADqhuHpGcfWM4uqZtItL11hERCSu1GMREZG4UmIREZG4UmLpBTObZ2arzazazK7vp22+Y2ZvmtnrZlYVTisys6fNbE34PrLN8jeE8a02sw+3mX5iuJ5qM7vdzDp6QFpXcdxjZnVmtrzNtLjFET724IFw+l/NbFIf4rrJzDaGx+x1MzsvCXFNMLPnzGylma0wsy+mwjHrIq6kHjMzyzGzV83sjTCur6XI8eosrlT4P5ZpZkvN7NFUOFYAuLtePXgRlPGvAaYA2cAbwIx+2O47wOh2074LXB9+vh74Tvh5RhjXUGByGG9mOO9V4FSCJ24+AczvYRxnALOB5YmIA/hH4K7w82XAA32I6ybgXzpYtj/jKgVmh59HAG+H20/qMesirqQes3Adw8PPQ4C/AqekwPHqLK5U+D/2ZeDXwKMp8/PYk18qejnhwX+yzfcbgBv6YbvvcGRiWQ2Uhp9LgdUdxUTwXJtTw2VWtZl+OfA/vYhlEof/Ao9bHK3LhJ+zCO4Mtl7G1dkPfb/G1W7bjwBzU+WYdRBXyhwzIBf4G3ByKh2vdnEl9XgRPCl3MXAOhxJL0o+VToX1XBmwoc332nBaojnwlJm9ZmbXhNPGePDETcL3km5iLAs/t5/eV/GM42Abd98PNACj+hDbQjNbZsGpstZTAkmJKzyN8D6Cv3ZT5pi1iwuSfMzCUzuvEzx2/Gl3T4nj1UlckNzjdRvwFaClzbSkHysllp7r6JpEf4zZPs3dZwPzgevM7Iwulu0sxv6OvTdxxDPGO4GpwCxgM/D9ZMVlZsOB3wNfcvf3ulq0P2PrIK6kHzN3P+Duswj+Gp9jZsd1tQtJjitpx8vMzgfq3P217mLvr5haKbH0XC0woc338cCmRG/U3TeF73XAQ8AcYIuZlQKE73XdxFgbfm4/va/iGcfBNmaWBRQA23oTlLtvCX8ZtAA/IThm/R6XmQ0h+OX9K3f/v3By0o9ZR3GlyjELY9kBPA/MIwWOV0dxJfl4nQZcYGbvAPcD55jZL0mBY6XE0nNLgAozm2xm2QQXtBYlcoNmlmdmI1o/Ax8ClofbvSpc7CqC8+SE0y8LR3RMBiqAV8Nu8U4zOyUc9XFlmzZ9Ec842q7rY8CzHp7g7anWH67QRQTHrF/jCtfzv8BKd//vNrOSesw6iyvZx8zMis2sMPw8DDgXWJUCx6vDuJJ5vNz9Bncf7+6TCH4PPevun0z2sWoNTq8evoDzCEbR1AD/rx+2N4VgNMcbwIrWbRKc61wMrAnfi9q0+X9hfKtpM/ILqCT4z18D/IieX+T9DUGXfx/BXzOfiWccQA7wW6CaYKTKlD7E9QvgTWBZ+ANSmoS4Tic4dbAMeD18nZfsY9ZFXEk9ZsAJwNJw+8uB/4j3//U4x5X0/2Nh27M4dPE+6T+PKukiIiJxpVNhIiISV0osIiISV0osIiISV0osIiISV0osIiISV0osIr1gZqPsUEXbd+3wCrfZ3bStNLPb4xDDp8xsXF/XIxJvGm4s0kdmdhPQ6O7fazMty4PaSonc7vMEBRCrErkdkZ7KSnYAIoOFmf2coNzF+4C/mdkDBEUChwG7gKvdfbWZnUWQEM4Pk1I5wU2w5cBt7n57u/VmEtwlX0lwU+M9BIUBK4Ffmdkugiq1M4D/BoYTVKH9lLtvDhPQ6wTlRvKBT7v7qwk6DCJKLCJxdhRwrrsfMLN84Ax3329m5wLfAi7uoM3RwNkEz0VZbWZ3uvu+NvNnAWXufhyAmRW6+w4zW0jYYwnrfv0QWODuUTO7FPgm8OlwHXnu/v6weOk9QFeFHUX6RIlFJL5+6+4Hws8FwL1mVkHQ0xjSSZvH3H0PsMfM6oAxHF7GPAJMMbMfAo8BT3WwjukEyeLpoNwTmQQlblr9BsDdXzSz/Nbk1Ks9FOmGEotIfDW1+fxfwHPufpEFzzx5vpM2e9p8PkC7n0t3325mM4EPA9cBH+dQT6SVASvc/dROttH+YqourkrCaFSYSOIUABvDz5/q7UrMbDSQ4e6/B24keAQzwE6C02cQFBUsNrNTwzZDzOzYNqu5NJx+OtDg7g29jUekO+qxiCTOdwlOhX0ZeLYP6ykDfmZmrX8I3hC+/xy4q83F+48Bt5tZAcHP9m0E1bABtpvZXwgv3vchFpFuabixyCCnYcnS33QqTERE4ko9FhERiSv1WEREJK6UWEREJK6UWEREJK6UWEREJK6UWEREJK7+f/qPMJXpEWaAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomizedSchedule(d_model)\n",
    "#下面是学习率的设计图\n",
    "plt.plot(\n",
    "    temp_learning_rate_schedule(\n",
    "        tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Leraning rate\")\n",
    "plt.xlabel(\"Train step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True, reduction = 'none')\n",
    "#为了不考虑padding部分的损失，需要mask\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造训练需要的3个mask\n",
    "def create_masks(inp, tar):\n",
    "    \"\"\"\n",
    "    Encoder:\n",
    "      - encoder_padding_mask (self attention of EncoderLayer)\n",
    "      对于encoder中padding值没作用，所以无需attention\n",
    "    Decoder:\n",
    "      - look_ahead_mask (self attention of DecoderLayer)\n",
    "      target位置上的词不能看到之后的词，因为之后的词没预测出来\n",
    "      - encoder_decoder_padding_mask (encoder-decoder attention of DecoderLayer)\n",
    "      decoder不应该到encoder的padding上去花费精力\n",
    "      - decoder_padding_mask (self attention of DecoderLayer)\n",
    "      decoder也有padding，所以mask掉\n",
    "    \"\"\"\n",
    "    encoder_padding_mask = create_padding_mask(inp)  #给padding做了mask\n",
    "    encoder_decoder_padding_mask = create_padding_mask(inp) #给encoder_decoder做的padding做了mask\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])  #不能看到后面的词\n",
    "    decoder_padding_mask = create_padding_mask(tar)  #tar的padding做了mask\n",
    "    decoder_mask = tf.maximum(decoder_padding_mask,\n",
    "                              look_ahead_mask)  #因为一个batch样本长度不一样，组合求max  # 20220524 这里备注有问题\n",
    "    \n",
    "    print( encoder_padding_mask.shape )\n",
    "    print( encoder_decoder_padding_mask.shape )\n",
    "    print( look_ahead_mask.shape )\n",
    "    print( decoder_padding_mask.shape )\n",
    "    print( decoder_mask.shape )\n",
    "\n",
    "    \n",
    "    return encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_inp, temp_tar = iter(train_dataset.take(1)).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40)\n",
      "(64, 40)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(40, 40)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 40, 40)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 1, 1, 40), dtype=float32, numpy=\n",
       " array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1, 40, 40), dtype=float32, numpy=\n",
       " array([[[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 1., 1.],\n",
       "          [0., 0., 0., ..., 0., 0., 1.],\n",
       "          [0., 0., 0., ..., 0., 0., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 1., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.],\n",
       "          [0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(64, 1, 1, 40), dtype=float32, numpy=\n",
       " array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
       " \n",
       " \n",
       "        [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(temp_inp.shape)\n",
    "print(temp_tar.shape)\n",
    "create_masks(temp_inp, temp_tar)\n",
    "#样本大小是64，不足的补齐35，或者39\n",
    "#最后是(64, 1, 39, 39)原因是既不关注前面的padding，也不关注后面的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "Epoch 1 Batch 0 Loss 4.0788 Accuracy 0.0000\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "Epoch 1 Batch 100 Loss 4.1737 Accuracy 0.0112\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "Epoch 1 Batch 200 Loss 4.0695 Accuracy 0.0177\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "Epoch 1 Batch 300 Loss 3.9252 Accuracy 0.0268\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(39, 39)\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 39, 39)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 1, 35)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "Epoch 1 Batch 400 Loss 3.7519 Accuracy 0.0329\n",
      "Epoch 1 Batch 500 Loss 3.6017 Accuracy 0.0382\n",
      "(64, 1, 1, 39)\n",
      "(64, 1, 1, 39)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "Epoch 1 Batch 600 Loss 3.4810 Accuracy 0.0449\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(38, 38)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 38, 38)\n",
      "Epoch 1 Batch 700 Loss 3.3707 Accuracy 0.0519\n",
      "(31, 1, 1, 40)\n",
      "(31, 1, 1, 40)\n",
      "(35, 35)\n",
      "(31, 1, 1, 35)\n",
      "(31, 1, 35, 35)\n",
      "Epoch 1 Loss 3.3689 Accuracy 0.0520\n",
      "Time take for 1 epoch: 402.2740333080292 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.5568 Accuracy 0.0955\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "Epoch 2 Batch 100 Loss 2.5357 Accuracy 0.1052\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "Epoch 2 Batch 200 Loss 2.4926 Accuracy 0.1100\n",
      "(64, 1, 1, 40)\n",
      "(64, 1, 1, 40)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "Epoch 2 Batch 300 Loss 2.4659 Accuracy 0.1138\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 1, 1, 38)\n",
      "(64, 1, 1, 38)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "Epoch 2 Batch 400 Loss 2.4423 Accuracy 0.1175\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(33, 33)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 33, 33)\n",
      "Epoch 2 Batch 500 Loss 2.4123 Accuracy 0.1205\n",
      "Epoch 2 Batch 600 Loss 2.3824 Accuracy 0.1230\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "Epoch 2 Batch 700 Loss 2.3617 Accuracy 0.1253\n",
      "(31, 1, 1, 32)\n",
      "(31, 1, 1, 32)\n",
      "(34, 34)\n",
      "(31, 1, 1, 34)\n",
      "(31, 1, 34, 34)\n",
      "Epoch 2 Loss 2.3614 Accuracy 0.1254\n",
      "Time take for 1 epoch: 132.17923021316528 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.0038 Accuracy 0.1303\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(36, 36)\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 36, 36)\n",
      "Epoch 3 Batch 100 Loss 2.1562 Accuracy 0.1449\n",
      "Epoch 3 Batch 200 Loss 2.1487 Accuracy 0.1459\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "Epoch 3 Batch 300 Loss 2.1412 Accuracy 0.1464\n",
      "(64, 1, 1, 36)\n",
      "(64, 1, 1, 36)\n",
      "(31, 31)\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 31, 31)\n",
      "Epoch 3 Batch 400 Loss 2.1337 Accuracy 0.1473\n",
      "Epoch 3 Batch 500 Loss 2.1245 Accuracy 0.1481\n",
      "Epoch 3 Batch 600 Loss 2.1106 Accuracy 0.1490\n",
      "Epoch 3 Batch 700 Loss 2.1012 Accuracy 0.1504\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 1, 38)\n",
      "(36, 36)\n",
      "(31, 1, 1, 36)\n",
      "(31, 1, 36, 36)\n",
      "Epoch 3 Loss 2.1008 Accuracy 0.1504\n",
      "Time take for 1 epoch: 96.18090009689331 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.8884 Accuracy 0.1617\n",
      "Epoch 4 Batch 100 Loss 1.9472 Accuracy 0.1642\n",
      "Epoch 4 Batch 200 Loss 1.9387 Accuracy 0.1649\n",
      "Epoch 4 Batch 300 Loss 1.9298 Accuracy 0.1662\n",
      "Epoch 4 Batch 400 Loss 1.9192 Accuracy 0.1682\n",
      "Epoch 4 Batch 500 Loss 1.9063 Accuracy 0.1705\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "(64, 1, 1, 33)\n",
      "(64, 1, 1, 33)\n",
      "(35, 35)\n",
      "(64, 1, 1, 35)\n",
      "(64, 1, 35, 35)\n",
      "Epoch 4 Batch 600 Loss 1.8917 Accuracy 0.1728\n",
      "Epoch 4 Batch 700 Loss 1.8823 Accuracy 0.1750\n",
      "(31, 1, 1, 36)\n",
      "(31, 1, 1, 36)\n",
      "(39, 39)\n",
      "(31, 1, 1, 39)\n",
      "(31, 1, 39, 39)\n",
      "Epoch 4 Loss 1.8822 Accuracy 0.1750\n",
      "Time take for 1 epoch: 90.11680722236633 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.7196 Accuracy 0.1879\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(37, 37)\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 37, 37)\n",
      "Epoch 5 Batch 100 Loss 1.6999 Accuracy 0.1925\n",
      "(64, 1, 1, 37)\n",
      "(64, 1, 1, 37)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "Epoch 5 Batch 200 Loss 1.6971 Accuracy 0.1957\n",
      "(64, 1, 1, 31)\n",
      "(64, 1, 1, 31)\n",
      "(32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 32, 32)\n",
      "Epoch 5 Batch 300 Loss 1.7014 Accuracy 0.1983\n",
      "Epoch 5 Batch 400 Loss 1.6916 Accuracy 0.1997\n",
      "Epoch 5 Batch 500 Loss 1.6813 Accuracy 0.2014\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 1, 34)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "Epoch 5 Batch 600 Loss 1.6693 Accuracy 0.2027\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(34, 34)\n",
      "(64, 1, 1, 34)\n",
      "(64, 1, 34, 34)\n",
      "Epoch 5 Batch 700 Loss 1.6606 Accuracy 0.2039\n",
      "(31, 1, 1, 32)\n",
      "(31, 1, 1, 32)\n",
      "(30, 30)\n",
      "(31, 1, 1, 30)\n",
      "(31, 1, 30, 30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 1.6607 Accuracy 0.2040\n",
      "Time take for 1 epoch: 107.58127069473267 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.5705 Accuracy 0.2096\n",
      "Epoch 6 Batch 100 Loss 1.5102 Accuracy 0.2204\n",
      "Epoch 6 Batch 200 Loss 1.4999 Accuracy 0.2203\n",
      "Epoch 6 Batch 300 Loss 1.4951 Accuracy 0.2204\n",
      "Epoch 6 Batch 400 Loss 1.4878 Accuracy 0.2218\n",
      "(64, 1, 1, 32)\n",
      "(64, 1, 1, 32)\n",
      "(30, 30)\n",
      "(64, 1, 1, 30)\n",
      "(64, 1, 30, 30)\n",
      "Epoch 6 Batch 500 Loss 1.4823 Accuracy 0.2231\n",
      "Epoch 6 Batch 600 Loss 1.4768 Accuracy 0.2240\n",
      "Epoch 6 Batch 700 Loss 1.4701 Accuracy 0.2252\n",
      "(31, 1, 1, 40)\n",
      "(31, 1, 1, 40)\n",
      "(38, 38)\n",
      "(31, 1, 1, 38)\n",
      "(31, 1, 38, 38)\n",
      "Epoch 6 Loss 1.4704 Accuracy 0.2252\n",
      "Time take for 1 epoch: 84.98980355262756 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
    "    name = 'train_accuracy')\n",
    "# start I  have  a  dream end\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp  = tar[:, :-1]  #没带end    decode 输入 ，比如输入start得到I\n",
    "    tar_real = tar[:, 1:]   #没有start  输入的是dream时，输入是end\n",
    "    \n",
    "    encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
    "    = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, True,\n",
    "                                     encoder_padding_mask,\n",
    "                                     decoder_mask,\n",
    "                                     encoder_decoder_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients, transformer.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "#一个epochs接近90秒\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    #reset后就会从零开始累计\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                epoch + 1, batch, train_loss.result(),\n",
    "                train_accuracy.result()))\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "        epoch + 1, train_loss.result(), train_accuracy.result()))\n",
    "    print('Time take for 1 epoch: {} secs\\n'.format(\n",
    "        time.time() - start))\n",
    "\n",
    "#loss是一个正常的指标，accuracy只是机器翻译的一个参考指标，可以看趋势\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eg: A B C D -> E F G H.\n",
    "Train: A B C D, E F G -> F G H\n",
    "Eval:  A B C D -> E\n",
    "       A B C D, E -> F\n",
    "       A B C D, E F -> G\n",
    "       A B C D, E F G -> H\n",
    "类似seq2seq2\n",
    "不同的是 transformer可以并行的处理，前后没有依赖，而seq2seq前后有依赖\n",
    "\"\"\"\n",
    "def evaluate(inp_sentence):\n",
    "    #文本的句子转换为id的句子\n",
    "    input_id_sentence = [pt_tokenizer.vocab_size] \\\n",
    "    + pt_tokenizer.encode(inp_sentence) + [pt_tokenizer.vocab_size + 1]\n",
    "    #transformer转换是两维的，因此转换\n",
    "    # encoder_input.shape: (1, input_sentence_length)\n",
    "    encoder_input = tf.expand_dims(input_id_sentence, 0)\n",
    "    \n",
    "    # decoder_input.shape: (1, 1)\n",
    "    #我们预测一个词就放入decoder_input，decoder_input给多个就可以预测多个，我们给一个\n",
    "    decoder_input = tf.expand_dims([en_tokenizer.vocab_size], 0)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        #产生mask并传给transformer\n",
    "        encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
    "        = create_masks(encoder_input, decoder_input)\n",
    "        # predictions.shape: (batch_size, output_target_len, target_vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            decoder_input,\n",
    "            False,\n",
    "            encoder_padding_mask,\n",
    "            decoder_mask,\n",
    "            encoder_decoder_padding_mask)\n",
    "        # predictions.shape: (batch_size, output_target_len, target_vocab_size)\n",
    "        #decoder每次输出的维度和输入的维度一致的，所以是最后一个\n",
    "        predictions = predictions[:, -1, :]\n",
    "        #预测值就是概率最大的那个的索引\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis = -1),\n",
    "                               tf.int32)\n",
    "        #如果等于end id，预测结束\n",
    "        if tf.equal(predicted_id, en_tokenizer.vocab_size + 1):\n",
    "            return tf.squeeze(decoder_input, axis = 0), attention_weights\n",
    "        #如果predicted_id不是end id，添加到新的decoder_input中\n",
    "        decoder_input = tf.concat([decoder_input, [predicted_id]],\n",
    "                                  axis = -1)\n",
    "    return tf.squeeze(decoder_input, axis = 0), attention_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoder_decoder_attention(attention, input_sentence,\n",
    "                                   result, layer_name):\n",
    "    fig = plt.figure(figsize = (16, 8))\n",
    "    \n",
    "    input_id_sentence = pt_tokenizer.encode(input_sentence)\n",
    "    \n",
    "    # attention.shape: (num_heads, tar_len, input_len)\n",
    "    attention = tf.squeeze(attention[layer_name], axis = 0)\n",
    "    \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head + 1)\n",
    "        \n",
    "        ax.matshow(attention[head][:-1, :])\n",
    "        \n",
    "        fontdict = {'fontsize': 10}\n",
    "        \n",
    "        ax.set_xticks(range(len(input_id_sentence) + 2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "        \n",
    "        ax.set_ylim(len(result) - 1.5, -0.5)\n",
    "        \n",
    "        ax.set_xticklabels(\n",
    "            ['<start>'] + [pt_tokenizer.decode([i]) for i in input_id_sentence] + ['<end>'],\n",
    "            fontdict = fontdict, rotation = 90)\n",
    "        ax.set_yticklabels(\n",
    "            [en_tokenizer.decode([i]) for i in result if i < en_tokenizer.vocab_size],\n",
    "            fontdict = fontdict)\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, layer_name = ''):\n",
    "    result, attention_weights = evaluate(input_sentence)\n",
    "    \n",
    "    predicted_sentence = en_tokenizer.decode(\n",
    "        [i for i in result if i < en_tokenizer.vocab_size])\n",
    "    \n",
    "    print(\"Input: {}\".format(input_sentence))\n",
    "    print(\"Predicted translation: {}\".format(predicted_sentence))\n",
    "    \n",
    "    if layer_name:\n",
    "        plot_encoder_decoder_attention(attention_weights, input_sentence,\n",
    "                                       result, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('está muito frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('isto é minha vida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('você ainda está em casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('este é o primeiro livro que eu já li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('este é o primeiro livro que eu já li',\n",
    "          layer_name = 'decoder_layer4_att2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
