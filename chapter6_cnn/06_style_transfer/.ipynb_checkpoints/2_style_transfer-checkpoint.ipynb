{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# 之前vgg模型，RGB通道的3个均值，我们输入图像时，需要减去这三个均值，这三个均值是写在vgg net的代码中的\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "       load parameters from pre-train models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "    \n",
    "    def get_conv_filter(self, name):  # name可能是conv1_2\n",
    "        # self.data_dict[name][0]是w参数，[1]是偏置\n",
    "        return tf.constant(self.data_dict[name][0], name='conv')\n",
    "    \n",
    "    def get_fc_weight(self, name):  # 和上面类似\n",
    "        return tf.constant(self.data_dict[name][0], name='fc')\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name='bias')\n",
    "    \n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"搭建卷积层\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding='SAME')  # 这个api比layers更基础\n",
    "            # 上面第三个参数是各个维度的stide\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "    \n",
    "    \n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"搭建池化层\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize = [1,2,2,1],\n",
    "                              strides = [1,2,2,1],\n",
    "                              padding = 'SAME',\n",
    "                              name = name)\n",
    "    \n",
    "    def fc_layer(self, x, name, activation=tf.nn.relu):\n",
    "        \"\"\"搭建全连接层\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w)\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation is None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "    \n",
    "    def flatten_layer(self, x, name):  # 通过展平将卷积层展平后给全连接\n",
    "        \"\"\"搭建展平层\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            # [batch_size, image_width, image_height, channel]  4维张量含义\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:  # 把后3个维度相乘\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim])  # -1就会变为batch_size\n",
    "            return x\n",
    "    \n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"Build VGG16 network structure.\n",
    "        Parameters:\n",
    "        - x_rgb: [1, 224, 224, 3]  #这个设置是vgg_net的设置\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print('building model ...')\n",
    "        \n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis=3)  # 切分为3份，每份只有一个通道，从轴3切割\n",
    "        x_bgr = tf.concat(\n",
    "            [b - VGG_MEAN[0],\n",
    "             g - VGG_MEAN[1],\n",
    "             r - VGG_MEAN[2]],\n",
    "            axis = 3)  # 每个通道减去均值后再次合并\n",
    "        \n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224, 224, 3]  # 做一个断言，防止后面出错\n",
    "        # 这里是第一组\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1')\n",
    "        # 第二组\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        # 大部分时间都花费在下面的全连接层上,可以注释掉看看\n",
    "        \n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation=None)  # fc8不加激活函数是因为最后我们要进行softmax\n",
    "        self.prob = tf.nn.softmax(self.fc8, name='prob')\n",
    "        \n",
    "        \n",
    "        print('building model finished: %4ds' % (time.time() - start_time))  # 模型构建好再次打印时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model ...\n",
      "building model finished:    6s\n"
     ]
    }
   ],
   "source": [
    "# 下面代码只是测试一下模型的构建时间\n",
    "vgg16_npy_path = 'vgg16.npy'\n",
    "data_dict =np.load('vgg16.npy', encoding='latin1',allow_pickle=True).item()\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32,shape=[1,224,224,3])\n",
    "vgg16_for_result.build(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载vgg16(参数)，建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_npy_path = 'vgg16.npy'\n",
    "content_img_path = 'gugong.jpg'  # 内容图像路径，这个可以修改\n",
    "style_img_path = 'xingkong.jpeg'  # 风格图像路径，这个也可以修改\n",
    "\n",
    "num_steps = 100  # 训练多少步\n",
    "learning_rate = 10  # 学习率\n",
    "\n",
    "lambda_c = 0.1   # 内容损失的系数，如果设置为0，就是只用风格特征重建图片\n",
    "lambda_s = 500   # 风格损失系数，通过最终的打印就可以明白为什么，如果为零，就是只有内容特征重建图片\n",
    "\n",
    "output_dir = './run_style_transfer'  # 输出文件夹\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "building model ...\n",
      "building model finished:    5s\n",
      "building model ...\n",
      "building model finished:    2s\n",
      "building model ...\n",
      "building model finished:    2s\n",
      "WARNING:tensorflow:From /home/wp/.virtualenvs/tf1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 对图像进行初始化\n",
    "# shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定\n",
    "def initial_result(shape, mean, stddev):\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 读取图像数据\n",
    "def read_img(img_name):\n",
    "    img = Image.open(img_name)\n",
    "    np_img = np.array(img) # (224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype=np.int32) # 转维度为(1, 224, 224, 3)\n",
    "    return np_img\n",
    "\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calulates gram matrix\n",
    "    Args:\n",
    "    - x: feaures extracted from VGG Net. shape: [1, width, height, ch]\n",
    "    \"\"\"\n",
    "    b, w, h, ch = x.get_shape().as_list()  # 获取各个维度的值\n",
    "    features = tf.reshape(x, [b, h*w, ch]) # [ch, ch] -> (i, j)  # 因为w和h维度像素点特点一致\n",
    "    # [h*w, ch] matrix -> [ch, h*w] * [h*w, ch] -> [ch, ch]  \n",
    "    # 计算任意两列的相似度，通过矩阵乘法即可，adjoint_a是把其中一个features进行转置\n",
    "    # 为了防止最终的数比较大，我们除以一个常量，矩阵维度的乘积\n",
    "    gram = tf.matmul(features, features, adjoint_a=True) / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "    \n",
    "\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "content = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])  # 这是1.0版本需要的\n",
    "style = tf.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "\n",
    "data_dict = np.load(vgg16_npy_path, encoding='latin1',allow_pickle=True).item()\n",
    "# 创建3个vggnet\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "vgg_for_content.build(content)  # content是它的输入\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "# 下面的层次也是超参数，多层效果比较好\n",
    "# 可以加其他层特征来尝试，感受不同的效果\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    # vgg_for_content.conv2_2,\n",
    "    # vgg_for_content.conv3_3,\n",
    "    # vgg_for_content.conv4_3,\n",
    "    # vgg_for_content.conv5_3\n",
    "]\n",
    "\n",
    "# 结果图像提取内容特征，结果一定要和内容的层数保持一致\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    # vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# 也给风格特征初始化层次\n",
    "# feature_size, [1, width, height, channel]\n",
    "style_features = [\n",
    "    # vgg_for_style.conv1_2,\n",
    "    # vgg_for_style.conv2_2,\n",
    "    # vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    # vgg_for_style.conv5_3\n",
    "]\n",
    "\n",
    "# 风格图像的gram矩阵，gram矩阵是两两通道之间的相似度\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "# 给结果图像提取风格特征，和风格特征图像的层次必须一致\n",
    "result_style_features = [\n",
    "    # vgg_for_result.conv1_2,\n",
    "    # vgg_for_result.conv2_2,\n",
    "    # vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    # vgg_for_result.conv5_3\n",
    "]\n",
    "\n",
    "# 结果图像的gram矩阵\n",
    "result_style_gram = \\\n",
    "    [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "content_loss = tf.zeros(1, tf.float32)\n",
    "# zip: [1, 2], [3, 4], zip([1,2], [3,4]) -> [(1, 3), (2, 4)]\n",
    "# shape: [1, width, height, channel]\n",
    "#因为是多层的，所以需要对每一层去计算损失，加起来\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1, 2, 3])\n",
    "\n",
    "# 风格损失是gram矩阵的损失\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1, 2])\n",
    "\n",
    "# 最终的损失是内容的损失，是内容损失和风格损失的加权\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练（无监督）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 14290.1348, content_loss: 60762.7617, style_loss:  16.4277\n",
      "step: 2, loss_value: 11897.3613, content_loss: 46404.1367, style_loss:  14.5139\n",
      "step: 3, loss_value: 9192.0273, content_loss: 37857.9609, style_loss:  10.8125\n",
      "step: 4, loss_value: 7623.8311, content_loss: 33294.2852, style_loss:   8.5888\n",
      "step: 5, loss_value: 6845.5195, content_loss: 30470.9531, style_loss:   7.5968\n",
      "step: 6, loss_value: 6176.9346, content_loss: 28812.4590, style_loss:   6.5914\n",
      "step: 7, loss_value: 5325.7437, content_loss: 27791.3223, style_loss:   5.0932\n",
      "step: 8, loss_value: 5141.1670, content_loss: 27275.6855, style_loss:   4.8272\n",
      "step: 9, loss_value: 4622.7500, content_loss: 26944.9023, style_loss:   3.8565\n",
      "step: 10, loss_value: 4503.7935, content_loss: 26751.0195, style_loss:   3.6574\n",
      "step: 11, loss_value: 4262.6870, content_loss: 26599.7695, style_loss:   3.2054\n",
      "step: 12, loss_value: 4172.7349, content_loss: 26377.6895, style_loss:   3.0699\n",
      "step: 13, loss_value: 3995.2620, content_loss: 26012.3926, style_loss:   2.7880\n",
      "step: 14, loss_value: 3874.4519, content_loss: 25574.1602, style_loss:   2.6341\n",
      "step: 15, loss_value: 3737.0923, content_loss: 25057.7188, style_loss:   2.4626\n",
      "step: 16, loss_value: 3589.5264, content_loss: 24370.5762, style_loss:   2.3049\n",
      "step: 17, loss_value: 3470.0527, content_loss: 23553.1230, style_loss:   2.2295\n",
      "step: 18, loss_value: 3319.9233, content_loss: 22685.9492, style_loss:   2.1027\n",
      "step: 19, loss_value: 3183.5005, content_loss: 21754.1777, style_loss:   2.0162\n",
      "step: 20, loss_value: 3066.2400, content_loss: 20777.2090, style_loss:   1.9770\n",
      "step: 21, loss_value: 2919.7234, content_loss: 19823.1953, style_loss:   1.8748\n",
      "step: 22, loss_value: 2789.8281, content_loss: 18865.7324, style_loss:   1.8065\n",
      "step: 23, loss_value: 2670.6841, content_loss: 17943.8262, style_loss:   1.7526\n",
      "step: 24, loss_value: 2561.4783, content_loss: 17086.3027, style_loss:   1.7057\n",
      "step: 25, loss_value: 2468.0220, content_loss: 16262.3428, style_loss:   1.6836\n",
      "step: 26, loss_value: 2368.6423, content_loss: 15534.2012, style_loss:   1.6304\n",
      "step: 27, loss_value: 2298.9680, content_loss: 14831.7598, style_loss:   1.6316\n",
      "step: 28, loss_value: 2202.1941, content_loss: 14235.6641, style_loss:   1.5573\n",
      "step: 29, loss_value: 2133.6689, content_loss: 13652.7734, style_loss:   1.5368\n",
      "step: 30, loss_value: 2037.4722, content_loss: 13158.6553, style_loss:   1.4432\n",
      "step: 31, loss_value: 1966.9861, content_loss: 12678.3594, style_loss:   1.3983\n",
      "step: 32, loss_value: 1892.6483, content_loss: 12240.6348, style_loss:   1.3372\n",
      "step: 33, loss_value: 1838.0870, content_loss: 11826.9590, style_loss:   1.3108\n",
      "step: 34, loss_value: 1793.3547, content_loss: 11437.4824, style_loss:   1.2992\n",
      "step: 35, loss_value: 1770.5115, content_loss: 11099.8721, style_loss:   1.3210\n",
      "step: 36, loss_value: 1905.1099, content_loss: 10750.6543, style_loss:   1.6601\n",
      "step: 37, loss_value: 1855.7863, content_loss: 10532.1084, style_loss:   1.6052\n",
      "step: 38, loss_value: 1771.5427, content_loss: 10261.4961, style_loss:   1.4908\n",
      "step: 39, loss_value: 1656.0854, content_loss: 10086.0156, style_loss:   1.2950\n",
      "step: 40, loss_value: 1710.6650, content_loss: 9968.8018, style_loss:   1.4276\n",
      "step: 41, loss_value: 1637.7887, content_loss: 9811.3701, style_loss:   1.3133\n",
      "step: 42, loss_value: 1577.4387, content_loss: 9694.5996, style_loss:   1.2160\n",
      "step: 43, loss_value: 1588.9761, content_loss: 9579.4990, style_loss:   1.2621\n",
      "step: 44, loss_value: 1532.7654, content_loss: 9401.9863, style_loss:   1.1851\n",
      "step: 45, loss_value: 1489.3323, content_loss: 9258.3398, style_loss:   1.1270\n",
      "step: 46, loss_value: 1492.8541, content_loss: 9137.3252, style_loss:   1.1582\n",
      "step: 47, loss_value: 1437.6025, content_loss: 8970.5596, style_loss:   1.0811\n",
      "step: 48, loss_value: 1414.4194, content_loss: 8821.1152, style_loss:   1.0646\n",
      "step: 49, loss_value: 1388.1541, content_loss: 8680.1045, style_loss:   1.0403\n",
      "step: 50, loss_value: 1362.5459, content_loss: 8514.6250, style_loss:   1.0222\n",
      "step: 51, loss_value: 1345.8342, content_loss: 8356.9209, style_loss:   1.0203\n",
      "step: 52, loss_value: 1337.1191, content_loss: 8185.3521, style_loss:   1.0372\n",
      "step: 53, loss_value: 1328.2047, content_loss: 8050.4731, style_loss:   1.0463\n",
      "step: 54, loss_value: 1353.4502, content_loss: 7883.5688, style_loss:   1.1302\n",
      "step: 55, loss_value: 1324.3457, content_loss: 7796.7539, style_loss:   1.0893\n",
      "step: 56, loss_value: 1324.6501, content_loss: 7674.7432, style_loss:   1.1144\n",
      "step: 57, loss_value: 1238.0051, content_loss: 7596.8438, style_loss:   0.9566\n",
      "step: 58, loss_value: 1263.8938, content_loss: 7512.3286, style_loss:   1.0253\n",
      "step: 59, loss_value: 1248.0303, content_loss: 7409.1318, style_loss:   1.0142\n",
      "step: 60, loss_value: 1238.4495, content_loss: 7355.2549, style_loss:   1.0058\n",
      "step: 61, loss_value: 1244.6428, content_loss: 7251.7690, style_loss:   1.0389\n",
      "step: 62, loss_value: 1209.0660, content_loss: 7177.2349, style_loss:   0.9827\n",
      "step: 63, loss_value: 1180.4192, content_loss: 7095.6104, style_loss:   0.9417\n",
      "step: 64, loss_value: 1162.3850, content_loss: 7037.1567, style_loss:   0.9173\n",
      "step: 65, loss_value: 1146.3971, content_loss: 6949.3975, style_loss:   0.9029\n",
      "step: 66, loss_value: 1122.3923, content_loss: 6875.6055, style_loss:   0.8697\n",
      "step: 67, loss_value: 1118.2729, content_loss: 6792.7451, style_loss:   0.8780\n",
      "step: 68, loss_value: 1099.7847, content_loss: 6708.0352, style_loss:   0.8580\n",
      "step: 69, loss_value: 1101.5828, content_loss: 6609.1064, style_loss:   0.8813\n",
      "step: 70, loss_value: 1122.9709, content_loss: 6552.5181, style_loss:   0.9354\n",
      "step: 71, loss_value: 1290.0210, content_loss: 6435.7207, style_loss:   1.2929\n",
      "step: 72, loss_value: 1302.9097, content_loss: 6441.2671, style_loss:   1.3176\n",
      "step: 73, loss_value: 1300.5725, content_loss: 6386.6309, style_loss:   1.3238\n",
      "step: 74, loss_value: 1163.4614, content_loss: 6433.9932, style_loss:   1.0401\n",
      "step: 75, loss_value: 1255.2101, content_loss: 6553.4424, style_loss:   1.1997\n",
      "step: 76, loss_value: 1190.1992, content_loss: 6605.8120, style_loss:   1.0592\n",
      "step: 77, loss_value: 1158.3184, content_loss: 6694.5605, style_loss:   0.9777\n",
      "step: 78, loss_value: 1178.2612, content_loss: 6776.5078, style_loss:   1.0012\n",
      "step: 79, loss_value: 1158.9568, content_loss: 6787.4663, style_loss:   0.9604\n",
      "step: 80, loss_value: 1137.9226, content_loss: 6809.7290, style_loss:   0.9139\n",
      "step: 81, loss_value: 1117.0583, content_loss: 6801.8755, style_loss:   0.8737\n",
      "step: 82, loss_value: 1107.9008, content_loss: 6744.8916, style_loss:   0.8668\n",
      "step: 83, loss_value: 1086.0731, content_loss: 6682.0244, style_loss:   0.8357\n",
      "step: 84, loss_value: 1071.5516, content_loss: 6611.1104, style_loss:   0.8209\n",
      "step: 85, loss_value: 1058.0364, content_loss: 6523.7896, style_loss:   0.8113\n",
      "step: 86, loss_value: 1042.2825, content_loss: 6413.8413, style_loss:   0.8018\n",
      "step: 87, loss_value: 1032.6622, content_loss: 6290.8037, style_loss:   0.8072\n",
      "step: 88, loss_value: 1021.7198, content_loss: 6194.0659, style_loss:   0.8046\n",
      "step: 89, loss_value: 1038.8313, content_loss: 6076.8599, style_loss:   0.8623\n",
      "step: 90, loss_value: 1060.0789, content_loss: 6016.1035, style_loss:   0.9169\n",
      "step: 91, loss_value: 1142.0286, content_loss: 5891.8535, style_loss:   1.1057\n",
      "step: 92, loss_value: 1028.2458, content_loss: 5871.3330, style_loss:   0.8822\n",
      "step: 93, loss_value: 1027.5388, content_loss: 5832.2861, style_loss:   0.8886\n",
      "step: 94, loss_value: 1034.0159, content_loss: 5790.8633, style_loss:   0.9099\n",
      "step: 95, loss_value: 1003.6968, content_loss: 5802.3755, style_loss:   0.8469\n",
      "step: 96, loss_value: 1004.8112, content_loss: 5791.1851, style_loss:   0.8514\n",
      "step: 97, loss_value: 985.7234, content_loss: 5770.8022, style_loss:   0.8173\n",
      "step: 98, loss_value: 969.3408, content_loss: 5742.8271, style_loss:   0.7901\n",
      "step: 99, loss_value: 958.9142, content_loss: 5703.0962, style_loss:   0.7772\n",
      "step: 100, loss_value: 951.7776, content_loss: 5651.7930, style_loss:   0.7732\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(num_steps):  # 训练步骤\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                     feed_dict = {\n",
    "                         content: content_val,  # 输入内容图像\n",
    "                         style: style_val,  # 输入风格图像\n",
    "                     })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \n",
    "              % (step+1,\n",
    "                 loss_value[0],\n",
    "                 content_loss_value[0],\n",
    "                 style_loss_value[0]))  # 每次训练打印loss，content_loss，style_loss\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step+1))  # 每一步都把结果图像存储\n",
    "        result_val = result.eval(sess)[0]  # 本身维度是(1, 224, 224, 3)，[0]就是(224, 224, 3)\n",
    "        result_val = np.clip(result_val, 0, 255)  # 值拉到0到255直接\n",
    "        img_arr = np.asarray(result_val, np.uint8)\n",
    "        img = Image.fromarray(img_arr)  # 这个可以将某个ndarray变为图像\n",
    "        img.save(result_img_path)  # 保存图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
