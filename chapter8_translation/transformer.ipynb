{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0ori6p2fvT3",
        "outputId": "93aa50d3-059e-4908-bc28-c5aa6111c1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "sys.version_info(major=3, minor=7, micro=13, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.2\n",
            "numpy 1.21.6\n",
            "pandas 1.3.5\n",
            "sklearn 1.0.2\n",
            "tensorflow 2.8.0\n",
            "keras.api._v2.keras 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "    print(module.__name__, module.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KQUXfQkfvT9",
        "outputId": "7a7000b3-70d2-46df-dc73-028f876305ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  1 05:34:08 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    34W /  70W |   1452MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "lR9OLJjDfvT_"
      },
      "outputs": [],
      "source": [
        "# 1. loads data\n",
        "# 2. preprocesses data -> dataset\n",
        "# 3. tools\n",
        "# 3.1 generates position embedding 位置编码\n",
        "# 3.2 create mask. (a. padding, b. decoder)\n",
        "# 3.3 scaled_dot_product_attention 缩放点积注意力\n",
        "# 4. builds model 分为以下6步\n",
        "    # 4.1 MultiheadAttention 多头注意力\n",
        "    # 4.2 EncoderLayer\n",
        "    # 4.3 DecoderLayer\n",
        "    # 4.4 EncoderModel\n",
        "    # 4.5 DecoderModel\n",
        "    # 4.6 Transformer\n",
        "# 5. optimizer & loss\n",
        "# 6. train step -> train\n",
        "# 7. Evaluate and Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QDT2wZifvUA"
      },
      "source": [
        "# 1 loads data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FTxCHL4fvUC",
        "outputId": "aaef9785-04dc-46bf-d421-5c72c86fb868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "tfds.core.DatasetInfo(\n",
            "    name='ted_hrlr_translate',\n",
            "    version=1.0.0,\n",
            "    description='Data sets derived from TED talk transcripts for comparing similar language pairs\n",
            "where one is high resource and the other is low resource.',\n",
            "    homepage='https://github.com/neulab/word-embeddings-for-nmt',\n",
            "    features=Translation({\n",
            "        'en': Text(shape=(), dtype=tf.string),\n",
            "        'pt': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    total_num_examples=54781,\n",
            "    splits={\n",
            "        'test': 1803,\n",
            "        'train': 51785,\n",
            "        'validation': 1193,\n",
            "    },\n",
            "    supervised_keys=('pt', 'en'),\n",
            "    citation=\"\"\"@inproceedings{Ye2018WordEmbeddings,\n",
            "      author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n",
            "      title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n",
            "      booktitle = {HLT-NAACL},\n",
            "      year    = {2018},\n",
            "      }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "# 葡萄牙语->英语，这个是基于subword的\n",
        "# as_supervised是True是tuple，是False返回的是字典\n",
        "examples, info = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                           with_info = True,\n",
        "                           as_supervised = True)\n",
        "print(type(examples))  # examples是字典，里边有训练集，验证集，测试集\n",
        "train_examples, val_examples = examples['train'], examples['validation']\n",
        "print(info)  # info里是数据集的描述"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X8EDUt7fvUE",
        "outputId": "39429c54-31e2-4f85-bbc4-3a13201629eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'\n",
            "b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "b'mas e se estes fatores fossem ativos ?'\n",
            "b'but what if it were active ?'\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "b'mas eles n\\xc3\\xa3o tinham a curiosidade de me testar .'\n",
            "b\"but they did n't test for curiosity .\"\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "b'e esta rebeldia consciente \\xc3\\xa9 a raz\\xc3\\xa3o pela qual eu , como agn\\xc3\\xb3stica , posso ainda ter f\\xc3\\xa9 .'\n",
            "b'and this conscious defiance is why i , as an agnostic , can still have faith .'\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "b\"`` `` '' podem usar tudo sobre a mesa no meu corpo . ''\"\n",
            "b'you can use everything on the table on me .'\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "<PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n"
          ]
        }
      ],
      "source": [
        "# 葡萄牙语中有一些特除的字符，用转义字符来打印\n",
        "for pt, en in train_examples.take(5):\n",
        "    print(pt.numpy())\n",
        "    print(en.numpy())\n",
        "    print(type(pt))\n",
        "print(train_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpEik3mTfvUF",
        "outputId": "8d4a523e-cc3a-494a-9494-1d6ba42326dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8192"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "2**13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGw4K0MGfvUF"
      },
      "source": [
        "# 2 preprocesses data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "LCcueXekfvUG"
      },
      "outputs": [],
      "source": [
        "# time-consuming\n",
        "# 转为subword数据集，2**13=8192，build_from_corpus\n",
        "en_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples),\n",
        "    target_vocab_size = 2 ** 13)\n",
        "pt_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples),\n",
        "    target_vocab_size = 2 ** 13)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VPRdB5uzfvUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a625878c-17f8-489c-db30-c14eaf8bdac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
            "The original string is Transformer is awesome.\n",
            "7915 --> \"T\"-->1\n",
            "1248 --> \"ran\"-->3\n",
            "7946 --> \"s\"-->1\n",
            "7194 --> \"former \"-->7\n",
            "13 --> \"is \"-->3\n",
            "2799 --> \"awesome\"-->7\n",
            "7877 --> \".\"-->1\n"
          ]
        }
      ],
      "source": [
        "# 测试一个字符串,subword里面是包含空格的\n",
        "sample_string = \"Transformer is awesome.\"\n",
        "\n",
        "tokenized_string = en_tokenizer.encode(sample_string)\n",
        "print('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "origin_string = en_tokenizer.decode(tokenized_string)\n",
        "print('The original string is {}'.format(origin_string))\n",
        "\n",
        "assert origin_string == sample_string\n",
        "\n",
        "for token in tokenized_string:\n",
        "    print('{} --> \"{}\"-->{}'.format(token, en_tokenizer.decode([token]),len(en_tokenizer.decode([token]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "wFHbXTAqfvUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e7b571-e9a5-4fe0-81a9-9ec158049196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8214"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# 真正拆出来的词根是接近8192，刚好等于的概率很小\n",
        "pt_tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iDOK3xqhfvUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eaf8991-75dd-4491-a5ea-3445424ca4a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8087"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "en_tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Z5ChDTWnfvUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d9c4d0-b34d-4a41-f756-c8ead256db71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "The original string is �\n"
          ]
        }
      ],
      "source": [
        "origin_string = en_tokenizer.decode([8086])\n",
        "print(len(origin_string))\n",
        "print('The original string is {}'.format(origin_string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "AhV95TgdfvUI"
      },
      "outputs": [],
      "source": [
        "buffer_size = 20000\n",
        "batch_size = 64\n",
        "max_length = 40  # 输入和输出的最大长度是40\n",
        "\n",
        "# 把两段文本转为subword的id形式，\n",
        "def encode_to_subword(pt_sentence, en_sentence):\n",
        "    pt_sequence = [pt_tokenizer.vocab_size] \\\n",
        "    + pt_tokenizer.encode(pt_sentence.numpy()) \\\n",
        "    + [pt_tokenizer.vocab_size + 1]\n",
        "    en_sequence = [en_tokenizer.vocab_size] \\\n",
        "    + en_tokenizer.encode(en_sentence.numpy()) \\\n",
        "    + [en_tokenizer.vocab_size + 1]\n",
        "    return pt_sequence, en_sequence\n",
        "\n",
        "# 用tf的API消去大于最大长度的\n",
        "def filter_by_max_length(pt, en):\n",
        "    return tf.logical_and(tf.size(pt) <= max_length,\n",
        "                          tf.size(en) <= max_length)\n",
        "# 用py_function封装一下encode_to_subword\n",
        "def tf_encode_to_subword(pt_sentence, en_sentence):\n",
        "    return tf.py_function(encode_to_subword,\n",
        "                          [pt_sentence, en_sentence],\n",
        "                          [tf.int64, tf.int64])\n",
        "# 把所有句子变为subword，subword都变为id\n",
        "train_dataset = train_examples.map(tf_encode_to_subword)\n",
        "train_dataset = train_dataset.filter(filter_by_max_length)  # 去掉长度超过40的（subword的40）\n",
        "# next: 洗牌，padding，batch    [-1]，[-1]代表两个维度，每个维度都在当前维度下扩展到最高的值？\n",
        "train_dataset = train_dataset.shuffle(\n",
        "    buffer_size).padded_batch(\n",
        "    batch_size, padded_shapes=([-1], [-1]))  # 葡萄牙语，英语都补到40，有两个[-1],不够补0\n",
        "\n",
        "valid_dataset = val_examples.map(tf_encode_to_subword)\n",
        "valid_dataset = valid_dataset.filter(\n",
        "    filter_by_max_length).padded_batch(\n",
        "    batch_size, padded_shapes=([-1], [-1]))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "0KSQRRrVfvUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15143a09-a1bf-4c19-ece1-50e1af8e2298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 39) (64, 38)\n",
            "(64, 39) (64, 39)\n",
            "(64, 40) (64, 38)\n",
            "(64, 36) (64, 37)\n",
            "(64, 37) (64, 37)\n"
          ]
        }
      ],
      "source": [
        "# 不同batch不一致，是因为先batch，后padding的\n",
        "for pt_batch, en_batch in train_dataset.take(5):\n",
        "    print(pt_batch.shape, en_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "r3fhyZoKfvUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39b4d25-8439-4886-cf44-127154c24b94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0],\n",
              "       [ 1],\n",
              "       [ 2],\n",
              "       [ 3],\n",
              "       [ 4],\n",
              "       [ 5],\n",
              "       [ 6],\n",
              "       [ 7],\n",
              "       [ 8],\n",
              "       [ 9],\n",
              "       [10],\n",
              "       [11],\n",
              "       [12]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "np.arange(13)[:, np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "BviRv894fvUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7333021c-7a82-48bc-d5fd-06e226a01772"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
              "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
              "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
              "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
              "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
              "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
              "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
              "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
              "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
              "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
              "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
              "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
              "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
              "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
              "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
              "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
              "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
              "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
              "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
              "        247, 248, 249, 250, 251, 252, 253, 254, 255]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "np.arange(256)[np.newaxis, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TAPKWOdfvUJ"
      },
      "source": [
        "# 3 tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LZSdeWufvUJ"
      },
      "source": [
        "# 3.1 generates position embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "rbSpfbNpfvUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bc1b9b-483e-4913-f038-3dd3a67ecfa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "angle_rads.shape:(50, 512)\n",
            "(50, 256)\n",
            "(50, 256)\n",
            "tf.Tensor(\n",
            "[[[ 0.          0.          0.         ...  1.          1.\n",
            "    1.        ]\n",
            "  [ 0.84147096  0.8218562   0.8019618  ...  1.          1.\n",
            "    1.        ]\n",
            "  [ 0.9092974   0.9364147   0.95814437 ...  1.          1.\n",
            "    1.        ]\n",
            "  ...\n",
            "  [ 0.12357312  0.97718984 -0.24295525 ...  0.9999863   0.99998724\n",
            "    0.99998814]\n",
            "  [-0.76825464  0.7312359   0.63279754 ...  0.9999857   0.9999867\n",
            "    0.9999876 ]\n",
            "  [-0.95375264 -0.14402692  0.99899054 ...  0.9999851   0.9999861\n",
            "    0.9999871 ]]], shape=(1, 50, 512), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# 第三步，写一些工具函数\n",
        "# PE(pos, 2i)   = sin(pos/10000^(2i/d_model))\n",
        "# PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "\n",
        "# pos和i都是矩阵\n",
        "# pos.shape: [sentence_length, 1]\n",
        "# i.shape  : [1, d_model]\n",
        "# result.shape: [sentence_length, d_model]\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000,\n",
        "                               (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "# 计算位置信息\n",
        "def get_position_embedding(sentence_length, d_model):\n",
        "    # sentence_length和d_model都扩展为矩阵\n",
        "#     print(np.arange(sentence_length)[:, np.newaxis])\n",
        "#     print(np.arange(d_model)[np.newaxis, :])\n",
        "    # pos是0到49，就是词的位置，i是从0到511，总计512，和dim相等\n",
        "    angle_rads = get_angles(np.arange(sentence_length)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "    print('angle_rads.shape:',end='')\n",
        "    print(angle_rads.shape)\n",
        "    # sines.shape: [sentence_length, d_model / 2]\n",
        "    # cosines.shape: [sentence_length, d_model / 2]\n",
        "    # 正弦一半，余弦一半\n",
        "    print(angle_rads[:, 0::2].shape)\n",
        "    print(angle_rads[:, 1::2].shape)\n",
        "    sines = np.sin(angle_rads[:, 0::2])\n",
        "    cosines = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    # 把sines和cosines进行拼接\n",
        "    # position_embedding.shape: [sentence_length, d_model]\n",
        "    position_embedding = np.concatenate([sines, cosines], axis = -1)\n",
        "    # 进行维度扩展\n",
        "    # position_embedding.shape: [1, sentence_length, d_model]\n",
        "    position_embedding = position_embedding[np.newaxis, ...]\n",
        "    # 变为float32类型\n",
        "    return tf.cast(position_embedding, dtype=tf.float32)\n",
        "\n",
        "position_embedding = get_position_embedding(50, 512)\n",
        "print(position_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Dlc8VImQfvUK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "2389029e-4fe9-44ce-9e8f-84ffcef06112"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 绘制位置编码示意图\n",
        "def plot_position_embedding(position_embedding):\n",
        "    plt.pcolormesh(position_embedding[0], cmap = 'RdBu')\n",
        "    plt.xlabel('Depth')\n",
        "    plt.xlim((0, 512))\n",
        "    plt.ylabel('Position')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "    \n",
        "plot_position_embedding(position_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCfNPXJgfvUK"
      },
      "source": [
        "# 3.2 create mask(padding & decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "WWkx3_1dfvUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee68207-d5bd-4b32-8e4c-3cee1a78a6ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "# 如何生成mask\n",
        "# 1. padding mask 2. look ahead\n",
        "\n",
        "# batch_data.shape: [batch_size, seq_len]\n",
        "def create_padding_mask(batch_data):\n",
        "    padding_mask = tf.cast(tf.math.equal(batch_data, 0), tf.float32)\n",
        "    # [batch_size, 1, 1, seq_len]\n",
        "    return padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "# 设置3x5矩阵，0都是padding，是零的得到的都是1，非零的位置都是零\n",
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FnnhWWHIfvUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6adc9e-3073-4916-a104-85feaedfaff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# attention_weights.shape: [3,3]\n",
        "# 第一个位置代表第一个单词和自己的attention，第二位置是第二个单词和第一个单词的attention\n",
        "# 看不到后面的词刚好是下三角，使用库函数来实现\n",
        "# [[1, 0, 0],\n",
        "#  [4, 5, 0],\n",
        "#  [7, 8, 9]]\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "# 前面看不到后面的padding，矩阵下面全部为0\n",
        "# 在mask里，应该被忽略的我们会设成1，应该被保留的会设成0，\n",
        "# 而如果mask相应位置上为1，那么我们就给对应的logits \n",
        "# 加上一个超级小的负数， (mask * -1e9)， 这样，\n",
        "# 对应的logits也就变成了一个超级小的数。然后在计算softmax的时候，\n",
        "# 一个超级小的数的指数会无限接近与0，也就是它对应的attention的权重就是0了\n",
        "create_look_ahead_mask(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72B_81vZfvUL"
      },
      "source": [
        "# 3.3 scaled_dot_product_attention 缩放点积注意力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5ovXx3dYfvUL"
      },
      "outputs": [],
      "source": [
        "# 参考文档，q是query，k，v代表key和value，q和k做完矩阵乘法后，做mask\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - q: shape == (..., seq_len_q, depth)\n",
        "    - k: shape == (..., seq_len_k, depth)\n",
        "    - v: shape == (..., seq_len_v, depth_v)\n",
        "    - seq_len_k == seq_len_v  这两个是相等的\n",
        "    - mask: shape == (..., seq_len_q, seq_len_k)\n",
        "    Returns:\n",
        "    - output: weighted sum\n",
        "    - attention_weights: weights of attention\n",
        "    \"\"\"\n",
        "    # 计算attentions时，只用了后两维在计算\n",
        "    # transpose_b代表第二个矩阵（即k）是否做转置，且为True时仅是把k的最后两维交换位置\n",
        "    # matmul_qk.shape: (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b = True)\n",
        "    \n",
        "    # 获得dk，cast转换类型\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    # 然后根据文档中的公式除以dk\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    \n",
        "    # 这一步是mask应用的核心啊\n",
        "    # 如果mask不是空的话，给scaled_attention_logits加一个mask（缩放）\n",
        "    if mask is not None:\n",
        "        # 使得在softmax后值趋近于0\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    \n",
        "    # attention_weights.shape: (..., seq_len_q, seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis = -1)\n",
        "    \n",
        "    # 根据原理图，v和attention_weights进行矩阵乘法\n",
        "    # output.shape: (..., seq_len_q, depth_v)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# 调用上面的函数，去验证\n",
        "def print_scaled_dot_product_attention(q, k, v):\n",
        "    temp_out, temp_att = scaled_dot_product_attention(q, k, v, None)\n",
        "    print(\"Attention weights are:\")\n",
        "    print(temp_att)\n",
        "    print(\"Output is:\")\n",
        "    print(temp_out)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Y3JcwSeufvUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aa14f7-cc8d-4c29-b14d-6323c05db391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# 定义一个测试的Q，K，V\n",
        "temp_k = tf.constant([[10, 0, 0],\n",
        "                      [0, 10, 0],\n",
        "                      [0, 0, 10],\n",
        "                      [0, 0, 10]], dtype=tf.float32) # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[1, 0],\n",
        "                      [10, 0],\n",
        "                      [100, 5],\n",
        "                      [1000, 6]], dtype=tf.float32) # (4, 2)\n",
        "\n",
        "temp_q1 = tf.constant([[0, 10, 0]], dtype=tf.float32) # (1, 3)\n",
        "# 做四舍五入，让结果清爽\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "print_scaled_dot_product_attention(temp_q1, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "-UoUqgZ3fvUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ce1dec-fe89-43a7-b8dc-acb29034ff20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "temp_q2 = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
        "# 0.  0.  0.5 0.5 会和temp_v去做平均，因此得到的是550,5.5\n",
        "print_scaled_dot_product_attention(temp_q2, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "kbP8V6vBfvUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50dd5e6c-cb4b-47d2-c219-1dce7dd1aa15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "temp_q3 = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_scaled_dot_product_attention(temp_q3, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "VMOtQjhpfvUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c590cc62-6c13-4b46-8a08-9cc9393bc019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor(\n",
            "[[0.  1.  0.  0. ]\n",
            " [0.  0.  0.5 0.5]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor(\n",
            "[[ 10.    0. ]\n",
            " [550.    5.5]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# 拼起来再来测试\n",
        "temp_q4 = tf.constant([[0, 10, 0],\n",
        "                       [0, 0, 10],\n",
        "                       [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
        "print_scaled_dot_product_attention(temp_q4, temp_k, temp_v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO6VAW1BfvUM"
      },
      "source": [
        "# 4 builds model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9deGBWZffvUM"
      },
      "source": [
        "# 4.1 multihead attention 多头注意力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "FyG267HAfvUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a328a537-232c-445a-9f75-3536ef119273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8, 60, 60)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 8, 60, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "q=np.ones((1,8,60,64))\n",
        "k=np.ones((1,8,60,64))\n",
        "v=np.ones((1,8,60,64))\n",
        "d=tf.matmul(q, k, transpose_b = True)\n",
        "print(d.shape)\n",
        "tf.matmul(d,v).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Y9I3M5PzfvUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48336e4c-5d09-4593-b69e-be5bb27ca898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 60, 512)\n",
            "(1, 8, 60, 60)\n"
          ]
        }
      ],
      "source": [
        "# 多头注意力的实现->多个缩放点积注意力\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    理论上:\n",
        "    x -> Wq0 -> q0\n",
        "    x -> Wk0 -> k0\n",
        "    x -> Wv0 -> v0\n",
        "    \n",
        "    实战中:把三个概念区分开\n",
        "    q -> Wq0 -> q0\n",
        "    k -> Wk0 -> k0\n",
        "    v -> Wv0 -> v0\n",
        "    \n",
        "    实战中技巧：q乘以W得到一个大的Q，然后分割为多个小q\n",
        "    q -> Wq -> Q -> split -> q0, q1, q2...\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert self.d_model % self.num_heads == 0\n",
        "        \n",
        "        # 大Q变小q怎么变，层次\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "        \n",
        "        self.WQ = keras.layers.Dense(self.d_model)\n",
        "        self.WK = keras.layers.Dense(self.d_model)\n",
        "        self.WV = keras.layers.Dense(self.d_model)\n",
        "        # 拼接\n",
        "        self.dense = keras.layers.Dense(self.d_model)\n",
        "    \n",
        "    def split_heads(self, x, batch_size):\n",
        "        # x.shape: (batch_size, seq_len, d_model)\n",
        "        # d_model = num_heads * depth\n",
        "        # 把x变为下面维度，用reshape\n",
        "        # x -> (batch_size, num_heads, seq_len, depth)\n",
        "        \n",
        "        x = tf.reshape(x,\n",
        "                       (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # 轴滚动\n",
        "    \n",
        "    def call(self, q, k, v, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        # 经过Q K V变化\n",
        "        # print(q.shape)\n",
        "        q = self.WQ(q)  # q.shape: (batch_size, seq_len_q, d_model)\n",
        "        k = self.WK(k)  # k.shape: (batch_size, seq_len_k, d_model)\n",
        "        v = self.WV(v)  # v.shape: (batch_size, seq_len_v, d_model)\n",
        "        # print('-'*50)\n",
        "        # print(q.shape)\n",
        "        # q.shape: (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # print('分割后的q:',end='')\n",
        "        # print(q.shape)\n",
        "        # k.shape: (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # v.shape: (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        \n",
        "        # 开始做缩放点积，得到的多头的信息存在在num_heads，depth上\n",
        "        # scaled_attention_outputs.shape: (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention_outputs, attention_weights = \\\n",
        "        scaled_dot_product_attention(q, k, v, mask)\n",
        "        # print('-'*50)\n",
        "        # print('scaled_attention_outputs.shape:',end='')\n",
        "        # print(scaled_attention_outputs.shape)\n",
        "        # print('attention_weights.shape:',end='')\n",
        "        # print(attention_weights.shape)\n",
        "        # print('-'*50)\n",
        "        # 因此这里做一下转置，让num_heads，depth在后面\n",
        "        # scaled_attention_outputs.shape: (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention_outputs = tf.transpose(\n",
        "            scaled_attention_outputs, perm = [0, 2, 1, 3])\n",
        "        # print('attention_weights.shape:',end='')\n",
        "        # print(scaled_attention_outputs.shape)\n",
        "        # 对注意力进行合并\n",
        "        # concat_attention.shape: (batch_size, seq_len_q, d_model)\n",
        "        concat_attention = tf.reshape(scaled_attention_outputs,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "        \n",
        "        # 多头注意力计算完毕\n",
        "        # output.shape : (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "# 创建一份虚拟数据\n",
        "y = tf.random.uniform((1, 60, 256))  # (batch_size, seq_len_q, dim)\n",
        "# 开始计算，把y既当q，又当k，v\n",
        "output, attn = temp_mha(y, y, y, mask = None)\n",
        "print(output.shape)\n",
        "print(attn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "_Er_Gj_mfvUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c549e85a-2126-425a-f100-177d35975431"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 40, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# 定义feed_forward_network，d_model: 节点数\n",
        "def feed_forward_network(d_model, dff):\n",
        "    # dff: dim of feed forward network.\n",
        "    return keras.Sequential([\n",
        "        keras.layers.Dense(dff, activation='relu'),\n",
        "        keras.layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "sample_ffn = feed_forward_network(512, 2048)\n",
        "# 给一个输入测试\n",
        "sample_ffn(tf.random.uniform((64, 40, 512))).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIdAGWOUfvUN"
      },
      "source": [
        "# 4.2 EncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "D-N3T0zNfvUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92263ee-21c8-4c7f-fe68-c5f77852bbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 40, 512)\n"
          ]
        }
      ],
      "source": [
        "# 自定义EncoderLayer\n",
        "class EncoderLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    x -> self attention -> add & normalize & dropout\n",
        "      -> feed_forward -> add & normalize & dropout\n",
        "    原理对应文档Add & Normalize 标题下的图\n",
        "    \"\"\"\n",
        "    # d_model给self attention和feed_forward_network，num_heads给self_attention用的\n",
        "    # dff给feed_forward_network，rate是做dropout的\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = feed_forward_network(d_model, dff)\n",
        "        \n",
        "        # epsilon 将小浮点数添加到方差以避免被零除\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
        "            epsilon = 1e-6)\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
        "            epsilon = 1e-6)\n",
        "        # 下面两个层次用了做dropout，每次有10%的几率被drop掉\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, encoder_padding_mask):\n",
        "        # x.shape          : (batch_size, seq_len, dim=d_model)\n",
        "        # attn_output.shape: (batch_size, seq_len, d_model)\n",
        "        # out1.shape       : (batch_size, seq_len, d_model)\n",
        "        # x作为q，k，v  原理对应文档Add & Normalize 标题下的图\n",
        "        attn_output, _ = self.mha(x, x, x, encoder_padding_mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # dim=d_model 两个必须相等，这样x才可以和attn_output做加法\n",
        "        out1 = self.layer_norm1(x + attn_output)\n",
        "        \n",
        "        # ffn_output.shape: (batch_size, seq_len, d_model)\n",
        "        # out2.shape      : (batch_size, seq_len, d_model)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layer_norm2(out1 + ffn_output)\n",
        "        \n",
        "        return out2\n",
        "# 测试，结果与最初的输入维度一致，相当于做了两次残差连接\n",
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "sample_input = tf.random.uniform((64, 40, 512))\n",
        "sample_output = sample_encoder_layer(sample_input, False, None)\n",
        "print(sample_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "JfnCFM5vfvUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db942d5-5343-4d68-8cb5-5c21b2c63b0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_158/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[-0.07269425, -0.02172845,  0.03022368, ..., -0.0445837 ,\n",
              "          0.06158007,  0.01299228],\n",
              "        [-0.00279603,  0.03706963, -0.07378405, ..., -0.03569598,\n",
              "          0.00161891, -0.07084925],\n",
              "        [-0.0667477 ,  0.07194062,  0.07426592, ...,  0.00488634,\n",
              "          0.05725632, -0.06169483],\n",
              "        ...,\n",
              "        [-0.05803926,  0.00137741,  0.01308957, ..., -0.05543611,\n",
              "          0.07180598, -0.06267793],\n",
              "        [ 0.0001921 , -0.0366058 ,  0.05820429, ...,  0.07595343,\n",
              "          0.03771806, -0.0144515 ],\n",
              "        [-0.06846354, -0.02164631, -0.05761899, ...,  0.02729866,\n",
              "         -0.01320945,  0.07294603]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_158/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_159/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[ 0.0201879 ,  0.06344602, -0.01582499, ..., -0.00487355,\n",
              "         -0.00492374,  0.06225763],\n",
              "        [-0.0590548 , -0.01649961, -0.07453977, ..., -0.0527642 ,\n",
              "          0.02699094, -0.07181966],\n",
              "        [-0.04108644, -0.00594994,  0.01581291, ..., -0.01141541,\n",
              "         -0.04619668, -0.02172215],\n",
              "        ...,\n",
              "        [-0.01102205,  0.04389508,  0.0037666 , ...,  0.07084519,\n",
              "         -0.01115747,  0.06820818],\n",
              "        [ 0.00854078, -0.07018591, -0.02004239, ..., -0.00871903,\n",
              "          0.06895732,  0.06192341],\n",
              "        [ 0.03482267, -0.04708402,  0.02253098, ...,  0.02079725,\n",
              "         -0.05988589, -0.07041776]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_159/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_160/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[-0.06082574,  0.07484575,  0.0136437 , ..., -0.03090893,\n",
              "          0.05994163,  0.02791345],\n",
              "        [-0.04321917,  0.06651806,  0.00753201, ...,  0.06695301,\n",
              "          0.06402779, -0.03039596],\n",
              "        [ 0.01539203, -0.00203412,  0.06292459, ...,  0.01939806,\n",
              "         -0.01761737, -0.05803613],\n",
              "        ...,\n",
              "        [ 0.03562123,  0.02562504, -0.00194272, ..., -0.01675781,\n",
              "         -0.07199185, -0.06055377],\n",
              "        [ 0.07178862,  0.07615168,  0.02789351, ...,  0.04194776,\n",
              "         -0.07189455, -0.07121683],\n",
              "        [-0.07246567,  0.06502288,  0.00734764, ..., -0.02442472,\n",
              "          0.06744783,  0.00956029]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_160/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_161/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[ 0.02006785,  0.05041633,  0.04828281, ..., -0.01207567,\n",
              "          0.00080308, -0.03108454],\n",
              "        [ 0.04201536, -0.04672484,  0.04881694, ...,  0.0647236 ,\n",
              "          0.03192983, -0.0703726 ],\n",
              "        [ 0.05704623,  0.02366289, -0.01298551, ...,  0.01748515,\n",
              "          0.01215941,  0.05914899],\n",
              "        ...,\n",
              "        [-0.06975941,  0.03810326, -0.05807751, ..., -0.03991141,\n",
              "         -0.07604643, -0.01091345],\n",
              "        [-0.04988339, -0.06884679,  0.02550062, ..., -0.05168732,\n",
              "         -0.02771929, -0.00521258],\n",
              "        [ 0.01194339, -0.00025807,  0.01305895, ..., -0.0381519 ,\n",
              "          0.0037898 , -0.00667792]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_161/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'dense_162/kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
              " array([[-0.01789801,  0.04340407, -0.0016043 , ..., -0.01067227,\n",
              "         -0.015831  ,  0.00766672],\n",
              "        [ 0.00014246, -0.04205634,  0.0405287 , ..., -0.02400882,\n",
              "          0.00556695, -0.04830577],\n",
              "        [ 0.04268505,  0.00999287, -0.02168976, ..., -0.03613469,\n",
              "          0.02336003, -0.02951704],\n",
              "        ...,\n",
              "        [-0.04651175,  0.01274027,  0.0357533 , ..., -0.03287341,\n",
              "         -0.04666309, -0.0203438 ],\n",
              "        [ 0.04451045, -0.01938249,  0.02511489, ...,  0.02132619,\n",
              "          0.01621532, -0.02305104],\n",
              "        [-0.00364764, -0.04774076, -0.02863106, ...,  0.03415134,\n",
              "         -0.00445813,  0.03637944]], dtype=float32)>,\n",
              " <tf.Variable 'dense_162/bias:0' shape=(2048,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'dense_163/kernel:0' shape=(2048, 512) dtype=float32, numpy=\n",
              " array([[ 0.01824208, -0.01335493,  0.03415506, ...,  0.02513094,\n",
              "         -0.00888943,  0.02645457],\n",
              "        [ 0.02176755,  0.01461439,  0.01117377, ...,  0.00760575,\n",
              "         -0.01991234,  0.03400126],\n",
              "        [-0.03338877,  0.0313627 ,  0.03176575, ..., -0.01436858,\n",
              "          0.01150408, -0.01164312],\n",
              "        ...,\n",
              "        [-0.03137033,  0.01665088, -0.04689709, ..., -0.03730356,\n",
              "         -0.03100744,  0.03538155],\n",
              "        [ 0.0210885 ,  0.0318537 , -0.0269567 , ..., -0.02763181,\n",
              "          0.03848402,  0.00585161],\n",
              "        [-0.02156442, -0.03160896,  0.04646473, ...,  0.03667862,\n",
              "         -0.02916512,  0.00043957]], dtype=float32)>,\n",
              " <tf.Variable 'dense_163/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_45/gamma:0' shape=(512,) dtype=float32, numpy=\n",
              " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_45/beta:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_46/gamma:0' shape=(512,) dtype=float32, numpy=\n",
              " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_46/beta:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "sample_encoder_layer.variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "4Y3hPOwCfvUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74a31e5-12b9-4b81-b824-86c665fbdf4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_158/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[-0.07269425, -0.02172845,  0.03022368, ..., -0.0445837 ,\n",
              "          0.06158007,  0.01299228],\n",
              "        [-0.00279603,  0.03706963, -0.07378405, ..., -0.03569598,\n",
              "          0.00161891, -0.07084925],\n",
              "        [-0.0667477 ,  0.07194062,  0.07426592, ...,  0.00488634,\n",
              "          0.05725632, -0.06169483],\n",
              "        ...,\n",
              "        [-0.05803926,  0.00137741,  0.01308957, ..., -0.05543611,\n",
              "          0.07180598, -0.06267793],\n",
              "        [ 0.0001921 , -0.0366058 ,  0.05820429, ...,  0.07595343,\n",
              "          0.03771806, -0.0144515 ],\n",
              "        [-0.06846354, -0.02164631, -0.05761899, ...,  0.02729866,\n",
              "         -0.01320945,  0.07294603]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_158/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_159/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[ 0.0201879 ,  0.06344602, -0.01582499, ..., -0.00487355,\n",
              "         -0.00492374,  0.06225763],\n",
              "        [-0.0590548 , -0.01649961, -0.07453977, ..., -0.0527642 ,\n",
              "          0.02699094, -0.07181966],\n",
              "        [-0.04108644, -0.00594994,  0.01581291, ..., -0.01141541,\n",
              "         -0.04619668, -0.02172215],\n",
              "        ...,\n",
              "        [-0.01102205,  0.04389508,  0.0037666 , ...,  0.07084519,\n",
              "         -0.01115747,  0.06820818],\n",
              "        [ 0.00854078, -0.07018591, -0.02004239, ..., -0.00871903,\n",
              "          0.06895732,  0.06192341],\n",
              "        [ 0.03482267, -0.04708402,  0.02253098, ...,  0.02079725,\n",
              "         -0.05988589, -0.07041776]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_159/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_160/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[-0.06082574,  0.07484575,  0.0136437 , ..., -0.03090893,\n",
              "          0.05994163,  0.02791345],\n",
              "        [-0.04321917,  0.06651806,  0.00753201, ...,  0.06695301,\n",
              "          0.06402779, -0.03039596],\n",
              "        [ 0.01539203, -0.00203412,  0.06292459, ...,  0.01939806,\n",
              "         -0.01761737, -0.05803613],\n",
              "        ...,\n",
              "        [ 0.03562123,  0.02562504, -0.00194272, ..., -0.01675781,\n",
              "         -0.07199185, -0.06055377],\n",
              "        [ 0.07178862,  0.07615168,  0.02789351, ...,  0.04194776,\n",
              "         -0.07189455, -0.07121683],\n",
              "        [-0.07246567,  0.06502288,  0.00734764, ..., -0.02442472,\n",
              "          0.06744783,  0.00956029]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_160/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_161/kernel:0' shape=(512, 512) dtype=float32, numpy=\n",
              " array([[ 0.02006785,  0.05041633,  0.04828281, ..., -0.01207567,\n",
              "          0.00080308, -0.03108454],\n",
              "        [ 0.04201536, -0.04672484,  0.04881694, ...,  0.0647236 ,\n",
              "          0.03192983, -0.0703726 ],\n",
              "        [ 0.05704623,  0.02366289, -0.01298551, ...,  0.01748515,\n",
              "          0.01215941,  0.05914899],\n",
              "        ...,\n",
              "        [-0.06975941,  0.03810326, -0.05807751, ..., -0.03991141,\n",
              "         -0.07604643, -0.01091345],\n",
              "        [-0.04988339, -0.06884679,  0.02550062, ..., -0.05168732,\n",
              "         -0.02771929, -0.00521258],\n",
              "        [ 0.01194339, -0.00025807,  0.01305895, ..., -0.0381519 ,\n",
              "          0.0037898 , -0.00667792]], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/multi_head_attention_29/dense_161/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'dense_162/kernel:0' shape=(512, 2048) dtype=float32, numpy=\n",
              " array([[-0.01789801,  0.04340407, -0.0016043 , ..., -0.01067227,\n",
              "         -0.015831  ,  0.00766672],\n",
              "        [ 0.00014246, -0.04205634,  0.0405287 , ..., -0.02400882,\n",
              "          0.00556695, -0.04830577],\n",
              "        [ 0.04268505,  0.00999287, -0.02168976, ..., -0.03613469,\n",
              "          0.02336003, -0.02951704],\n",
              "        ...,\n",
              "        [-0.04651175,  0.01274027,  0.0357533 , ..., -0.03287341,\n",
              "         -0.04666309, -0.0203438 ],\n",
              "        [ 0.04451045, -0.01938249,  0.02511489, ...,  0.02132619,\n",
              "          0.01621532, -0.02305104],\n",
              "        [-0.00364764, -0.04774076, -0.02863106, ...,  0.03415134,\n",
              "         -0.00445813,  0.03637944]], dtype=float32)>,\n",
              " <tf.Variable 'dense_162/bias:0' shape=(2048,) dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'dense_163/kernel:0' shape=(2048, 512) dtype=float32, numpy=\n",
              " array([[ 0.01824208, -0.01335493,  0.03415506, ...,  0.02513094,\n",
              "         -0.00888943,  0.02645457],\n",
              "        [ 0.02176755,  0.01461439,  0.01117377, ...,  0.00760575,\n",
              "         -0.01991234,  0.03400126],\n",
              "        [-0.03338877,  0.0313627 ,  0.03176575, ..., -0.01436858,\n",
              "          0.01150408, -0.01164312],\n",
              "        ...,\n",
              "        [-0.03137033,  0.01665088, -0.04689709, ..., -0.03730356,\n",
              "         -0.03100744,  0.03538155],\n",
              "        [ 0.0210885 ,  0.0318537 , -0.0269567 , ..., -0.02763181,\n",
              "          0.03848402,  0.00585161],\n",
              "        [-0.02156442, -0.03160896,  0.04646473, ...,  0.03667862,\n",
              "         -0.02916512,  0.00043957]], dtype=float32)>,\n",
              " <tf.Variable 'dense_163/bias:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_45/gamma:0' shape=(512,) dtype=float32, numpy=\n",
              " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_45/beta:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_46/gamma:0' shape=(512,) dtype=float32, numpy=\n",
              " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "        1., 1.], dtype=float32)>,\n",
              " <tf.Variable 'encoder_layer_9/layer_normalization_46/beta:0' shape=(512,) dtype=float32, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "sample_encoder_layer.trainable_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4QzS1PvfvUO"
      },
      "source": [
        "# 4.3 DecoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "0guiZm2EfvUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db38a32-237c-48f8-dd87-e0a8067a17e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 60, 512)\n",
            "(64, 8, 60, 60)\n",
            "(64, 8, 60, 40)\n"
          ]
        }
      ],
      "source": [
        "class DecoderLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    x -> self attention -> add & normalize & dropout -> out1\n",
        "    out1 , encoding_outputs -> attention -> add & normalize & dropout -> out2\n",
        "    out2 -> ffn -> add & normalize & dropout -> out3\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        \n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        self.ffn = feed_forward_network(d_model, dff)\n",
        "        # 因为有两个attention，还有一个feed_forward_network，所以有3个\n",
        "        # LayerNormalization和3个dropout\n",
        "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
        "            epsilon = 1e-6)\n",
        "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
        "            epsilon = 1e-6)\n",
        "        self.layer_norm3 = keras.layers.LayerNormalization(\n",
        "            epsilon = 1e-6)\n",
        "        \n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "        self.dropout3 = keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, encoding_outputs, training,\n",
        "             decoder_mask, encoder_decoder_padding_mask):\n",
        "        # decoder_mask: 由look_ahead_mask和decoder_padding_mask合并而来\n",
        "        \n",
        "        # x.shape: (batch_size, target_seq_len, d_model)\n",
        "        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        # 按照上面类的注释的步骤依次来编写call实现\n",
        "        # attn1, out1.shape : (batch_size, target_seq_len, d_model)\n",
        "        attn1, attn_weights1 = self.mha1(x, x, x, decoder_mask)\n",
        "        attn1 = self.dropout1(attn1, training = training)\n",
        "        out1 = self.layer_norm1(attn1 + x)\n",
        "        \n",
        "        # attn2, out2.shape : (batch_size, target_seq_len, d_model)\n",
        "        attn2, attn_weights2 = self.mha2(\n",
        "            out1, encoding_outputs, encoding_outputs,\n",
        "            encoder_decoder_padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training = training)\n",
        "        out2 = self.layer_norm2(attn2 + out1)\n",
        "        \n",
        "        # ffn_output, out3.shape: (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layer_norm3(ffn_output + out2)\n",
        "        \n",
        "        return out3, attn_weights1, attn_weights2\n",
        "\n",
        "# 测试\n",
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "sample_decoder_input = tf.random.uniform((64, 60, 512))\n",
        "sample_decoder_output, sample_decoder_attn_weights1, sample_decoder_attn_weights2 = sample_decoder_layer(\n",
        "    sample_decoder_input, sample_output, False, None, None)\n",
        "\n",
        "print(sample_decoder_output.shape)\n",
        "print(sample_decoder_attn_weights1.shape)  # 最后一维60是和x的维度一致的\n",
        "print(sample_decoder_attn_weights2.shape)  # 最后一维60是和x的维度相关的\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVAgiPkSfvUP"
      },
      "source": [
        "# 4.4 EncoderModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "w2QvNqR7fvUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc6625e-9b1a-4670-cb23-46b67f8a581c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "angle_rads.shape:(40, 512)\n",
            "(40, 256)\n",
            "(40, 256)\n",
            "(64, 37, 512)\n"
          ]
        }
      ],
      "source": [
        "# EncoderLayer堆叠->EncoderModel\n",
        "class EncoderModel(keras.layers.Layer):\n",
        "    def __init__(self, num_layers, input_vocab_size, max_length,\n",
        "                 d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderModel, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        # 这是layers数目\n",
        "        self.num_layers = num_layers\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # 构建embedding层\n",
        "        self.embedding = keras.layers.Embedding(input_vocab_size,\n",
        "                                                self.d_model)\n",
        "        # position_embedding.shape: (1, max_length, d_model)\n",
        "        self.position_embedding = get_position_embedding(max_length,\n",
        "                                                         self.d_model)\n",
        "        \n",
        "        self.dropout = keras.layers.Dropout(rate)\n",
        "        self.encoder_layers = [\n",
        "            EncoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(self.num_layers)]\n",
        "        \n",
        "    \n",
        "    def call(self, x, training, encoder_padding_mask):\n",
        "        # x.shape: (batch_size, input_seq_len)\n",
        "        input_seq_len = tf.shape(x)[1]  # 拿到输入的长度是40\n",
        "        tf.debugging.assert_less_equal(\n",
        "            input_seq_len, self.max_length,\n",
        "            \"input_seq_len should be less or equal to self.max_length\")\n",
        "        \n",
        "        # x.shape: (batch_size, input_seq_len, d_model)\n",
        "        x = self.embedding(x)\n",
        "        # x做缩放，是值在0到d_model之间\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # 因为x长度比position_embedding可能要小，因此embedding切片后和x相加\n",
        "        x += self.position_embedding[:, :input_seq_len, :]\n",
        "        \n",
        "        x = self.dropout(x, training = training)\n",
        "        \n",
        "        # 得到的x不断作为下一层的输入\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoder_layers[i](x, training,\n",
        "                                       encoder_padding_mask)\n",
        "        # x最终shape如下\n",
        "        # x.shape: (batch_size, input_seq_len, d_model)\n",
        "        return x\n",
        "\n",
        "# 测试\n",
        "sample_encoder_model = EncoderModel(2, 8500, max_length,\n",
        "                                    512, 8, 2048)\n",
        "sample_encoder_model_input = tf.random.uniform((64, 37))\n",
        "sample_encoder_model_output = sample_encoder_model(\n",
        "    sample_encoder_model_input, False, encoder_padding_mask = None)\n",
        "print(sample_encoder_model_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GR_ajycfvUP"
      },
      "source": [
        "# 4.5 DecoderModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "scrolled": true,
        "id": "oCrzoi5lfvUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d0dc24-ad3d-48ce-d1bc-801547b8ad0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "angle_rads.shape:(40, 512)\n",
            "(40, 256)\n",
            "(40, 256)\n",
            "(64, 35, 512)\n",
            "(64, 8, 35, 35)\n",
            "(64, 8, 35, 37)\n",
            "(64, 8, 35, 35)\n",
            "(64, 8, 35, 37)\n"
          ]
        }
      ],
      "source": [
        "# 和EncoderModel类似\n",
        "class DecoderModel(keras.layers.Layer):\n",
        "    def __init__(self, num_layers, target_vocab_size, max_length,\n",
        "                 d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderModel, self).__init__()\n",
        "        self.num_layers = num_layers  # decoderlayer的数量\n",
        "        self.max_length = max_length\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = keras.layers.Embedding(target_vocab_size,\n",
        "                                                d_model)\n",
        "        self.position_embedding = get_position_embedding(max_length,\n",
        "                                                         d_model)\n",
        "        \n",
        "        self.dropout = keras.layers.Dropout(rate)\n",
        "        self.decoder_layers = [\n",
        "            DecoderLayer(d_model, num_heads, dff, rate)\n",
        "            for _ in range(self.num_layers)]\n",
        "        \n",
        "    \n",
        "    def call(self, x, encoding_outputs, training,\n",
        "             decoder_mask, encoder_decoder_padding_mask):\n",
        "        # x.shape: (batch_size, output_seq_len)\n",
        "        output_seq_len = tf.shape(x)[1]\n",
        "        tf.debugging.assert_less_equal(\n",
        "            output_seq_len, self.max_length,\n",
        "            \"output_seq_len should be less or equal to self.max_length\")\n",
        "        \n",
        "        # attention_weights都是由decoder layer返回，把它保存下来\n",
        "        attention_weights = {}\n",
        "        \n",
        "        # x.shape: (batch_size, output_seq_len, d_model)\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # 做缩放\n",
        "        x += self.position_embedding[:, :output_seq_len, :]\n",
        "        \n",
        "        x = self.dropout(x, training = training)\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            # attn1, attn2分别是两个attention\n",
        "            x, attn1, attn2 = self.decoder_layers[i](\n",
        "                x, encoding_outputs, training,\n",
        "                decoder_mask, encoder_decoder_padding_mask)\n",
        "            attention_weights[\n",
        "                'decoder_layer{}_att1'.format(i+1)] = attn1\n",
        "            attention_weights[\n",
        "                'decoder_layer{}_att2'.format(i+1)] = attn2\n",
        "        # x.shape: (batch_size, output_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "sample_decoder_model = DecoderModel(2, 8000, max_length,\n",
        "                                    512, 8, 2048)\n",
        "# 测试\n",
        "sample_decoder_model_input = tf.random.uniform((64, 35))\n",
        "sample_decoder_model_output, sample_decoder_model_att \\\n",
        "= sample_decoder_model(\n",
        "    sample_decoder_model_input,\n",
        "    sample_encoder_model_output,  # 这里是encoder的output\n",
        "    training = False, decoder_mask = None,\n",
        "    encoder_decoder_padding_mask = None)\n",
        "\n",
        "print(sample_decoder_model_output.shape)\n",
        "for key in sample_decoder_model_att:\n",
        "    print(sample_decoder_model_att[key].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcFfOfyTfvUQ"
      },
      "source": [
        "# 4.6 Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "pK6O1dpXfvUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e026e71-78ba-43f8-9957-27c1e9cff870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "angle_rads.shape:(40, 512)\n",
            "(40, 256)\n",
            "(40, 256)\n",
            "angle_rads.shape:(40, 512)\n",
            "(40, 256)\n",
            "(40, 256)\n",
            "(64, 31, 8000)\n",
            "--------------------------------------------------\n",
            "decoder_layer1_att1 (64, 8, 31, 31)\n",
            "decoder_layer1_att2 (64, 8, 31, 26)\n",
            "decoder_layer2_att1 (64, 8, 31, 31)\n",
            "decoder_layer2_att2 (64, 8, 31, 26)\n"
          ]
        }
      ],
      "source": [
        "# 以上均为这一步APIs的实现\n",
        "class Transformer(keras.Model):\n",
        "    def __init__(self, num_layers, input_vocab_size, target_vocab_size,\n",
        "                 max_length, d_model, num_heads, dff, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.encoder_model = EncoderModel(\n",
        "            num_layers, input_vocab_size, max_length,\n",
        "            d_model, num_heads, dff, rate)\n",
        "        \n",
        "        self.decoder_model = DecoderModel(\n",
        "            num_layers, target_vocab_size, max_length,\n",
        "            d_model, num_heads, dff, rate)\n",
        "        \n",
        "        self.final_layer = keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, encoder_padding_mask,\n",
        "             decoder_mask, encoder_decoder_padding_mask):\n",
        "        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n",
        "        encoding_outputs = self.encoder_model(\n",
        "            inp, training, encoder_padding_mask)\n",
        "        \n",
        "        # decoding_outputs.shape: (batch_size, output_seq_len, d_model)\n",
        "        decoding_outputs, attention_weights = self.decoder_model(\n",
        "            tar, encoding_outputs, training,\n",
        "            decoder_mask, encoder_decoder_padding_mask)\n",
        "        \n",
        "        # predictions.shape: (batch_size, output_seq_len, target_vocab_size)\n",
        "        predictions = self.final_layer(decoding_outputs)\n",
        "        \n",
        "        return predictions, attention_weights\n",
        "\n",
        "# 测试\n",
        "sample_transformer = Transformer(2, 8500, 8000, max_length,\n",
        "                                 512, 8, 2048, rate = 0.1)\n",
        "temp_input = tf.random.uniform((64, 26))\n",
        "temp_target = tf.random.uniform((64, 31))\n",
        "\n",
        "# 得到输出\n",
        "predictions, attention_weights = sample_transformer(\n",
        "    temp_input, temp_target, training = False,\n",
        "    encoder_padding_mask = None,\n",
        "    decoder_mask = None,\n",
        "    encoder_decoder_padding_mask = None)\n",
        "# 输出shape\n",
        "print(predictions.shape)\n",
        "print('-'*50)\n",
        "# attention_weights的shape打印\n",
        "for key in attention_weights:\n",
        "    print(key, attention_weights[key].shape)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "3PflPksOfvUQ"
      },
      "outputs": [],
      "source": [
        "# 1. initializes model.\n",
        "# 2. define loss, optimizer, learning_rate schedule\n",
        "# 3. train_step\n",
        "# 4. train process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "WZLFeWDwfvUQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea1b833-c0ec-4698-d7b1-12930827e492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "angle_rads.shape:(40, 128)\n",
            "(40, 64)\n",
            "(40, 64)\n",
            "angle_rads.shape:(40, 128)\n",
            "(40, 64)\n",
            "(40, 64)\n"
          ]
        }
      ],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "# 加2是因为最后两个位置是start和end\n",
        "input_vocab_size = pt_tokenizer.vocab_size + 2\n",
        "target_vocab_size = en_tokenizer.vocab_size + 2\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "transformer = Transformer(num_layers,\n",
        "                          input_vocab_size,\n",
        "                          target_vocab_size,\n",
        "                          max_length,\n",
        "                          d_model, num_heads, dff, dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl5FAaPPfvUQ"
      },
      "source": [
        "# 5 optimizer & loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "HHOnQU6DfvUQ"
      },
      "outputs": [],
      "source": [
        "# 学习率变化，是先增后减，因为前期可以快点，后期模型比较好，就要放慢\n",
        "# lrate = (d_model ** -0.5) * min(step_num ** (-0.5), step_num * warm_up_steps **(-1.5))\n",
        "# 自定义的学习率调整设计实现\n",
        "# 公式参考链接：https://tensorflow.google.cn/tutorials/text/transformer\n",
        "class CustomizedSchedule(\n",
        "    keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps = 4000):\n",
        "        super(CustomizedSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** (-1.5))\n",
        "        \n",
        "        arg3 = tf.math.rsqrt(self.d_model)\n",
        "        \n",
        "        return arg3 * tf.math.minimum(arg1, arg2)\n",
        "    \n",
        "learning_rate = CustomizedSchedule(d_model)\n",
        "optimizer = keras.optimizers.Adam(learning_rate,\n",
        "                                  beta_1 = 0.9,\n",
        "                                  beta_2 = 0.98,\n",
        "                                  epsilon = 1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "9aAlWqEufvUR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f1cf9e1e-7f2a-46a1-9b0e-3ed321d9bc2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train step')"
            ]
          },
          "metadata": {},
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "temp_learning_rate_schedule = CustomizedSchedule(d_model)\n",
        "# 下面是学习率的设计图\n",
        "plt.plot(\n",
        "    temp_learning_rate_schedule(\n",
        "        tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Leraning rate\")\n",
        "plt.xlabel(\"Train step\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "Xcr5eooDfvUR"
      },
      "outputs": [],
      "source": [
        "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits = True, reduction = 'none')\n",
        "\n",
        "# 为了不考虑padding部分的损失，需要mask\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "E7OQEFBgfvUR"
      },
      "outputs": [],
      "source": [
        "# 利用create_padding_mask和create_look_ahead_mask接口构造训练需要的3个mask\n",
        "def create_masks(inp, tar):\n",
        "    \"\"\"\n",
        "    Encoder:\n",
        "      - encoder_padding_mask (self attention of EncoderLayer)\n",
        "      对于encoder中padding值没作用，所以无需attention\n",
        "    Decoder:\n",
        "      - look_ahead_mask (self attention of DecoderLayer)\n",
        "      target位置上的词不能看到之后的词，因为之后的词没预测出来\n",
        "      - encoder_decoder_padding_mask (encoder-decoder attention of DecoderLayer)\n",
        "      decoder不应该到encoder的padding上去花费精力\n",
        "      - decoder_padding_mask (self attention of DecoderLayer)\n",
        "      decoder也有padding，所以mask掉\n",
        "    \"\"\"\n",
        "    encoder_padding_mask = create_padding_mask(inp)  # 给padding做了mask\n",
        "    encoder_decoder_padding_mask = create_padding_mask(inp)  # 给encoder_decoder做的padding\n",
        "    \n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])  # 不能看到后面的词\n",
        "    decoder_padding_mask = create_padding_mask(tar)  # tar的padding做了mask\n",
        "    \n",
        "    decoder_mask = tf.maximum(decoder_padding_mask,\n",
        "                              look_ahead_mask)  # 广播比较（应该与batch是没关系的？）\n",
        "    \n",
        "#     print(encoder_padding_mask.shape)\n",
        "#     print(encoder_decoder_padding_mask.shape)\n",
        "#     print(look_ahead_mask.shape)\n",
        "#     print(decoder_padding_mask.shape)\n",
        "#     print(decoder_mask.shape)\n",
        "\n",
        "    \n",
        "    return encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "RnjvjQV_fvUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf96489a-7c7d-4dde-db35-73bc7526a778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[-5.  0.  0.]]]\n",
            "\n",
            "\n",
            " [[[ 0.  8.  9.]]]], shape=(2, 1, 1, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.  9.  8.]\n",
            " [ 7.  8.  0.]\n",
            " [ 0.  8.  9.]], shape=(3, 3), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=\n",
              "array([[[[-3.,  9.,  8.],\n",
              "         [ 7.,  8.,  0.],\n",
              "         [ 0.,  8.,  9.]]],\n",
              "\n",
              "\n",
              "       [[[ 0.,  9.,  9.],\n",
              "         [ 7.,  8.,  9.],\n",
              "         [ 0.,  8.,  9.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "# maximum broadcast 有点意思\n",
        "import tensorflow as tf\n",
        "x = tf.constant([-5., 0., 0., 0., 8, 9])\n",
        "x1 = tf.reshape(x, (2, 1, 1, 3))\n",
        "y = tf.constant([-3., 9, 8, 7, 8, 0., 0., 8, 9])\n",
        "y1 = tf.reshape(y, (3, 3))\n",
        "# print(x1.shape)\n",
        "# print(y1.shape)\n",
        "print(x1)\n",
        "print(y1)\n",
        "tf.maximum(x1, y1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "IlD51NIbfvUR"
      },
      "outputs": [],
      "source": [
        "temp_inp, temp_tar = iter(train_dataset.take(1)).next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "cY9wW8z_fvUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283fca8e-1a0f-4972-c58b-afb9d51fedfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 39)\n",
            "(64, 40)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 1, 1, 39), dtype=float32, numpy=\n",
              " array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        ...,\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(64, 1, 40, 40), dtype=float32, numpy=\n",
              " array([[[[0., 1., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        ...,\n",
              " \n",
              " \n",
              "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 1., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 1., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.],\n",
              "          [0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(64, 1, 1, 39), dtype=float32, numpy=\n",
              " array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        ...,\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
              " \n",
              " \n",
              "        [[[0., 0., 0., ..., 1., 1., 1.]]]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "print(temp_inp.shape)\n",
        "print(temp_tar.shape)\n",
        "create_masks(temp_inp, temp_tar)\n",
        "# 样本大小是64，不足的补齐35，或者39\n",
        "# 最后是(64, 1, 39, 39)原因是既不关注前面的padding，也不关注后面的单词"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcRei1hUfvUS"
      },
      "source": [
        "# 6 train step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "fEK4wb2jfvUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1dc40d1-cf4c-40fe-bb0d-ffdccaeac013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 3.9801 Accuracy 0.0000\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7f618dd10ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7f618dd10ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function train_step at 0x7f618dd10ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function train_step at 0x7f618dd10ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 100 Loss 4.1645 Accuracy 0.0164\n",
            "Epoch 1 Batch 200 Loss 4.0480 Accuracy 0.0238\n",
            "Epoch 1 Batch 300 Loss 3.9153 Accuracy 0.0295\n",
            "Epoch 1 Batch 400 Loss 3.7444 Accuracy 0.0356\n",
            "Epoch 1 Batch 500 Loss 3.6010 Accuracy 0.0410\n",
            "Epoch 1 Batch 600 Loss 3.4870 Accuracy 0.0463\n",
            "Epoch 1 Batch 700 Loss 3.3767 Accuracy 0.0527\n",
            "Epoch 1 Loss 3.3750 Accuracy 0.0528\n",
            "Time take for 1 epoch: 374.00975346565247 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.6511 Accuracy 0.1016\n",
            "Epoch 2 Batch 100 Loss 2.5789 Accuracy 0.1031\n",
            "Epoch 2 Batch 200 Loss 2.5187 Accuracy 0.1082\n",
            "Epoch 2 Batch 300 Loss 2.4896 Accuracy 0.1130\n",
            "Epoch 2 Batch 400 Loss 2.4601 Accuracy 0.1166\n",
            "Epoch 2 Batch 500 Loss 2.4256 Accuracy 0.1197\n",
            "Epoch 2 Batch 600 Loss 2.3988 Accuracy 0.1225\n",
            "Epoch 2 Batch 700 Loss 2.3720 Accuracy 0.1248\n",
            "Epoch 2 Loss 2.3714 Accuracy 0.1248\n",
            "Time take for 1 epoch: 129.94147634506226 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.0604 Accuracy 0.1354\n",
            "Epoch 3 Batch 100 Loss 2.1629 Accuracy 0.1433\n",
            "Epoch 3 Batch 200 Loss 2.1614 Accuracy 0.1444\n",
            "Epoch 3 Batch 300 Loss 2.1522 Accuracy 0.1453\n",
            "Epoch 3 Batch 400 Loss 2.1473 Accuracy 0.1466\n",
            "Epoch 3 Batch 500 Loss 2.1354 Accuracy 0.1477\n",
            "Epoch 3 Batch 600 Loss 2.1228 Accuracy 0.1486\n",
            "Epoch 3 Batch 700 Loss 2.1065 Accuracy 0.1499\n",
            "Epoch 3 Loss 2.1069 Accuracy 0.1500\n",
            "Time take for 1 epoch: 95.61403751373291 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.5351 Accuracy 0.1778\n",
            "Epoch 4 Batch 100 Loss 1.9469 Accuracy 0.1641\n",
            "Epoch 4 Batch 200 Loss 1.9384 Accuracy 0.1644\n",
            "Epoch 4 Batch 300 Loss 1.9313 Accuracy 0.1667\n",
            "Epoch 4 Batch 400 Loss 1.9219 Accuracy 0.1687\n",
            "Epoch 4 Batch 500 Loss 1.9114 Accuracy 0.1709\n",
            "Epoch 4 Batch 600 Loss 1.8979 Accuracy 0.1732\n",
            "Epoch 4 Batch 700 Loss 1.8874 Accuracy 0.1755\n",
            "Epoch 4 Loss 1.8864 Accuracy 0.1755\n",
            "Time take for 1 epoch: 99.13467907905579 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.7275 Accuracy 0.1855\n",
            "Epoch 5 Batch 100 Loss 1.6950 Accuracy 0.1923\n",
            "Epoch 5 Batch 200 Loss 1.6948 Accuracy 0.1940\n",
            "Epoch 5 Batch 300 Loss 1.6868 Accuracy 0.1960\n",
            "Epoch 5 Batch 400 Loss 1.6861 Accuracy 0.1984\n",
            "Epoch 5 Batch 500 Loss 1.6760 Accuracy 0.1998\n",
            "Epoch 5 Batch 600 Loss 1.6678 Accuracy 0.2016\n",
            "Epoch 5 Batch 700 Loss 1.6602 Accuracy 0.2031\n",
            "Epoch 5 Loss 1.6603 Accuracy 0.2031\n",
            "Time take for 1 epoch: 95.08850049972534 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4199 Accuracy 0.2047\n",
            "Epoch 6 Batch 100 Loss 1.5011 Accuracy 0.2188\n",
            "Epoch 6 Batch 200 Loss 1.5050 Accuracy 0.2201\n",
            "Epoch 6 Batch 300 Loss 1.5004 Accuracy 0.2204\n",
            "Epoch 6 Batch 400 Loss 1.4938 Accuracy 0.2213\n",
            "Epoch 6 Batch 500 Loss 1.4863 Accuracy 0.2223\n",
            "Epoch 6 Batch 600 Loss 1.4810 Accuracy 0.2233\n",
            "Epoch 6 Batch 700 Loss 1.4764 Accuracy 0.2241\n",
            "Epoch 6 Loss 1.4761 Accuracy 0.2241\n",
            "Time take for 1 epoch: 99.14938259124756 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.4840 Accuracy 0.2799\n",
            "Epoch 7 Batch 100 Loss 1.3150 Accuracy 0.2401\n",
            "Epoch 7 Batch 200 Loss 1.3070 Accuracy 0.2389\n",
            "Epoch 7 Batch 300 Loss 1.3026 Accuracy 0.2412\n",
            "Epoch 7 Batch 400 Loss 1.3004 Accuracy 0.2424\n",
            "Epoch 7 Batch 500 Loss 1.2923 Accuracy 0.2430\n",
            "Epoch 7 Batch 600 Loss 1.2894 Accuracy 0.2443\n",
            "Epoch 7 Batch 700 Loss 1.2870 Accuracy 0.2453\n",
            "Epoch 7 Loss 1.2873 Accuracy 0.2453\n",
            "Time take for 1 epoch: 105.75423836708069 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.2515 Accuracy 0.2552\n",
            "Epoch 8 Batch 100 Loss 1.1460 Accuracy 0.2629\n",
            "Epoch 8 Batch 200 Loss 1.1504 Accuracy 0.2640\n",
            "Epoch 8 Batch 300 Loss 1.1453 Accuracy 0.2648\n",
            "Epoch 8 Batch 400 Loss 1.1392 Accuracy 0.2648\n",
            "Epoch 8 Batch 500 Loss 1.1357 Accuracy 0.2652\n",
            "Epoch 8 Batch 600 Loss 1.1326 Accuracy 0.2652\n",
            "Epoch 8 Batch 700 Loss 1.1334 Accuracy 0.2652\n",
            "Epoch 8 Loss 1.1338 Accuracy 0.2652\n",
            "Time take for 1 epoch: 99.28454327583313 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.1668 Accuracy 0.3086\n",
            "Epoch 9 Batch 100 Loss 1.0081 Accuracy 0.2754\n",
            "Epoch 9 Batch 200 Loss 1.0130 Accuracy 0.2776\n",
            "Epoch 9 Batch 300 Loss 1.0130 Accuracy 0.2778\n",
            "Epoch 9 Batch 400 Loss 1.0152 Accuracy 0.2779\n",
            "Epoch 9 Batch 500 Loss 1.0164 Accuracy 0.2783\n",
            "Epoch 9 Batch 600 Loss 1.0163 Accuracy 0.2785\n",
            "Epoch 9 Batch 700 Loss 1.0197 Accuracy 0.2786\n",
            "Epoch 9 Loss 1.0200 Accuracy 0.2787\n",
            "Time take for 1 epoch: 103.78962826728821 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0130 Accuracy 0.3212\n",
            "Epoch 10 Batch 100 Loss 0.9068 Accuracy 0.2904\n",
            "Epoch 10 Batch 200 Loss 0.9145 Accuracy 0.2905\n",
            "Epoch 10 Batch 300 Loss 0.9163 Accuracy 0.2895\n",
            "Epoch 10 Batch 400 Loss 0.9213 Accuracy 0.2897\n",
            "Epoch 10 Batch 500 Loss 0.9208 Accuracy 0.2888\n",
            "Epoch 10 Batch 600 Loss 0.9256 Accuracy 0.2891\n",
            "Epoch 10 Batch 700 Loss 0.9336 Accuracy 0.2892\n",
            "Epoch 10 Loss 0.9341 Accuracy 0.2892\n",
            "Time take for 1 epoch: 86.72996520996094 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.9016 Accuracy 0.3125\n",
            "Epoch 11 Batch 100 Loss 0.8376 Accuracy 0.3013\n",
            "Epoch 11 Batch 200 Loss 0.8574 Accuracy 0.3020\n",
            "Epoch 11 Batch 300 Loss 0.8566 Accuracy 0.3006\n",
            "Epoch 11 Batch 400 Loss 0.8584 Accuracy 0.3007\n",
            "Epoch 11 Batch 500 Loss 0.8628 Accuracy 0.3006\n",
            "Epoch 11 Batch 600 Loss 0.8624 Accuracy 0.2998\n",
            "Epoch 11 Batch 700 Loss 0.8697 Accuracy 0.2995\n",
            "Epoch 11 Loss 0.8701 Accuracy 0.2995\n",
            "Time take for 1 epoch: 90.67238593101501 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.7655 Accuracy 0.2874\n",
            "Epoch 12 Batch 100 Loss 0.7779 Accuracy 0.3102\n",
            "Epoch 12 Batch 200 Loss 0.7903 Accuracy 0.3090\n",
            "Epoch 12 Batch 300 Loss 0.7952 Accuracy 0.3079\n",
            "Epoch 12 Batch 400 Loss 0.7987 Accuracy 0.3072\n",
            "Epoch 12 Batch 500 Loss 0.8028 Accuracy 0.3075\n",
            "Epoch 12 Batch 600 Loss 0.8079 Accuracy 0.3068\n",
            "Epoch 12 Batch 700 Loss 0.8130 Accuracy 0.3061\n",
            "Epoch 12 Loss 0.8129 Accuracy 0.3060\n",
            "Time take for 1 epoch: 91.55628800392151 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.7766 Accuracy 0.3084\n",
            "Epoch 13 Batch 100 Loss 0.7354 Accuracy 0.3193\n",
            "Epoch 13 Batch 200 Loss 0.7399 Accuracy 0.3150\n",
            "Epoch 13 Batch 300 Loss 0.7439 Accuracy 0.3144\n",
            "Epoch 13 Batch 400 Loss 0.7472 Accuracy 0.3138\n",
            "Epoch 13 Batch 500 Loss 0.7519 Accuracy 0.3133\n",
            "Epoch 13 Batch 600 Loss 0.7577 Accuracy 0.3130\n",
            "Epoch 13 Batch 700 Loss 0.7637 Accuracy 0.3122\n",
            "Epoch 13 Loss 0.7638 Accuracy 0.3122\n",
            "Time take for 1 epoch: 91.48057150840759 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.6696 Accuracy 0.2981\n",
            "Epoch 14 Batch 100 Loss 0.6912 Accuracy 0.3228\n",
            "Epoch 14 Batch 200 Loss 0.6981 Accuracy 0.3230\n",
            "Epoch 14 Batch 300 Loss 0.6979 Accuracy 0.3206\n",
            "Epoch 14 Batch 400 Loss 0.7059 Accuracy 0.3209\n",
            "Epoch 14 Batch 500 Loss 0.7100 Accuracy 0.3199\n",
            "Epoch 14 Batch 600 Loss 0.7181 Accuracy 0.3187\n",
            "Epoch 14 Batch 700 Loss 0.7234 Accuracy 0.3182\n",
            "Epoch 14 Loss 0.7240 Accuracy 0.3183\n",
            "Time take for 1 epoch: 82.91158890724182 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.5968 Accuracy 0.2796\n",
            "Epoch 15 Batch 100 Loss 0.6490 Accuracy 0.3253\n",
            "Epoch 15 Batch 200 Loss 0.6569 Accuracy 0.3252\n",
            "Epoch 15 Batch 300 Loss 0.6686 Accuracy 0.3250\n",
            "Epoch 15 Batch 400 Loss 0.6744 Accuracy 0.3249\n",
            "Epoch 15 Batch 500 Loss 0.6799 Accuracy 0.3248\n",
            "Epoch 15 Batch 600 Loss 0.6836 Accuracy 0.3238\n",
            "Epoch 15 Batch 700 Loss 0.6886 Accuracy 0.3234\n",
            "Epoch 15 Loss 0.6884 Accuracy 0.3234\n",
            "Time take for 1 epoch: 90.95767259597778 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.5501 Accuracy 0.3072\n",
            "Epoch 16 Batch 100 Loss 0.6167 Accuracy 0.3327\n",
            "Epoch 16 Batch 200 Loss 0.6229 Accuracy 0.3317\n",
            "Epoch 16 Batch 300 Loss 0.6355 Accuracy 0.3312\n",
            "Epoch 16 Batch 400 Loss 0.6414 Accuracy 0.3303\n",
            "Epoch 16 Batch 500 Loss 0.6447 Accuracy 0.3296\n",
            "Epoch 16 Batch 600 Loss 0.6516 Accuracy 0.3290\n",
            "Epoch 16 Batch 700 Loss 0.6560 Accuracy 0.3284\n",
            "Epoch 16 Loss 0.6561 Accuracy 0.3284\n",
            "Time take for 1 epoch: 98.5626220703125 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.5983 Accuracy 0.3336\n",
            "Epoch 17 Batch 100 Loss 0.5840 Accuracy 0.3367\n",
            "Epoch 17 Batch 200 Loss 0.5987 Accuracy 0.3357\n",
            "Epoch 17 Batch 300 Loss 0.6060 Accuracy 0.3349\n",
            "Epoch 17 Batch 400 Loss 0.6130 Accuracy 0.3351\n",
            "Epoch 17 Batch 500 Loss 0.6180 Accuracy 0.3343\n",
            "Epoch 17 Batch 600 Loss 0.6221 Accuracy 0.3333\n",
            "Epoch 17 Batch 700 Loss 0.6285 Accuracy 0.3322\n",
            "Epoch 17 Loss 0.6284 Accuracy 0.3322\n",
            "Time take for 1 epoch: 95.63136506080627 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.6146 Accuracy 0.3520\n",
            "Epoch 18 Batch 100 Loss 0.5630 Accuracy 0.3441\n",
            "Epoch 18 Batch 200 Loss 0.5749 Accuracy 0.3403\n",
            "Epoch 18 Batch 300 Loss 0.5805 Accuracy 0.3392\n",
            "Epoch 18 Batch 400 Loss 0.5846 Accuracy 0.3382\n",
            "Epoch 18 Batch 500 Loss 0.5915 Accuracy 0.3376\n",
            "Epoch 18 Batch 600 Loss 0.5966 Accuracy 0.3365\n",
            "Epoch 18 Batch 700 Loss 0.6020 Accuracy 0.3360\n",
            "Epoch 18 Loss 0.6022 Accuracy 0.3361\n",
            "Time take for 1 epoch: 87.09704113006592 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.5231 Accuracy 0.3498\n",
            "Epoch 19 Batch 100 Loss 0.5447 Accuracy 0.3441\n",
            "Epoch 19 Batch 200 Loss 0.5501 Accuracy 0.3441\n",
            "Epoch 19 Batch 300 Loss 0.5578 Accuracy 0.3427\n",
            "Epoch 19 Batch 400 Loss 0.5636 Accuracy 0.3424\n",
            "Epoch 19 Batch 500 Loss 0.5705 Accuracy 0.3416\n",
            "Epoch 19 Batch 600 Loss 0.5756 Accuracy 0.3406\n",
            "Epoch 19 Batch 700 Loss 0.5811 Accuracy 0.3400\n",
            "Epoch 19 Loss 0.5812 Accuracy 0.3400\n",
            "Time take for 1 epoch: 87.54561018943787 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4549 Accuracy 0.3025\n",
            "Epoch 20 Batch 100 Loss 0.5171 Accuracy 0.3467\n",
            "Epoch 20 Batch 200 Loss 0.5237 Accuracy 0.3472\n",
            "Epoch 20 Batch 300 Loss 0.5337 Accuracy 0.3460\n",
            "Epoch 20 Batch 400 Loss 0.5408 Accuracy 0.3458\n",
            "Epoch 20 Batch 500 Loss 0.5491 Accuracy 0.3456\n",
            "Epoch 20 Batch 600 Loss 0.5548 Accuracy 0.3450\n",
            "Epoch 20 Batch 700 Loss 0.5598 Accuracy 0.3443\n",
            "Epoch 20 Loss 0.5599 Accuracy 0.3442\n",
            "Time take for 1 epoch: 96.13289904594421 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_loss = keras.metrics.Mean(name = 'train_loss')\n",
        "train_accuracy = keras.metrics.SparseCategoricalAccuracy(\n",
        "    name = 'train_accuracy')\n",
        "# start I have a dream end\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp  = tar[:, :-1]  # 没带end  decode输入, 比如输入start得到I\n",
        "    tar_real = tar[:, 1:]  # 没有start  输入的是dream时，输出是end\n",
        "    \n",
        "    encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
        "    = create_masks(inp, tar_inp)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, True,\n",
        "                                     encoder_padding_mask,\n",
        "                                     decoder_mask,\n",
        "                                     encoder_decoder_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "    \n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(gradients, transformer.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)\n",
        "# 一个epochs接近90秒\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    # reset后就会从零开始累计\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "                epoch + 1, batch, train_loss.result(),\n",
        "                train_accuracy.result()))\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "        epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "    print('Time take for 1 epoch: {} secs\\n'.format(\n",
        "        time.time() - start))\n",
        "\n",
        "# loss是正常指标，accuracy只是机器翻译的一个参考指标（没啥意义），可以看趋势\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9b4cCl2fvUS"
      },
      "source": [
        "# 7 Evaluate and Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "0DkMpRAefvUS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "eg: A B C D -> E F G H.\n",
        "Train: A B C D, E F G -> F G H\n",
        "Eval:  A B C D -> E\n",
        "       A B C D, E -> F\n",
        "       A B C D, E F -> G\n",
        "       A B C D, E F G -> H\n",
        "类似seq2seq2\n",
        "不同的是 transformer可以并行的处理，前后没有依赖，而seq2seq前后有依赖\n",
        "\"\"\"\n",
        "def evaluate(inp_sentence):\n",
        "    # 文本的句子转换为id的句子\n",
        "    input_id_sentence = [pt_tokenizer.vocab_size] \\\n",
        "    + pt_tokenizer.encode(inp_sentence) + [pt_tokenizer.vocab_size + 1]\n",
        "    # transformer转换是两维的，因此转换\n",
        "    # encoder_input.shape: (1, input_sentence_length)\n",
        "    encoder_input = tf.expand_dims(input_id_sentence, 0)\n",
        "    \n",
        "    # decoder_input.shape: (1, 1)\n",
        "    # 我们预测一个词就放入decoder_input，decoder_input给多个就可以预测多个，我们给一个\n",
        "    decoder_input = tf.expand_dims([en_tokenizer.vocab_size], 0)\n",
        "    \n",
        "    for i in range(max_length):\n",
        "        # 产生mask并传给transformer\n",
        "        encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
        "        = create_masks(encoder_input, decoder_input)\n",
        "        # predictions.shape: (batch_size, output_target_len, target_vocab_size)\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            decoder_input,\n",
        "            False,\n",
        "            encoder_padding_mask,\n",
        "            decoder_mask,\n",
        "            encoder_decoder_padding_mask)\n",
        "        # predictions.shape: (batch_size, output_target_len, target_vocab_size)\n",
        "        # decoder每次输出的维度和输入的维度是一致的，所以是最后一个\n",
        "        predictions = predictions[:, -1, :]\n",
        "        # 预测值就是概率最大的那个的索引\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis = -1),\n",
        "                               tf.int32)\n",
        "        # 如果等于end_id，预测结束\n",
        "        if tf.equal(predicted_id, en_tokenizer.vocab_size + 1):\n",
        "            return tf.squeeze(decoder_input, axis = 0), attention_weights\n",
        "        # 如果predicted_id不是end_id，添加到新的decoder_input中\n",
        "        decoder_input = tf.concat([decoder_input, [predicted_id]],\n",
        "                                  axis = -1)\n",
        "    return tf.squeeze(decoder_input, axis = 0), attention_weights\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "dcqhVGCcfvUS"
      },
      "outputs": [],
      "source": [
        "def plot_encoder_decoder_attention(attention, input_sentence,\n",
        "                                   result, layer_name):\n",
        "    fig = plt.figure(figsize = (16, 8))\n",
        "    \n",
        "    input_id_sentence = pt_tokenizer.encode(input_sentence)\n",
        "    \n",
        "    # attention.shape: (num_heads, tar_len, input_len)\n",
        "    attention = tf.squeeze(attention[layer_name], axis = 0)\n",
        "    \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head + 1)\n",
        "        \n",
        "        ax.matshow(attention[head][:-1, :])\n",
        "        \n",
        "        fontdict = {'fontsize': 10}\n",
        "        \n",
        "        ax.set_xticks(range(len(input_id_sentence) + 2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "        \n",
        "        ax.set_ylim(len(result) - 1.5, -0.5)\n",
        "        \n",
        "        ax.set_xticklabels(\n",
        "            ['<start>'] + [pt_tokenizer.decode([i]) for i in input_id_sentence] + ['<end>'],\n",
        "            fontdict = fontdict, rotation = 90)\n",
        "        ax.set_yticklabels(\n",
        "            [en_tokenizer.decode([i]) for i in result if i < en_tokenizer.vocab_size],\n",
        "            fontdict = fontdict)\n",
        "        ax.set_xlabel('Head {}'.format(head + 1))\n",
        "    plt.tight_layout()\n",
        "    plt.show()      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o_fFQbsfvUT"
      },
      "source": [
        "# translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "QJWkmGfXfvUT"
      },
      "outputs": [],
      "source": [
        "def translate(input_sentence, layer_name = ''):\n",
        "    result, attention_weights = evaluate(input_sentence)\n",
        "    \n",
        "    predicted_sentence = en_tokenizer.decode(\n",
        "        [i for i in result if i < en_tokenizer.vocab_size])\n",
        "    \n",
        "    print(\"Input: {}\".format(input_sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))\n",
        "    \n",
        "    if layer_name:\n",
        "        plot_encoder_decoder_attention(attention_weights, input_sentence,\n",
        "                                       result, layer_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "FnV1WEqXfvUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b635141f-5451-424e-bb95-d969480d96bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: está muito frio aqui.\n",
            "Predicted translation: it 's very cold here , ok , ok , here 's a lot of cold here .\n"
          ]
        }
      ],
      "source": [
        "translate('está muito frio aqui.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "b5YTNqaOfvUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8b5d04-5bf5-454a-a999-e7f843edfbc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: isto é minha vida\n",
            "Predicted translation: this is my life .\n"
          ]
        }
      ],
      "source": [
        "translate('isto é minha vida')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "LaWAcc9JfvUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add8eee1-99ca-4e2d-81ee-69315cfdfdd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: você ainda está em casa?\n",
            "Predicted translation: are you home , then ?\n"
          ]
        }
      ],
      "source": [
        "translate('você ainda está em casa?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "IQNfV2jOfvUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561fecbf-a19b-4047-833d-643aa034d335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: este é o primeiro livro que eu já li\n",
            "Predicted translation: this is the first book i already have .\n"
          ]
        }
      ],
      "source": [
        "translate('este é o primeiro livro que eu já li')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "quaOZUREfvUT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "d4756a34-be5f-43a9-f4da-93ae8b8b5a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: este é o primeiro livro que eu já li\n",
            "Predicted translation: this is the first book i already have .\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "translate('este é o primeiro livro que eu já li',\n",
        "          layer_name = 'decoder_layer4_att2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "2TD2LUMtfvUT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8TAPKWOdfvUJ",
        "TO6VAW1BfvUM",
        "eIdAGWOUfvUN",
        "nVAgiPkSfvUP",
        "0GR_ajycfvUP",
        "hcFfOfyTfvUQ",
        "sl5FAaPPfvUQ"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}