{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845447ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n",
      "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.4\n",
      "numpy 1.19.5\n",
      "pandas 1.1.5\n",
      "sklearn 0.24.2\n",
      "tensorflow 2.6.2\n",
      "keras.api._v2.keras 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c005d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.layers.core.Dense'>\n",
      "tf.Tensor(\n",
      "[[-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]\n",
      " [-0.03858528 -0.04326512 -0.40369868 -0.07883093  0.14905742  0.5259073\n",
      "   0.23213664  0.0812642  -0.21733259  0.25261706 -0.4368534   0.50468326\n",
      "  -0.00351025  0.31512833  0.2976399   0.3284366   0.00214215  0.0527195\n",
      "   0.22180012  0.23747945  0.09429927 -0.02105241  0.586831    0.23301435\n",
      "  -0.06834011 -0.2400554  -0.06622551  0.0168791  -0.38880605 -0.10877822\n",
      "   0.7442751   0.42637402 -0.08878884  0.3183239  -0.08019064  0.40293998\n",
      "  -0.14290643  0.4058032  -0.3952074   0.07849577  0.28416297  0.29664862\n",
      "  -0.6057565   0.7171316  -0.14009866  0.33841184  0.09854349  0.00387268\n",
      "  -0.60850817  0.3112979  -0.35937917 -0.3794381   0.49908143 -0.11791952\n",
      "  -0.07312256 -0.195146    0.03248172  0.03947824 -0.1817753  -0.03234681\n",
      "  -0.13936858 -0.34608474 -0.37376216 -0.28410918 -0.59412324 -0.51248664\n",
      "  -0.12014176 -0.4455418  -0.50779426 -0.7726519  -0.2584548  -0.5457475\n",
      "   0.3729671   0.20195566 -0.0587009   0.01301901  0.2876087   0.3259067\n",
      "  -0.0680588   0.36561763 -0.3887627   0.45433843 -0.23064752  0.14572623\n",
      "  -0.05868407  0.14415596 -0.14276318 -0.26402974 -0.00570729  0.12494555\n",
      "   0.09644516  0.8100053  -0.14129598 -0.25313538 -0.13151966  0.02487528\n",
      "   0.08342281  0.29354423  0.00398831  0.55206674]], shape=(10, 100), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100, input_shape=(None, 5)) # input_shape我们往往第一层指定\n",
    "# layer是输入为5，输出为100的全连接层，所以对于输入为10x5的矩阵来看，会乘以一个5x100的矩阵，所以输出就是10x100.\n",
    "print(type(layer))\n",
    "print(layer(tf.ones([10, 5])))  # 这里是对应层的输出\n",
    "layer(tf.ones([10, 5])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e374b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
      "array([[ 8.76515657e-02,  1.63759634e-01,  1.23443350e-01,\n",
      "         2.28176728e-01, -1.65437117e-01,  1.34951994e-01,\n",
      "         4.78382260e-02, -1.26122072e-01, -6.43809885e-02,\n",
      "         1.92526773e-01, -1.74308583e-01,  2.14738801e-01,\n",
      "         9.65906829e-02,  3.28875035e-02,  1.12534210e-01,\n",
      "        -2.37516716e-01, -1.04671240e-01, -1.31085813e-01,\n",
      "        -1.87234640e-01,  3.78387421e-02,  7.19331950e-02,\n",
      "         2.07264736e-01,  1.34577081e-01, -9.01134759e-02,\n",
      "        -2.02017471e-01, -6.35806918e-02, -3.77652347e-02,\n",
      "        -1.59196109e-02, -1.78650707e-01,  1.09656468e-01,\n",
      "         2.25820169e-01,  1.64796963e-01, -2.79951096e-02,\n",
      "         1.33565858e-01,  6.33350164e-02,  2.20507070e-01,\n",
      "        -1.94790021e-01,  1.81800261e-01, -1.49349958e-01,\n",
      "        -1.70640633e-01, -2.30241448e-01,  6.10775799e-02,\n",
      "        -2.15838477e-01,  2.23176882e-01,  1.44481793e-01,\n",
      "         1.73466966e-01, -1.78428888e-02, -5.76483011e-02,\n",
      "        -1.90688133e-01,  2.23039374e-01, -1.52609602e-01,\n",
      "        -1.07276782e-01, -1.55911267e-01, -2.32439101e-01,\n",
      "        -1.38754666e-01, -4.74510193e-02,  1.42169222e-01,\n",
      "        -1.14471011e-01, -1.41547322e-01, -9.75002199e-02,\n",
      "        -1.61147535e-01,  1.56412676e-01, -4.07534838e-02,\n",
      "        -1.48057967e-01, -1.40735865e-01, -1.46086305e-01,\n",
      "         1.97343066e-01, -1.79447860e-01, -1.43775403e-01,\n",
      "        -1.86488599e-01, -1.99196666e-01, -1.60157859e-01,\n",
      "         2.09551603e-02,  2.27250978e-01, -1.03507221e-01,\n",
      "        -1.12358227e-01,  5.07774502e-02,  1.03986695e-01,\n",
      "        -5.35435081e-02,  4.82556969e-02,  4.18761373e-03,\n",
      "         1.65579244e-01,  5.35783917e-02, -1.04759812e-01,\n",
      "         3.64375263e-02,  9.85208154e-03,  1.22031853e-01,\n",
      "         4.05208319e-02,  8.42744261e-02,  1.74491301e-01,\n",
      "        -1.77033693e-02,  2.17937544e-01,  1.36359826e-01,\n",
      "        -1.32972002e-01, -1.88168928e-01, -1.43185019e-01,\n",
      "         1.73243329e-01,  2.00820908e-01, -1.43987596e-01,\n",
      "         2.05482855e-01],\n",
      "       [ 2.88741738e-02, -7.28779584e-02, -1.64006710e-01,\n",
      "        -1.26626462e-01,  2.19967976e-01, -5.63451052e-02,\n",
      "        -2.00303227e-01,  1.91941217e-01,  1.16733566e-01,\n",
      "         1.97886094e-01,  1.49157301e-01,  9.74584371e-02,\n",
      "         1.18652001e-01, -3.83444577e-02,  3.78837734e-02,\n",
      "         1.34819821e-01,  1.03986815e-01, -1.68388486e-02,\n",
      "         1.49921522e-01,  1.37092486e-01, -1.89186871e-01,\n",
      "         7.78899640e-02,  1.53078362e-01,  1.28179342e-02,\n",
      "         2.69393623e-03, -6.43434972e-02, -4.20355946e-02,\n",
      "         9.33914334e-02,  2.34320909e-02, -2.35727191e-01,\n",
      "         1.81203708e-01, -1.50926545e-01, -6.26070350e-02,\n",
      "         8.50701779e-02, -2.27237180e-01, -8.87105465e-02,\n",
      "        -4.53983247e-03, -1.05064273e-01, -1.52008101e-01,\n",
      "         2.20614448e-01,  1.76443323e-01,  1.75780013e-01,\n",
      "         4.00200933e-02,  2.38835707e-01,  1.71358690e-01,\n",
      "        -8.30648839e-03, -2.17652291e-01,  5.43932170e-02,\n",
      "        -1.20372117e-01, -1.09598279e-01,  1.65439859e-01,\n",
      "        -1.63031667e-02,  1.73817202e-01,  1.44431695e-01,\n",
      "        -8.49939734e-02, -1.98776454e-01, -1.32910788e-01,\n",
      "         1.44826621e-02,  4.62997407e-02,  1.73308983e-01,\n",
      "        -1.13550350e-01, -2.09215283e-03, -8.13931525e-02,\n",
      "         6.24280125e-02, -1.94077671e-01, -2.09774315e-01,\n",
      "        -1.27032936e-01, -1.65985957e-01,  6.30660355e-03,\n",
      "        -6.57159984e-02, -1.11578614e-01, -1.98083475e-01,\n",
      "         1.58133581e-01, -8.73812884e-02, -1.60824329e-01,\n",
      "        -8.44665021e-02, -1.86026692e-02,  2.35291317e-01,\n",
      "         1.34028479e-01,  2.25209102e-01, -1.67703956e-01,\n",
      "        -2.92271227e-02, -1.26784161e-01,  1.97709724e-01,\n",
      "         1.86205655e-02,  5.55080622e-02,  2.53899544e-02,\n",
      "        -1.19837917e-01, -5.07370532e-02,  3.99104506e-02,\n",
      "         7.53564090e-02,  6.82490021e-02, -1.96507558e-01,\n",
      "         9.42771584e-02,  9.49583799e-02, -4.19915468e-02,\n",
      "         1.77613154e-01, -8.68974179e-02,  1.28029689e-01,\n",
      "        -1.79702789e-02],\n",
      "       [-2.03574285e-01,  5.86357564e-02, -3.09439749e-02,\n",
      "        -1.35595948e-02, -1.99989945e-01,  1.84454516e-01,\n",
      "         2.36554295e-02,  1.51387855e-01, -1.59291804e-01,\n",
      "        -1.53912336e-01, -1.71172202e-01,  8.11239928e-02,\n",
      "        -1.32211655e-01,  4.39769477e-02,  1.22390524e-01,\n",
      "         2.18494251e-01,  2.15207979e-01,  1.62761346e-01,\n",
      "         1.96873099e-02,  1.02055147e-01,  1.60359293e-02,\n",
      "        -1.78391606e-01,  6.46068007e-02, -1.09280199e-01,\n",
      "         6.87785000e-02, -2.05762476e-01,  1.59476563e-01,\n",
      "        -2.06570461e-01, -8.86658579e-02, -4.01774645e-02,\n",
      "         2.18915865e-01,  4.45223600e-02,  1.70998499e-01,\n",
      "         6.36273175e-02,  2.87605077e-02,  7.52149671e-02,\n",
      "         1.36150792e-01,  1.34795532e-01,  1.74570635e-01,\n",
      "         4.65161651e-02,  7.84250945e-02,  1.01202890e-01,\n",
      "        -1.12379417e-01,  9.17132646e-02, -2.13852331e-01,\n",
      "         1.89143345e-01,  2.37984017e-01,  1.68072239e-01,\n",
      "        -1.53686255e-01,  4.64565307e-02, -2.09323674e-01,\n",
      "        -2.01841816e-01,  2.32750341e-01, -1.36203900e-01,\n",
      "         1.27885088e-01, -1.19684555e-01,  4.50333804e-02,\n",
      "         8.88982862e-02, -2.07032338e-01,  1.71401650e-02,\n",
      "         1.02122679e-01, -1.76325276e-01, -6.55350089e-03,\n",
      "         1.01073220e-01,  3.44062895e-02,  1.21196792e-01,\n",
      "        -8.82651955e-02,  1.92535371e-02, -2.12632746e-01,\n",
      "        -2.16982961e-01, -1.59378707e-01, -9.24424380e-02,\n",
      "        -2.29079723e-02,  3.31026465e-02,  1.10132262e-01,\n",
      "         7.17239082e-03,  2.68153995e-02, -7.10410178e-02,\n",
      "        -2.16502279e-01,  8.62368792e-02, -1.19414978e-01,\n",
      "         1.07390389e-01,  1.02014795e-01, -2.05341414e-01,\n",
      "        -2.11862937e-01,  3.87328118e-02,  1.28039718e-03,\n",
      "        -2.22825080e-01,  1.03357777e-01,  1.83989450e-01,\n",
      "        -9.48845297e-02,  1.81216881e-01,  6.13474995e-02,\n",
      "        -2.29247272e-01, -2.06613034e-01, -7.74991214e-02,\n",
      "        -1.87430412e-01, -3.24448347e-02, -2.35822663e-01,\n",
      "        -4.90206480e-02],\n",
      "       [-1.47023886e-01, -2.26602063e-01, -2.18972132e-01,\n",
      "        -2.71711051e-02,  2.22722784e-01,  2.12436244e-01,\n",
      "         2.15106234e-01, -9.66175646e-02, -2.26696208e-01,\n",
      "        -8.51142257e-02, -2.25325704e-01,  2.15059742e-01,\n",
      "        -2.27616072e-01,  1.58730462e-01,  2.11403742e-01,\n",
      "         1.71209142e-01, -1.08976603e-01,  1.11002371e-01,\n",
      "         2.08051249e-01,  1.35599837e-01, -3.34814191e-02,\n",
      "         2.64210254e-02,  1.90650269e-01,  1.85209051e-01,\n",
      "        -1.48707062e-01, -1.33920699e-01,  7.07403868e-02,\n",
      "         6.38399571e-02, -1.27666071e-01, -1.42869219e-01,\n",
      "         8.91815275e-02,  2.23532423e-01, -2.30622455e-01,\n",
      "        -1.23606294e-01,  1.13038674e-01,  8.52624029e-02,\n",
      "         5.42098731e-02,  2.20638260e-01, -1.93530187e-01,\n",
      "         1.52782127e-01,  1.01371303e-01,  1.90211251e-01,\n",
      "        -9.09190774e-02,  1.16877064e-01, -1.51052922e-02,\n",
      "         1.37547269e-01,  1.56104073e-01, -2.79458016e-02,\n",
      "        -1.09537929e-01, -2.41234004e-02, -1.78996935e-01,\n",
      "        -1.20763823e-01,  5.26839346e-02,  1.14420637e-01,\n",
      "         9.04498547e-02,  1.90493390e-01, -2.01376915e-01,\n",
      "         1.60355970e-01,  1.68714210e-01, -2.15301841e-01,\n",
      "         2.15559155e-02, -1.47873193e-01, -9.15493071e-02,\n",
      "        -1.21298589e-01, -1.86164483e-01, -9.08643603e-02,\n",
      "         8.78241807e-02,  5.93290776e-02, -5.24398386e-02,\n",
      "        -8.31821710e-02,  1.02937236e-01,  1.42362162e-01,\n",
      "         4.13352102e-02,  7.71491230e-03,  8.43498856e-02,\n",
      "         5.00561744e-02,  1.88847080e-01,  1.41972259e-01,\n",
      "         4.93963510e-02, -1.87678903e-01, -1.05760261e-01,\n",
      "         2.08997175e-01, -6.42773211e-02,  1.56906977e-01,\n",
      "        -1.07676476e-01, -7.07404912e-02, -1.82540014e-01,\n",
      "        -2.78194249e-03,  8.45329016e-02, -2.03880057e-01,\n",
      "         3.13100964e-02,  1.44442067e-01,  1.01762563e-02,\n",
      "         1.42391458e-01,  2.08556101e-01,  1.89854547e-01,\n",
      "         1.30900994e-01,  1.88471392e-01,  2.36400351e-01,\n",
      "         2.15390071e-01],\n",
      "       [ 1.95487157e-01,  3.38195115e-02, -1.13219216e-01,\n",
      "        -1.39650494e-01,  7.17937201e-02,  5.04096299e-02,\n",
      "         1.45839974e-01, -3.93252373e-02,  1.16302863e-01,\n",
      "         1.01230755e-01, -1.52042359e-02, -1.03697687e-01,\n",
      "         1.41074792e-01,  1.17877886e-01, -1.86572328e-01,\n",
      "         4.14301008e-02, -1.03404805e-01, -7.31195509e-02,\n",
      "         3.13746780e-02, -1.75106764e-01,  2.28998438e-01,\n",
      "        -1.54236525e-01,  4.39184159e-02,  2.34381035e-01,\n",
      "         2.10912004e-01,  2.27551952e-01, -2.16641635e-01,\n",
      "         8.21377784e-02, -1.72554702e-02,  2.00339183e-01,\n",
      "         2.91538388e-02,  1.44448832e-01,  6.14372641e-02,\n",
      "         1.59666881e-01, -5.80876619e-02,  1.10666111e-01,\n",
      "        -1.33937240e-01, -2.63665915e-02, -7.48898089e-02,\n",
      "        -1.70776337e-01,  1.58164695e-01, -2.31623128e-01,\n",
      "        -2.26639614e-01,  4.65286821e-02, -2.26981521e-01,\n",
      "        -1.53439254e-01, -6.00494146e-02, -1.32998675e-01,\n",
      "        -3.42237502e-02,  1.75523683e-01,  1.61112100e-02,\n",
      "         6.67475015e-02,  1.95741221e-01, -8.12885165e-03,\n",
      "        -6.77088648e-02, -1.97273642e-02,  1.79566815e-01,\n",
      "        -1.09787673e-01, -4.82095927e-02,  9.00060982e-02,\n",
      "         1.16507262e-02, -1.76206797e-01, -1.53512716e-01,\n",
      "        -1.78253859e-01, -1.07551545e-01, -1.86958447e-01,\n",
      "        -1.90010875e-01, -1.78690597e-01, -1.05252862e-01,\n",
      "        -2.20282227e-01,  1.08761981e-01, -2.37425923e-01,\n",
      "         1.75451115e-01,  2.12684125e-02,  1.11484975e-02,\n",
      "         1.52615175e-01,  3.97714525e-02, -8.43025297e-02,\n",
      "         1.85621530e-02,  1.93594828e-01, -7.11232424e-05,\n",
      "         1.59876049e-03, -1.95179224e-01,  1.01210758e-01,\n",
      "         2.05797240e-01,  1.10803500e-01, -1.08925372e-01,\n",
      "         4.08943743e-02, -2.27135345e-01, -6.95655793e-02,\n",
      "         1.02366552e-01,  1.98159799e-01, -1.52672008e-01,\n",
      "        -1.27584696e-01, -4.02521789e-02,  9.76964384e-02,\n",
      "        -2.10904270e-01,  2.35942155e-02,  1.93685442e-02,\n",
      "         1.98184744e-01]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 8.76515657e-02,  1.63759634e-01,  1.23443350e-01,\n",
       "          2.28176728e-01, -1.65437117e-01,  1.34951994e-01,\n",
       "          4.78382260e-02, -1.26122072e-01, -6.43809885e-02,\n",
       "          1.92526773e-01, -1.74308583e-01,  2.14738801e-01,\n",
       "          9.65906829e-02,  3.28875035e-02,  1.12534210e-01,\n",
       "         -2.37516716e-01, -1.04671240e-01, -1.31085813e-01,\n",
       "         -1.87234640e-01,  3.78387421e-02,  7.19331950e-02,\n",
       "          2.07264736e-01,  1.34577081e-01, -9.01134759e-02,\n",
       "         -2.02017471e-01, -6.35806918e-02, -3.77652347e-02,\n",
       "         -1.59196109e-02, -1.78650707e-01,  1.09656468e-01,\n",
       "          2.25820169e-01,  1.64796963e-01, -2.79951096e-02,\n",
       "          1.33565858e-01,  6.33350164e-02,  2.20507070e-01,\n",
       "         -1.94790021e-01,  1.81800261e-01, -1.49349958e-01,\n",
       "         -1.70640633e-01, -2.30241448e-01,  6.10775799e-02,\n",
       "         -2.15838477e-01,  2.23176882e-01,  1.44481793e-01,\n",
       "          1.73466966e-01, -1.78428888e-02, -5.76483011e-02,\n",
       "         -1.90688133e-01,  2.23039374e-01, -1.52609602e-01,\n",
       "         -1.07276782e-01, -1.55911267e-01, -2.32439101e-01,\n",
       "         -1.38754666e-01, -4.74510193e-02,  1.42169222e-01,\n",
       "         -1.14471011e-01, -1.41547322e-01, -9.75002199e-02,\n",
       "         -1.61147535e-01,  1.56412676e-01, -4.07534838e-02,\n",
       "         -1.48057967e-01, -1.40735865e-01, -1.46086305e-01,\n",
       "          1.97343066e-01, -1.79447860e-01, -1.43775403e-01,\n",
       "         -1.86488599e-01, -1.99196666e-01, -1.60157859e-01,\n",
       "          2.09551603e-02,  2.27250978e-01, -1.03507221e-01,\n",
       "         -1.12358227e-01,  5.07774502e-02,  1.03986695e-01,\n",
       "         -5.35435081e-02,  4.82556969e-02,  4.18761373e-03,\n",
       "          1.65579244e-01,  5.35783917e-02, -1.04759812e-01,\n",
       "          3.64375263e-02,  9.85208154e-03,  1.22031853e-01,\n",
       "          4.05208319e-02,  8.42744261e-02,  1.74491301e-01,\n",
       "         -1.77033693e-02,  2.17937544e-01,  1.36359826e-01,\n",
       "         -1.32972002e-01, -1.88168928e-01, -1.43185019e-01,\n",
       "          1.73243329e-01,  2.00820908e-01, -1.43987596e-01,\n",
       "          2.05482855e-01],\n",
       "        [ 2.88741738e-02, -7.28779584e-02, -1.64006710e-01,\n",
       "         -1.26626462e-01,  2.19967976e-01, -5.63451052e-02,\n",
       "         -2.00303227e-01,  1.91941217e-01,  1.16733566e-01,\n",
       "          1.97886094e-01,  1.49157301e-01,  9.74584371e-02,\n",
       "          1.18652001e-01, -3.83444577e-02,  3.78837734e-02,\n",
       "          1.34819821e-01,  1.03986815e-01, -1.68388486e-02,\n",
       "          1.49921522e-01,  1.37092486e-01, -1.89186871e-01,\n",
       "          7.78899640e-02,  1.53078362e-01,  1.28179342e-02,\n",
       "          2.69393623e-03, -6.43434972e-02, -4.20355946e-02,\n",
       "          9.33914334e-02,  2.34320909e-02, -2.35727191e-01,\n",
       "          1.81203708e-01, -1.50926545e-01, -6.26070350e-02,\n",
       "          8.50701779e-02, -2.27237180e-01, -8.87105465e-02,\n",
       "         -4.53983247e-03, -1.05064273e-01, -1.52008101e-01,\n",
       "          2.20614448e-01,  1.76443323e-01,  1.75780013e-01,\n",
       "          4.00200933e-02,  2.38835707e-01,  1.71358690e-01,\n",
       "         -8.30648839e-03, -2.17652291e-01,  5.43932170e-02,\n",
       "         -1.20372117e-01, -1.09598279e-01,  1.65439859e-01,\n",
       "         -1.63031667e-02,  1.73817202e-01,  1.44431695e-01,\n",
       "         -8.49939734e-02, -1.98776454e-01, -1.32910788e-01,\n",
       "          1.44826621e-02,  4.62997407e-02,  1.73308983e-01,\n",
       "         -1.13550350e-01, -2.09215283e-03, -8.13931525e-02,\n",
       "          6.24280125e-02, -1.94077671e-01, -2.09774315e-01,\n",
       "         -1.27032936e-01, -1.65985957e-01,  6.30660355e-03,\n",
       "         -6.57159984e-02, -1.11578614e-01, -1.98083475e-01,\n",
       "          1.58133581e-01, -8.73812884e-02, -1.60824329e-01,\n",
       "         -8.44665021e-02, -1.86026692e-02,  2.35291317e-01,\n",
       "          1.34028479e-01,  2.25209102e-01, -1.67703956e-01,\n",
       "         -2.92271227e-02, -1.26784161e-01,  1.97709724e-01,\n",
       "          1.86205655e-02,  5.55080622e-02,  2.53899544e-02,\n",
       "         -1.19837917e-01, -5.07370532e-02,  3.99104506e-02,\n",
       "          7.53564090e-02,  6.82490021e-02, -1.96507558e-01,\n",
       "          9.42771584e-02,  9.49583799e-02, -4.19915468e-02,\n",
       "          1.77613154e-01, -8.68974179e-02,  1.28029689e-01,\n",
       "         -1.79702789e-02],\n",
       "        [-2.03574285e-01,  5.86357564e-02, -3.09439749e-02,\n",
       "         -1.35595948e-02, -1.99989945e-01,  1.84454516e-01,\n",
       "          2.36554295e-02,  1.51387855e-01, -1.59291804e-01,\n",
       "         -1.53912336e-01, -1.71172202e-01,  8.11239928e-02,\n",
       "         -1.32211655e-01,  4.39769477e-02,  1.22390524e-01,\n",
       "          2.18494251e-01,  2.15207979e-01,  1.62761346e-01,\n",
       "          1.96873099e-02,  1.02055147e-01,  1.60359293e-02,\n",
       "         -1.78391606e-01,  6.46068007e-02, -1.09280199e-01,\n",
       "          6.87785000e-02, -2.05762476e-01,  1.59476563e-01,\n",
       "         -2.06570461e-01, -8.86658579e-02, -4.01774645e-02,\n",
       "          2.18915865e-01,  4.45223600e-02,  1.70998499e-01,\n",
       "          6.36273175e-02,  2.87605077e-02,  7.52149671e-02,\n",
       "          1.36150792e-01,  1.34795532e-01,  1.74570635e-01,\n",
       "          4.65161651e-02,  7.84250945e-02,  1.01202890e-01,\n",
       "         -1.12379417e-01,  9.17132646e-02, -2.13852331e-01,\n",
       "          1.89143345e-01,  2.37984017e-01,  1.68072239e-01,\n",
       "         -1.53686255e-01,  4.64565307e-02, -2.09323674e-01,\n",
       "         -2.01841816e-01,  2.32750341e-01, -1.36203900e-01,\n",
       "          1.27885088e-01, -1.19684555e-01,  4.50333804e-02,\n",
       "          8.88982862e-02, -2.07032338e-01,  1.71401650e-02,\n",
       "          1.02122679e-01, -1.76325276e-01, -6.55350089e-03,\n",
       "          1.01073220e-01,  3.44062895e-02,  1.21196792e-01,\n",
       "         -8.82651955e-02,  1.92535371e-02, -2.12632746e-01,\n",
       "         -2.16982961e-01, -1.59378707e-01, -9.24424380e-02,\n",
       "         -2.29079723e-02,  3.31026465e-02,  1.10132262e-01,\n",
       "          7.17239082e-03,  2.68153995e-02, -7.10410178e-02,\n",
       "         -2.16502279e-01,  8.62368792e-02, -1.19414978e-01,\n",
       "          1.07390389e-01,  1.02014795e-01, -2.05341414e-01,\n",
       "         -2.11862937e-01,  3.87328118e-02,  1.28039718e-03,\n",
       "         -2.22825080e-01,  1.03357777e-01,  1.83989450e-01,\n",
       "         -9.48845297e-02,  1.81216881e-01,  6.13474995e-02,\n",
       "         -2.29247272e-01, -2.06613034e-01, -7.74991214e-02,\n",
       "         -1.87430412e-01, -3.24448347e-02, -2.35822663e-01,\n",
       "         -4.90206480e-02],\n",
       "        [-1.47023886e-01, -2.26602063e-01, -2.18972132e-01,\n",
       "         -2.71711051e-02,  2.22722784e-01,  2.12436244e-01,\n",
       "          2.15106234e-01, -9.66175646e-02, -2.26696208e-01,\n",
       "         -8.51142257e-02, -2.25325704e-01,  2.15059742e-01,\n",
       "         -2.27616072e-01,  1.58730462e-01,  2.11403742e-01,\n",
       "          1.71209142e-01, -1.08976603e-01,  1.11002371e-01,\n",
       "          2.08051249e-01,  1.35599837e-01, -3.34814191e-02,\n",
       "          2.64210254e-02,  1.90650269e-01,  1.85209051e-01,\n",
       "         -1.48707062e-01, -1.33920699e-01,  7.07403868e-02,\n",
       "          6.38399571e-02, -1.27666071e-01, -1.42869219e-01,\n",
       "          8.91815275e-02,  2.23532423e-01, -2.30622455e-01,\n",
       "         -1.23606294e-01,  1.13038674e-01,  8.52624029e-02,\n",
       "          5.42098731e-02,  2.20638260e-01, -1.93530187e-01,\n",
       "          1.52782127e-01,  1.01371303e-01,  1.90211251e-01,\n",
       "         -9.09190774e-02,  1.16877064e-01, -1.51052922e-02,\n",
       "          1.37547269e-01,  1.56104073e-01, -2.79458016e-02,\n",
       "         -1.09537929e-01, -2.41234004e-02, -1.78996935e-01,\n",
       "         -1.20763823e-01,  5.26839346e-02,  1.14420637e-01,\n",
       "          9.04498547e-02,  1.90493390e-01, -2.01376915e-01,\n",
       "          1.60355970e-01,  1.68714210e-01, -2.15301841e-01,\n",
       "          2.15559155e-02, -1.47873193e-01, -9.15493071e-02,\n",
       "         -1.21298589e-01, -1.86164483e-01, -9.08643603e-02,\n",
       "          8.78241807e-02,  5.93290776e-02, -5.24398386e-02,\n",
       "         -8.31821710e-02,  1.02937236e-01,  1.42362162e-01,\n",
       "          4.13352102e-02,  7.71491230e-03,  8.43498856e-02,\n",
       "          5.00561744e-02,  1.88847080e-01,  1.41972259e-01,\n",
       "          4.93963510e-02, -1.87678903e-01, -1.05760261e-01,\n",
       "          2.08997175e-01, -6.42773211e-02,  1.56906977e-01,\n",
       "         -1.07676476e-01, -7.07404912e-02, -1.82540014e-01,\n",
       "         -2.78194249e-03,  8.45329016e-02, -2.03880057e-01,\n",
       "          3.13100964e-02,  1.44442067e-01,  1.01762563e-02,\n",
       "          1.42391458e-01,  2.08556101e-01,  1.89854547e-01,\n",
       "          1.30900994e-01,  1.88471392e-01,  2.36400351e-01,\n",
       "          2.15390071e-01],\n",
       "        [ 1.95487157e-01,  3.38195115e-02, -1.13219216e-01,\n",
       "         -1.39650494e-01,  7.17937201e-02,  5.04096299e-02,\n",
       "          1.45839974e-01, -3.93252373e-02,  1.16302863e-01,\n",
       "          1.01230755e-01, -1.52042359e-02, -1.03697687e-01,\n",
       "          1.41074792e-01,  1.17877886e-01, -1.86572328e-01,\n",
       "          4.14301008e-02, -1.03404805e-01, -7.31195509e-02,\n",
       "          3.13746780e-02, -1.75106764e-01,  2.28998438e-01,\n",
       "         -1.54236525e-01,  4.39184159e-02,  2.34381035e-01,\n",
       "          2.10912004e-01,  2.27551952e-01, -2.16641635e-01,\n",
       "          8.21377784e-02, -1.72554702e-02,  2.00339183e-01,\n",
       "          2.91538388e-02,  1.44448832e-01,  6.14372641e-02,\n",
       "          1.59666881e-01, -5.80876619e-02,  1.10666111e-01,\n",
       "         -1.33937240e-01, -2.63665915e-02, -7.48898089e-02,\n",
       "         -1.70776337e-01,  1.58164695e-01, -2.31623128e-01,\n",
       "         -2.26639614e-01,  4.65286821e-02, -2.26981521e-01,\n",
       "         -1.53439254e-01, -6.00494146e-02, -1.32998675e-01,\n",
       "         -3.42237502e-02,  1.75523683e-01,  1.61112100e-02,\n",
       "          6.67475015e-02,  1.95741221e-01, -8.12885165e-03,\n",
       "         -6.77088648e-02, -1.97273642e-02,  1.79566815e-01,\n",
       "         -1.09787673e-01, -4.82095927e-02,  9.00060982e-02,\n",
       "          1.16507262e-02, -1.76206797e-01, -1.53512716e-01,\n",
       "         -1.78253859e-01, -1.07551545e-01, -1.86958447e-01,\n",
       "         -1.90010875e-01, -1.78690597e-01, -1.05252862e-01,\n",
       "         -2.20282227e-01,  1.08761981e-01, -2.37425923e-01,\n",
       "          1.75451115e-01,  2.12684125e-02,  1.11484975e-02,\n",
       "          1.52615175e-01,  3.97714525e-02, -8.43025297e-02,\n",
       "          1.85621530e-02,  1.93594828e-01, -7.11232424e-05,\n",
       "          1.59876049e-03, -1.95179224e-01,  1.01210758e-01,\n",
       "          2.05797240e-01,  1.10803500e-01, -1.08925372e-01,\n",
       "          4.08943743e-02, -2.27135345e-01, -6.95655793e-02,\n",
       "          1.02366552e-01,  1.98159799e-01, -1.52672008e-01,\n",
       "         -1.27584696e-01, -4.02521789e-02,  9.76964384e-02,\n",
       "         -2.10904270e-01,  2.35942155e-02,  1.93685442e-02,\n",
       "          1.98184744e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer.variables 可以打印layer里包含的所有参数\n",
    "# x * w + b  w就是指层的参数，kernel就是w，b就是bias\n",
    "print(layer.variables)\n",
    "print('-'*50)\n",
    "# 获得所有可训练的变量\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa7ec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module keras.layers.core object:\n",
      "\n",
      "class Dense(keras.engine.base_layer.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`). These are all attributes of\n",
      " |  `Dense`.\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  When a popular kwarg `input_shape` is passed, then keras will create\n",
      " |  an input layer to insert before the current layer. This can be treated\n",
      " |  equivalent to explicitly defining an `InputLayer`.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      " |  >>> # Now the model will take as input arrays of shape (None, 16)\n",
      " |  >>> # and output arrays of shape (None, 32).\n",
      " |  >>> # Note that after the first layer, you don't need to specify\n",
      " |  >>> # the size of the input anymore:\n",
      " |  >>> model.add(tf.keras.layers.Dense(32))\n",
      " |  >>> model.output_shape\n",
      " |  (None, 32)\n",
      " |  \n",
      " |  Args:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\").\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs` only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask generated\n",
      " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
      " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
      " |            layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
      " |      every time it is called. The callers should make a copy of the returned dict\n",
      " |      if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after updating\n",
      " |      a layer weights. It can be overridden to finalize any additional layer state\n",
      " |      after a weight update.\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f866b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70806098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e93134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7ef527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.log(1+np.exp(2*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4691ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5417706e-05 6.7153489e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 使用lamba，一行代码就搞定\n",
    "# tf.nn.softplus : log(1+e^x)\n",
    "customized_softplus = keras.layers.Lambda(lambda x: tf.math.log(1+tf.math.exp(x)))\n",
    "print(customized_softplus([-10., -5., 0., 5., 10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa02a6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006715348489117967"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1+np.exp(-5))  # 验证上一个cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78ad487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "(None, 8)\n",
      "--------------------------------------------------\n",
      "(None, 30)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_2 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_3 (Cu (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# customized dense layer\n",
    "# 自定义全连接层\n",
    "class CustomizedDenseLayer(keras.layers.Layer):  # Layer是所有层的父类\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)  # 直接使用tf提供的\n",
    "        super().__init__(**kwargs)\n",
    "#         super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):  # 注意这里的input_shape传的是什么？什么格式？\n",
    "        \"\"\"构建所需要的参数，即kernel(w)和bias(b)\"\"\"\n",
    "        # x * w + b. \n",
    "        # eg: input_shape:[None, a] w:[a,b] output_shape: [None, b]\n",
    "        print('-'*50)\n",
    "        print(input_shape)\n",
    "        \n",
    "        # add_weight: Adds a new variable to the layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                     shape=(input_shape[1], self.units),\n",
    "                                     initializer='uniform',  # 使用均匀分布的方法去初始化kernel\n",
    "                                     trainable=True)\n",
    "        \n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                    shape = (self.units, ),  # 注意这里\n",
    "                                    initializer = 'zeros',\n",
    "                                    trainable = True)\n",
    "        \n",
    "        # 接着继承父类的build\n",
    "#         super(CustomizedDenseLayer, self).build(input_shape)\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"完成正向计算\"\"\"\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "    \n",
    "# 完全模仿dense来实现自定义层，因此input_shape传的和dense一致，只需要是特征数\n",
    "# 父类Layer自动会转为二维的input_shape，然后再传递给build\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu',\n",
    "                        input_shape=x_train.shape[1:]),  # 这里传入的是特征数\n",
    "    \n",
    "#     CustomizedDenseLayer(1),  # 不加激活函数\n",
    "    \n",
    "    # 再加一个激活函数层，这个和下面注释的两行是等价的\n",
    "#     customized_softplus,\n",
    "    CustomizedDenseLayer(1,activation=customized_softplus),\n",
    "#     keras.layers.Dense(1, activation=\"softplus\"),  # 一层实现\n",
    "#     keras.layers.Dense(1), keras.layers.Activation('softplus'), # 两层实现\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")  # 之前没有自定义层的时候，compile内部会调用build\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cee210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.1976 - val_loss: 0.6751\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5898 - val_loss: 0.5852\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5188 - val_loss: 0.5221\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4770 - val_loss: 0.4944\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4654 - val_loss: 0.5058\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4670 - val_loss: 0.4863\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4394 - val_loss: 0.4468\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4398 - val_loss: 0.4699\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4306 - val_loss: 0.4848\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4291 - val_loss: 0.4257\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train,\n",
    "                    validation_data = (x_valid_scaled, y_valid),\n",
    "                    epochs = 10,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8418b0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49118239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b19a937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42464107275009155"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29b316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
